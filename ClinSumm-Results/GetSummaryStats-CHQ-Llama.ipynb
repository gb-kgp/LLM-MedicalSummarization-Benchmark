{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de46360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer_chat = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base.get_vocab() == tokenizer_chat.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TGT  = pd.read_json('../../../data/chq/test.jsonl',orient='record',lines=True)\n",
    "list_rs = df_TGT['target'].to_list()\n",
    "\n",
    "gs_base_icl = [x.splitlines()[0] for x in open('./chq_30_2_Llama_Base.txt','r').read().split('\\n++++++++++++++++++\\n')]\n",
    "gs_base_sft = open('./CHQ_20_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_chat_icl = open('./chq_20_2_Llama_Chat.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_chat_sft = open('./CHQ_20_Chat.txt','r').read().split('\\n++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_cmp = defaultdict(list)\n",
    "\n",
    "for gs,rs in zip(gs_base_icl, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_base_sft, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_chat_icl, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_chat_sft, list_rs):\n",
    "    dict_cmp[rs].append(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def RL(ref, hyp):\n",
    "    scores = scorer.score(ref,hyp)\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs,gss in dict_cmp.items():\n",
    "  try:\n",
    "    rl_base_icl = RL(rs.split('||')[0], gss[0])\n",
    "    rl_base_sft = RL(rs.split('||')[0], gss[1])\n",
    "    rl_chat_icl = RL(rs.split('||')[0], gss[2])\n",
    "    rl_chat_sft = RL(rs.split('||')[0], gss[3])\n",
    "    \n",
    "    dict_cmp[rs].extend([rl_base_icl, rl_base_sft, rl_chat_icl, rl_chat_sft])\n",
    "    \n",
    "  except: print(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_src = df_TGT['inputs'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src[0], gs_base_icl[0], list_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                \n",
    "    return oov_frac/consider\n",
    "\n",
    "def Novel_frac(src,rs):\n",
    "    novel_frac = 0\n",
    "    novel_list = list()\n",
    "    for tok in rs.split():\n",
    "        if tok not in src: \n",
    "            novel_frac += 1\n",
    "            novel_list.append(tok)\n",
    "    \n",
    "    return novel_frac/len(rs.split()), novel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c918a-fd33-456a-9291-edc3a0cf137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_frac(tokenizer_base,list_rs[10]), list_rs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_src = defaultdict(list)\n",
    "for rs,src in zip(list_rs,list_src): dict_src[rs].append(src)\n",
    "\n",
    "list_del_key = list()\n",
    "for key,val in dict_src.items():\n",
    "    # print(key,val)\n",
    "    if len(val[0]) == 0: \n",
    "        list_del_key.append(key)\n",
    "        continue\n",
    "        \n",
    "    dict_src[key].extend(Novel_frac(val[0].lower(),key))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_base,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_chat,val[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cc4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rl_base_icl = list()\n",
    "list_rl_base_sft = list()\n",
    "list_rl_chat_icl = list()\n",
    "list_rl_chat_sft = list()\n",
    "\n",
    "list_oov_src_base = list()\n",
    "list_oov_src_chat = list()\n",
    "\n",
    "list_oov_rs_base = list()\n",
    "list_oov_rs_chat = list()\n",
    "\n",
    "list_len_rs_base = list()\n",
    "list_len_rs_chat = list()\n",
    "\n",
    "list_novel_rs = list()\n",
    "\n",
    "list_gs_base_icl = list()\n",
    "list_gs_base_sft = list()\n",
    "list_gs_chat_icl = list()\n",
    "list_gs_chat_sft = list()\n",
    "\n",
    "\n",
    "list_rs = list()\n",
    "list_sd = list()\n",
    "\n",
    "for rs in dict_cmp:\n",
    "  try:\n",
    "    gen_base_icl,gen_base_sft,gen_chat_icl,gen_chat_sft,rl_base_icl,rl_base_sft,rl_chat_icl,rl_chat_sft = dict_cmp[rs]\n",
    "    \n",
    "    sd = dict_src[rs][0]\n",
    "    \n",
    "    len_rs_base = len(tokenizer_base.tokenize(rs))\n",
    "    len_rs_chat = len(tokenizer_chat.tokenize(rs))\n",
    "\n",
    "\n",
    "    oov_sd_base = dict_src[rs][-2]\n",
    "    oov_sd_chat = dict_src[rs][-1]\n",
    "\n",
    "    oov_rs_base = OOV_frac(tokenizer_base,rs)\n",
    "    oov_rs_chat = OOV_frac(tokenizer_chat,rs)\n",
    "    \n",
    "    novel_rs = dict_src[rs][1]\n",
    "    \n",
    "    list_rl_base_icl.append(rl_base_icl)\n",
    "    list_rl_base_sft.append(rl_base_sft)\n",
    "    list_rl_chat_icl.append(rl_chat_icl)\n",
    "    list_rl_chat_sft.append(rl_chat_sft)\n",
    "    \n",
    "    list_len_rs_base.append(len_rs_base)\n",
    "    list_len_rs_chat.append(len_rs_chat)\n",
    "    \n",
    "    list_gs_base_icl.append(gen_base_icl)\n",
    "    list_gs_base_sft.append(gen_base_sft)\n",
    "    list_gs_chat_icl.append(gen_chat_icl)\n",
    "    list_gs_chat_sft.append(gen_chat_sft)\n",
    "    \n",
    "    list_rs.append(rs)\n",
    "    list_sd.append(sd)\n",
    "    \n",
    "    list_novel_rs.append(novel_rs)\n",
    "\n",
    "    list_oov_rs_base.append(oov_rs_base)\n",
    "    list_oov_rs_chat.append(oov_rs_chat)\n",
    "    \n",
    "    list_oov_src_base.append(oov_sd_base)\n",
    "    list_oov_src_chat.append(oov_sd_chat)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "#     print(rs)\n",
    "    print(len(dict_cmp[rs]), len(dict_src[rs]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({ 'SD': list_sd,'RS':list_rs, \\\n",
    "                    'GS_Base_ICL': list_gs_base_icl , 'GS_Base_SFT': list_gs_base_sft, \\\n",
    "                    'GS_Chat_ICL': list_gs_chat_icl, 'GS_Chat_SFT': list_gs_chat_sft, \\\n",
    "                    'RS_Len_Base': list_len_rs_base, 'RS_Len_Chat':list_len_rs_chat, \\\n",
    "                    'OOV_RS_Base': list_oov_rs_base, 'OOV_RS_Chat': list_oov_rs_chat, \\\n",
    "                    'Novel_RS': list_novel_rs, \\\n",
    "                    'OOV_SD_Base': list_oov_src_base, 'OOV_SD_Chat': list_oov_src_chat, \\\n",
    "                    'R-L_Base_ICL': list_rl_base_icl , 'R-L_Base_SFT': list_rl_base_sft, \\\n",
    "                    'R-L_Chat_ICL': list_rl_chat_icl, 'R-L_Chat_SFT': list_rl_chat_sft,\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dc0d-760a-46ca-a626-e7525d551d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./CHQ_Llama2_Compare.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08f7b-fb68-41ef-a236-92370936dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./CHQ_Llama2_Compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/Users/gunjanbalde/Documents/QuickUMLS_Files/'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_TGT_RS = defaultdict(list)\n",
    "\n",
    "lines_TGT = df['SD'].to_list()\n",
    "\n",
    "for idx,abs in enumerate(lines_TGT):\n",
    "    if idx%10 == 0: print(f'Processed till {idx+1}... ')\n",
    "\n",
    "    flag = 0\n",
    "    d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "    if len(d) == 0:\n",
    "        counter_TGT_RS[idx].append('happy')\n",
    "    \n",
    "    else:\n",
    "        for l in d:\n",
    "            counter_TGT_RS[idx].append(l[0]['ngram'])\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "counter_TGT_RS_Medical_Words = defaultdict(set)\n",
    "\n",
    "for key,val in counter_TGT_RS.items():\n",
    "    try:\n",
    "      print('---------')\n",
    "      for words in val:\n",
    "        for word in words.split():\n",
    "          print(word)\n",
    "          if re.match(pattern,word.strip()): counter_TGT_RS_Medical_Words[key].add(word.strip())\n",
    "    except: print(\"Error in\", key,val)\n",
    "\n",
    "for key,val in counter_TGT_RS_Medical_Words.items():\n",
    "    print(key,val)\n",
    "    print('-----------------')\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./CHQ_SD_MedicalWords.pkl','wb') as f:\n",
    "    pkl.dump(counter_TGT_RS_Medical_Words,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter_TGT_RS_Medical_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open('./CHQ_SD_MedicalWords.pkl','rb') as f:\n",
    "    counter_TGT_RS_Medical_Words = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_medical_words_splitmorethan1 = list()\n",
    "list_medical_words_splitmorethan3 = list()\n",
    "\n",
    "for idx, words in counter_TGT_RS_Medical_Words.items():\n",
    "    total_words = len(words)\n",
    "    splitmorethan1 = 0\n",
    "    splitmorethan2 = 0\n",
    "    splitmorethan3 = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(tokenizer_chat.tokenize(word)) > 1: splitmorethan1 += 1\n",
    "        if len(tokenizer_chat.tokenize(word)) > 3: splitmorethan3 += 1\n",
    "    \n",
    "    list_medical_words_splitmorethan1.append(splitmorethan1/total_words)\n",
    "    list_medical_words_splitmorethan3.append(splitmorethan3/total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medical_Words_SplitMoreThan1_SD'] = list_medical_words_splitmorethan1\n",
    "df['Medical_Words_SplitMoreThan3_SD'] = list_medical_words_splitmorethan3\n",
    "\n",
    "df.to_csv('./CHQ_Llama2_Compare_WithMedicalWords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df4fe24-0242-4a59-b7fa-50be7b223156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    150.000000\n",
       "mean       0.585700\n",
       "std        0.182671\n",
       "min        0.090909\n",
       "10%        0.357143\n",
       "50%        0.571429\n",
       "90%        0.833333\n",
       "max        1.000000\n",
       "Name: Novel_RS, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./CHQ_Llama2_Compare_WithMedicalWords.csv')\n",
    "df['Novel_RS'].describe(percentiles=[0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8dc85a-cd81-47d0-9c40-8a8f69d95d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Novel_RS</th>\n",
       "      <th>R-L_Base_ICL</th>\n",
       "      <th>R-L_Base_SFT</th>\n",
       "      <th>R-L_Chat_ICL</th>\n",
       "      <th>R-L_Chat_SFT</th>\n",
       "      <th>CSr__Base_ICL</th>\n",
       "      <th>CSr__Base_SFT</th>\n",
       "      <th>CSr__Chat_ICL</th>\n",
       "      <th>CSr__Chat_SFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.273783</td>\n",
       "      <td>0.398119</td>\n",
       "      <td>0.287364</td>\n",
       "      <td>0.453018</td>\n",
       "      <td>0.249466</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.234830</td>\n",
       "      <td>0.268239</td>\n",
       "      <td>0.254230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.077848</td>\n",
       "      <td>0.264844</td>\n",
       "      <td>0.140730</td>\n",
       "      <td>0.220840</td>\n",
       "      <td>0.184576</td>\n",
       "      <td>0.409206</td>\n",
       "      <td>0.276619</td>\n",
       "      <td>0.268450</td>\n",
       "      <td>0.270754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.292857</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.233032</td>\n",
       "      <td>0.437666</td>\n",
       "      <td>0.208711</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.236111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Novel_RS  R-L_Base_ICL  R-L_Base_SFT  R-L_Chat_ICL  R-L_Chat_SFT  \\\n",
       "count  16.000000     16.000000     16.000000     16.000000     16.000000   \n",
       "mean    0.273783      0.398119      0.287364      0.453018      0.249466   \n",
       "std     0.077848      0.264844      0.140730      0.220840      0.184576   \n",
       "min     0.090909      0.076923      0.083333      0.000000      0.000000   \n",
       "50%     0.292857      0.366071      0.233032      0.437666      0.208711   \n",
       "max     0.357143      0.900000      0.583333      0.900000      0.727273   \n",
       "\n",
       "       CSr__Base_ICL  CSr__Base_SFT  CSr__Chat_ICL  CSr__Chat_SFT  \n",
       "count      16.000000      16.000000      16.000000      16.000000  \n",
       "mean        0.404762       0.234830       0.268239       0.254230  \n",
       "std         0.409206       0.276619       0.268450       0.270754  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.433333       0.174242       0.250000       0.236111  \n",
       "max         1.000000       0.923077       1.000000       0.750000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_high_oov = df[df['Novel_RS'] <=0.357143]\n",
    "df_desc = df_high_oov.drop(columns=['SD', 'RS', 'GS_Base_ICL', 'GS_Base_SFT', 'GS_Chat_ICL', 'GS_Chat_SFT',\n",
    "       'RS_Len_Base', 'RS_Len_Chat', 'OOV_RS_Base', 'OOV_RS_Chat',\n",
    "       'OOV_SD_Base', 'OOV_SD_Chat', 'Medical_Words_SplitMoreThan1',\n",
    "       'Medical_Words_SplitMoreThan3', 'Medical_Words_SplitMoreThan1_SD',\n",
    "       'Medical_Words_SplitMoreThan3_SD'] )\n",
    "df_desc.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019682e",
   "metadata": {},
   "source": [
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.272727\t0.250000    0.279070\t0.260870\t0.210391\t0.263768\t0.232946\t0.280702\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.272727\t0.266667    0.279070\t0.232558\t0.196721\t0.250000\t0.226415\t0.297872\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.267072\t0.269160    0.244400\t0.230769\t0.233032\t0.237647\t0.224747\t0.228390\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.280000\t0.285714    0.285714\t0.263158\t0.257143\t0.280000\t0.254545\t0.300000\n",
    "\n",
    "High Novelty\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.215219\t0.198361    0.203144\t0.165301\t0.160645\t0.188988\t0.167671\t0.211646\n",
    "\n",
    "Low Novelty\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.317647\t0.324324    0.280000\t0.222222\t0.236559\t0.227273\t0.238095\t0.303030\n",
    "\n",
    "## CSr\n",
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.183977\t0.163005\t0.172201\t0.121966\t0.104587\t0.167962\t0.130010\t0.146381\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.195927\t0.186363\t0.185171\t0.127157\t0.121422\t0.170255\t0.129106\t0.169170\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.148352\t0.179144\t0.138095\t0.133333\t0.144599\t0.156923\t0.145503\t0.142857\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.166667\t0.173913\t0.153846\t0.117647\t0.111111\t0.166667\t0.117647\t0.153846\n",
    "\n",
    "High Novelty\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.077510\t0.081059\t0.090653\t0.085247\t0.093905\t0.078628\t0.090186\t0.099235\n",
    "\n",
    "Low Novelty\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.250000\t0.320000\t0.339623\t0.181818\t0.181818\t0.250000\t0.210526\t0.352941\n",
    "\n",
    "Long RS (All)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.216561\t0.208955\t0.197802\t0.185185\t0.212389\t0.184874\t0.194175\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.280949\t0.251761\t0.245256\t0.244400\t0.229670\t0.254521\t0.262726\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.255567\t0.232558\t0.218750\t0.216539\t0.208644\t0.214286\t0.240000\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.233333\t0.237288\t0.218182\t0.210526\t0.225806\t0.212766\t0.22727\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.229508\t0.208696\t0.208333\t0.200000\t0.200000\t0.210526\t0.227273\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = df_desc[df_desc['R-L_Base']<df_desc['R-L_CPT_Lookup']]\n",
    "df_desc = df_desc[df_desc['R-L_CPT_Without_Vocab']<df_desc['R-L_CPT_Lookup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_greatest = df_desc.nlargest(int(20), 'R-L_CPT_Lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfdb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in df_desc_greatest.iterrows():\n",
    "    print('------------------------'*8)\n",
    "    print('+++SD',row['SD'])\n",
    "    print('+++RS        :',row['RS'])\n",
    "    print('######################################')\n",
    "    print('**GS_Base    :',row['GS_Base'])\n",
    "    print('**GS_W/oVocab:',row['GS_CPT_Without_Vocab'])\n",
    "    print('**GS_Lookup  :',row['GS_CPT_Lookup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb0411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8cccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847fd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e2fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Base'] >= 104]\n",
    "df_long_rs.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b917ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./BioASQ_Summaries/BioASQ_Llama2_Compare_SelfAdaptBPE_MedicalWords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87262a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def getMatchInfo(list_matches):\n",
    "    dict_concepts = defaultdict(list)\n",
    "    for match_l in list_matches:\n",
    "        for match_l_d in match_l:\n",
    "            if match_l_d[\"preferred\"] == 1: \n",
    "                cui = match_l_d['cui'] #the concept-id\n",
    "                start = match_l_d['start'] \n",
    "                end = match_l_d['end']\n",
    "                n_gram = match_l_d['ngram'].strip() #the surface form\n",
    "\n",
    "                if '\\n' in n_gram : continue\n",
    "                key = str(start)+'_'+str(end)+'_'+n_gram\n",
    "\n",
    "                if not cui in dict_concepts[key]: dict_concepts[key].append(cui)\n",
    "                    \n",
    "    str_ret = str()\n",
    "    \n",
    "    for key,val in dict_concepts.items():\n",
    "        str_ret += str(key) + ':'\n",
    "        for c_name in val:\n",
    "            if not '\\n' in c_name:\n",
    "                str_ret += c_name + '|'\n",
    "        str_ret += '\\n'\n",
    "    return (str_ret)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "def CIEval(f_score):\n",
    "    dataset = np.array([x[1] for x in f_score])\n",
    "    max_l = len(dataset)\n",
    "    scores = list()\n",
    "    for _ in range(1000):\n",
    "        # bootstrap sample\n",
    "        indices = randint(0, max_l, max_l)\n",
    "        sample = dataset[indices]\n",
    "        # calculate and store statistic\n",
    "        statistic = mean(sample)\n",
    "        scores.append(statistic)\n",
    "\n",
    "    print('50th percentile (median) = %.4f' % median(scores))\n",
    "    # calculate 95% confidence intervals (100 - alpha)\n",
    "    alpha = 5.0\n",
    "    # calculate lower percentile (e.g. 2.5)\n",
    "    lower_p = alpha / 2.0\n",
    "    # retrieve observation at lower percentile\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "#     print('%.1fth percentile = %.4f' % (lower_p, lower))\n",
    "    # calculate upper percentile (e.g. 97.5)\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    # retrieve observation at upper percentile\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "#     print('%.1fth percentile = %.4f' % (upper_p, upper))\n",
    "    print('C.I. Window = %.4f '%max([upper-median(scores),median(scores)-lower]))\n",
    "for column in ['GS_Base_ICL', 'GS_Base_SFT', 'GS_Chat_ICL', 'GS_Chat_SFT']:\n",
    "\n",
    "    print(f'--Processing {column}.....')\n",
    "\n",
    "    f_score = list()\n",
    "    for row_id in range(df.shape[0]):\n",
    "        try:                \n",
    "            dec_sum = df.iloc[row_id][column]\n",
    "            ref_sum = df.iloc[row_id]['RS']\n",
    "            ref_sum = ref_sum.split('||')[0]\n",
    "            \n",
    "            ref_con = getMatchInfo(matcher.match(ref_sum, best_match=True, ignore_syntax=False))\n",
    "            dec_con = getMatchInfo(matcher.match(dec_sum, best_match=True, ignore_syntax=False))\n",
    "            \n",
    "            # print(f'REF_Sum: {ref_sum}')\n",
    "            # print(f'DEC_Sum: {dec_sum}')\n",
    "\n",
    "            # print(check_overlap(ref_con,dec_con))\n",
    "            f_score.append((row_id,check_overlap(ref_con,dec_con)))\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "#         break\n",
    "#     break\n",
    "    df['CSr_'+column[2:]] = [x[1] for x in f_score]\n",
    "    CIEval(f_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./CHQ_Llama2_Compare_WithMedicalWords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da07f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5836b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df869f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "print(wilcoxon(df_high_oov['R-L_CPT_Without_Vocab'].to_list(),df_high_oov['R-L_CPT_Lookup'].to_list(),alternative='less'), \\\n",
    "wilcoxon(df_low_oov['R-L_CPT_Without_Vocab'].to_list(),df_low_oov['R-L_CPT_Lookup'].to_list(),alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fff050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                oov_list.append([token,tokenizer.tokenize(token)])\n",
    "    return oov_frac/consider, oov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157230",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "list_common_words_worst = []\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_worst = df_high_oov_worst.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE')\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               : ',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Filtering w/o AdaptBPE           :',OOV_frac(tokenizer_filtering,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words_worst.append(common_words)\n",
    "    print('-- Common words Frac                :', common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34241a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Lookup']>=0.27]\n",
    "df_high_oov.describe()\n",
    "\n",
    "df_long_rs = df[df['RS_Len_Lookup']>=93]\n",
    "df_long_rs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long-RS:\n",
    "Base > CPT_Filter_W/OABPE > CPT_W/O_Vocab > CPT_Lookup > CPT_Filter_AdaptBPE\n",
    "0.219003 > 0.213338 > 0.208825 > 0.205657 > 0.185030\n",
    "\n",
    "High-OOV:\n",
    "CPT_Lookup > Base > CPT_Filter_AdaptBPE > CPT_Filter_W/OABPE > CPT_W/O_Vocab\n",
    "0.263158 > 0.258094 > 0.255865 > 0.251984 > 0.245526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee666116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Filtering_WithoutAdaptBPE']>=0.25]\n",
    "df_high_oov.describe()\n",
    "\n",
    "High-OOV:\n",
    "Lookup25.59 > W/O_Vocab25.00 > Base24.83 > Filter_W/OABPE22.04 > Filter_ABPE20.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Filtering_WithoutAdaptBPE']>=89]\n",
    "df_long_rs.describe()\n",
    "\n",
    "Long-RS:\n",
    "Base20.64 > Lookup20.15 > W/O_Vocab19.33 > Filter_W/OABPE18.90 > Filter_ABPE18.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2068244",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Lookup']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Lookup']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_worst.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_BioASQ = pd.read_csv('./BioASQ_Summaries/BioASQ_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv')\n",
    "df_BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab88bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMLS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
