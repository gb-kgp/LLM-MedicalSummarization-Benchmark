{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de46360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer_chat = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base.get_vocab() == tokenizer_chat.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TGT  = pd.read_json('../../../data/opi/test.jsonl',orient='record',lines=True)\n",
    "list_rs = df_TGT['target'].to_list()\n",
    "list_rs = [f'{x}||{idx}'for idx,x in enumerate(list_rs)]\n",
    "\n",
    "gs_base_icl = [x.splitlines()[0] for x in open('./opi_20_8_Llama_Base.txt','r').read().split('\\n++++++++++++++++++\\n')]\n",
    "gs_base_sft = open('./opi_10_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_chat_icl = open('./opi_20_4_Llama_Chat.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_chat_sft = open('./opi_10_Chat.txt','r').read().split('\\n++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c31029",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_cmp = defaultdict(list)\n",
    "\n",
    "for gs,rs in zip(gs_base_icl, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_base_sft, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_chat_icl, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_chat_sft, list_rs):\n",
    "    dict_cmp[rs].append(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def RL(ref, hyp):\n",
    "    scores = scorer.score(ref,hyp)\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs,gss in dict_cmp.items():\n",
    "  try:\n",
    "    rl_base_icl = RL(rs.split('||')[0], gss[0])\n",
    "    rl_base_sft = RL(rs.split('||')[0], gss[1])\n",
    "    rl_chat_icl = RL(rs.split('||')[0], gss[2])\n",
    "    rl_chat_sft = RL(rs.split('||')[0], gss[3])\n",
    "    \n",
    "    dict_cmp[rs].extend([rl_base_icl, rl_base_sft, rl_chat_icl, rl_chat_sft])\n",
    "    \n",
    "  except: print(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_src = df_TGT['inputs'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src[0], gs_base_icl[0], list_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                \n",
    "    return oov_frac/consider\n",
    "\n",
    "def Novel_frac(src,rs):\n",
    "    novel_frac = 0\n",
    "    novel_list = list()\n",
    "    for tok in rs.split():\n",
    "        if tok not in src: \n",
    "            novel_frac += 1\n",
    "            novel_list.append(tok)\n",
    "    \n",
    "    return novel_frac/len(rs.split()), novel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c918a-fd33-456a-9291-edc3a0cf137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_frac(tokenizer_base,list_rs[10]), list_rs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_src = defaultdict(list)\n",
    "for rs,src in zip(list_rs,list_src): dict_src[rs].append(src)\n",
    "\n",
    "list_del_key = list()\n",
    "for key,val in dict_src.items():\n",
    "    # print(key,val)\n",
    "    if len(val[0]) == 0: \n",
    "        list_del_key.append(key)\n",
    "        continue\n",
    "        \n",
    "    dict_src[key].extend(Novel_frac(val[0].lower(),key))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_base,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_chat,val[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cc4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rl_base_icl = list()\n",
    "list_rl_base_sft = list()\n",
    "list_rl_chat_icl = list()\n",
    "list_rl_chat_sft = list()\n",
    "\n",
    "list_oov_src_base = list()\n",
    "list_oov_src_chat = list()\n",
    "\n",
    "list_oov_rs_base = list()\n",
    "list_oov_rs_chat = list()\n",
    "\n",
    "list_len_rs_base = list()\n",
    "list_len_rs_chat = list()\n",
    "\n",
    "list_novel_rs = list()\n",
    "\n",
    "list_gs_base_icl = list()\n",
    "list_gs_base_sft = list()\n",
    "list_gs_chat_icl = list()\n",
    "list_gs_chat_sft = list()\n",
    "\n",
    "\n",
    "list_rs = list()\n",
    "list_sd = list()\n",
    "\n",
    "for rs in dict_cmp:\n",
    "  try:\n",
    "    gen_base_icl,gen_base_sft,gen_chat_icl,gen_chat_sft,rl_base_icl,rl_base_sft,rl_chat_icl,rl_chat_sft = dict_cmp[rs]\n",
    "    \n",
    "    sd = dict_src[rs][0]\n",
    "    \n",
    "    len_rs_base = len(tokenizer_base.tokenize(rs))\n",
    "    len_rs_chat = len(tokenizer_chat.tokenize(rs))\n",
    "\n",
    "\n",
    "    oov_sd_base = dict_src[rs][-2]\n",
    "    oov_sd_chat = dict_src[rs][-1]\n",
    "\n",
    "    oov_rs_base = OOV_frac(tokenizer_base,rs)\n",
    "    oov_rs_chat = OOV_frac(tokenizer_chat,rs)\n",
    "    \n",
    "    novel_rs = dict_src[rs][1]\n",
    "    \n",
    "    list_rl_base_icl.append(rl_base_icl)\n",
    "    list_rl_base_sft.append(rl_base_sft)\n",
    "    list_rl_chat_icl.append(rl_chat_icl)\n",
    "    list_rl_chat_sft.append(rl_chat_sft)\n",
    "    \n",
    "    list_len_rs_base.append(len_rs_base)\n",
    "    list_len_rs_chat.append(len_rs_chat)\n",
    "    \n",
    "    list_gs_base_icl.append(gen_base_icl)\n",
    "    list_gs_base_sft.append(gen_base_sft)\n",
    "    list_gs_chat_icl.append(gen_chat_icl)\n",
    "    list_gs_chat_sft.append(gen_chat_sft)\n",
    "    \n",
    "    list_rs.append(rs)\n",
    "    list_sd.append(sd)\n",
    "    \n",
    "    list_novel_rs.append(novel_rs)\n",
    "\n",
    "    list_oov_rs_base.append(oov_rs_base)\n",
    "    list_oov_rs_chat.append(oov_rs_chat)\n",
    "    \n",
    "    list_oov_src_base.append(oov_sd_base)\n",
    "    list_oov_src_chat.append(oov_sd_chat)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "#     print(rs)\n",
    "    print(len(dict_cmp[rs]), len(dict_src[rs]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({ 'SD': list_sd,'RS':list_rs, \\\n",
    "                    'GS_Base_ICL': list_gs_base_icl , 'GS_Base_SFT': list_gs_base_sft, \\\n",
    "                    'GS_Chat_ICL': list_gs_chat_icl, 'GS_Chat_SFT': list_gs_chat_sft, \\\n",
    "                    'RS_Len_Base': list_len_rs_base, 'RS_Len_Chat':list_len_rs_chat, \\\n",
    "                    'OOV_RS_Base': list_oov_rs_base, 'OOV_RS_Chat': list_oov_rs_chat, \\\n",
    "                    'Novel_RS': list_novel_rs, \\\n",
    "                    'OOV_SD_Base': list_oov_src_base, 'OOV_SD_Chat': list_oov_src_chat, \\\n",
    "                    'R-L_Base_ICL': list_rl_base_icl , 'R-L_Base_SFT': list_rl_base_sft, \\\n",
    "                    'R-L_Chat_ICL': list_rl_chat_icl, 'R-L_Chat_SFT': list_rl_chat_sft,\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dc0d-760a-46ca-a626-e7525d551d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./OPI_Llama2_Compare.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08f7b-fb68-41ef-a236-92370936dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./OPI_Llama2_Compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/Users/gunjanbalde/Documents/QuickUMLS_Files/'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_TGT_RS = defaultdict(list)\n",
    "\n",
    "lines_TGT = df['RS'].to_list()\n",
    "\n",
    "for idx,abs in enumerate(lines_TGT):\n",
    "    if idx%10 == 0: print(f'Processed till {idx+1}... ')\n",
    "\n",
    "    flag = 0\n",
    "    d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "    if len(d) == 0:\n",
    "        counter_TGT_RS[idx].append('happy')\n",
    "    \n",
    "    else:\n",
    "        for l in d:\n",
    "            counter_TGT_RS[idx].append(l[0]['ngram'])\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "counter_TGT_RS_Medical_Words = defaultdict(set)\n",
    "\n",
    "for key,val in counter_TGT_RS.items():\n",
    "    try:\n",
    "      print('---------')\n",
    "      for words in val:\n",
    "        for word in words.split():\n",
    "          print(word)\n",
    "          if re.match(pattern,word.strip()): counter_TGT_RS_Medical_Words[key].add(word.strip())\n",
    "    except: print(\"Error in\", key,val)\n",
    "\n",
    "for key,val in counter_TGT_RS_Medical_Words.items():\n",
    "    print(key,val)\n",
    "    print('-----------------')\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./OPI_RS_MedicalWords.pkl','wb') as f:\n",
    "    pkl.dump(counter_TGT_RS_Medical_Words,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter_TGT_RS_Medical_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open('./OPI_RS_MedicalWords.pkl','rb') as f:\n",
    "    counter_TGT_RS_Medical_Words = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_medical_words_splitmorethan1 = list()\n",
    "list_medical_words_splitmorethan3 = list()\n",
    "\n",
    "for idx, words in counter_TGT_RS_Medical_Words.items():\n",
    "    total_words = len(words)\n",
    "    splitmorethan1 = 0\n",
    "    splitmorethan2 = 0\n",
    "    splitmorethan3 = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(tokenizer_chat.tokenize(word)) > 1: splitmorethan1 += 1\n",
    "        if len(tokenizer_chat.tokenize(word)) > 3: splitmorethan3 += 1\n",
    "    \n",
    "    list_medical_words_splitmorethan1.append(splitmorethan1/total_words)\n",
    "    list_medical_words_splitmorethan3.append(splitmorethan3/total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medical_Words_SplitMoreThan1'] = list_medical_words_splitmorethan1\n",
    "df['Medical_Words_SplitMoreThan3'] = list_medical_words_splitmorethan3\n",
    "\n",
    "df.to_csv('./OPI_Llama2_Compare_WithMedicalWords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df4fe24-0242-4a59-b7fa-50be7b223156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    343.000000\n",
       "mean       0.833367\n",
       "std        0.196804\n",
       "min        0.230769\n",
       "10%        0.555556\n",
       "50%        0.909091\n",
       "90%        1.000000\n",
       "max        1.000000\n",
       "Name: Novel_RS, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./OPI_Llama2_Compare_WithMedicalWords.csv')\n",
    "df['Novel_RS'].describe(percentiles=[0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8dc85a-cd81-47d0-9c40-8a8f69d95d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Novel_RS</th>\n",
       "      <th>R-L_Base_ICL</th>\n",
       "      <th>R-L_Base_SFT</th>\n",
       "      <th>R-L_Chat_ICL</th>\n",
       "      <th>R-L_Chat_SFT</th>\n",
       "      <th>CSr__Base_ICL</th>\n",
       "      <th>CSr__Base_SFT</th>\n",
       "      <th>CSr__Chat_ICL</th>\n",
       "      <th>CSr__Chat_SFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.779238</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.393616</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.231618</td>\n",
       "      <td>0.258769</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334131</td>\n",
       "      <td>0.216956</td>\n",
       "      <td>0.249552</td>\n",
       "      <td>0.221727</td>\n",
       "      <td>0.474858</td>\n",
       "      <td>0.424322</td>\n",
       "      <td>0.374423</td>\n",
       "      <td>0.386953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Novel_RS  R-L_Base_ICL  R-L_Base_SFT  R-L_Chat_ICL  R-L_Chat_SFT  \\\n",
       "count      34.0     34.000000     34.000000     34.000000     34.000000   \n",
       "mean        1.0      0.779238      0.609621      0.393616      0.604781   \n",
       "std         0.0      0.334131      0.216956      0.249552      0.221727   \n",
       "min         1.0      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         1.0      1.000000      0.666667      0.400000      0.666667   \n",
       "max         1.0      1.000000      0.888889      0.923077      0.888889   \n",
       "\n",
       "       CSr__Base_ICL  CSr__Base_SFT  CSr__Chat_ICL  CSr__Chat_SFT  \n",
       "count      34.000000      34.000000      34.000000      34.000000  \n",
       "mean        0.323529       0.231618       0.258769       0.176471  \n",
       "std         0.474858       0.424322       0.374423       0.386953  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_high_oov = df.nlargest(int(0.1*df.shape[0]), [\"Novel_RS\"])\n",
    "df_desc = df_high_oov.drop(columns=['SD', 'RS', 'GS_Base_ICL', 'GS_Base_SFT', 'GS_Chat_ICL', 'GS_Chat_SFT',\n",
    "       'RS_Len_Base', 'RS_Len_Chat', 'OOV_RS_Base', 'OOV_RS_Chat',\n",
    "       'OOV_SD_Base', 'OOV_SD_Chat', 'Medical_Words_SplitMoreThan1',\n",
    "       'Medical_Words_SplitMoreThan3', 'Medical_Words_SplitMoreThan1_SD',\n",
    "       'Medical_Words_SplitMoreThan3_SD'] )\n",
    "df_desc.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019682e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40e295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87262a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def getMatchInfo(list_matches):\n",
    "    dict_concepts = defaultdict(list)\n",
    "    for match_l in list_matches:\n",
    "        for match_l_d in match_l:\n",
    "            if match_l_d[\"preferred\"] == 1: \n",
    "                cui = match_l_d['cui'] #the concept-id\n",
    "                start = match_l_d['start'] \n",
    "                end = match_l_d['end']\n",
    "                n_gram = match_l_d['ngram'].strip() #the surface form\n",
    "\n",
    "                if '\\n' in n_gram : continue\n",
    "                key = str(start)+'_'+str(end)+'_'+n_gram\n",
    "\n",
    "                if not cui in dict_concepts[key]: dict_concepts[key].append(cui)\n",
    "                    \n",
    "    str_ret = str()\n",
    "    \n",
    "    for key,val in dict_concepts.items():\n",
    "        str_ret += str(key) + ':'\n",
    "        for c_name in val:\n",
    "            if not '\\n' in c_name:\n",
    "                str_ret += c_name + '|'\n",
    "        str_ret += '\\n'\n",
    "    return (str_ret)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "def CIEval(f_score):\n",
    "    dataset = np.array([x[1] for x in f_score])\n",
    "    max_l = len(dataset)\n",
    "    scores = list()\n",
    "    for _ in range(1000):\n",
    "        # bootstrap sample\n",
    "        indices = randint(0, max_l, max_l)\n",
    "        sample = dataset[indices]\n",
    "        # calculate and store statistic\n",
    "        statistic = mean(sample)\n",
    "        scores.append(statistic)\n",
    "\n",
    "    print('50th percentile (median) = %.4f' % median(scores))\n",
    "    # calculate 95% confidence intervals (100 - alpha)\n",
    "    alpha = 5.0\n",
    "    # calculate lower percentile (e.g. 2.5)\n",
    "    lower_p = alpha / 2.0\n",
    "    # retrieve observation at lower percentile\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "#     print('%.1fth percentile = %.4f' % (lower_p, lower))\n",
    "    # calculate upper percentile (e.g. 97.5)\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    # retrieve observation at upper percentile\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "#     print('%.1fth percentile = %.4f' % (upper_p, upper))\n",
    "    print('C.I. Window = %.4f '%max([upper-median(scores),median(scores)-lower]))\n",
    "for column in ['GS_Base_ICL', 'GS_Base_SFT', 'GS_Chat_ICL', 'GS_Chat_SFT']:\n",
    "\n",
    "    print(f'--Processing {column}.....')\n",
    "\n",
    "    f_score = list()\n",
    "    for row_id in range(df.shape[0]):\n",
    "        try:                \n",
    "            dec_sum = df.iloc[row_id][column]\n",
    "            ref_sum = df.iloc[row_id]['RS']\n",
    "            ref_sum = ref_sum.split('||')[0]\n",
    "            \n",
    "            ref_con = getMatchInfo(matcher.match(ref_sum, best_match=True, ignore_syntax=False))\n",
    "            dec_con = getMatchInfo(matcher.match(dec_sum, best_match=True, ignore_syntax=False))\n",
    "            \n",
    "            # print(f'REF_Sum: {ref_sum}')\n",
    "            # print(f'DEC_Sum: {dec_sum}')\n",
    "\n",
    "            # print(check_overlap(ref_con,dec_con))\n",
    "            f_score.append((row_id,check_overlap(ref_con,dec_con)))\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "#         break\n",
    "#     break\n",
    "    df['CSr_'+column[2:]] = [x[1] for x in f_score]\n",
    "    CIEval(f_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./OPI_Llama2_Compare_WithMedicalWords.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
