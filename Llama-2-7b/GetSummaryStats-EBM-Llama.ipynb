{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de46360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenization_llama_Lookup import LlamaTokenizer\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "tokenizer_MEDVOC = AutoTokenizer.from_pretrained('./Llama-2-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Llama2/25K_0.5_/')\n",
    "tokenizer_MEDVOC_ABPE = LlamaTokenizer.from_pretrained('./Llama-2-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Llama2/25K_0.75_',\n",
    "                                                             added_vocab = './Llama-2-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Llama2/25K_0.5_/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "\n",
    "tokenizer_filter = AutoTokenizer.from_pretrained('./Llama-2-7B-EBM-Filter/EBM_Filter_Vocabs_Llama2/20K')\n",
    "\n",
    "tokenizer_filter_ABPE = LlamaTokenizer.from_pretrained('./Llama-2-7B-EBM-Filter/EBM_Filter_Vocabs_Llama2/20K',\n",
    "                                                             added_vocab = './Llama-2-7B-EBM-Filter/EBM_Filter_Vocabs_Llama2/20K/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "tokenizer_lookup = LlamaTokenizer.from_pretrained('./Llama-2-7B-EBM-MedicalLookup-Fragment/EBM_Lookup_Vocabs_SplitMoreThan1_Also_Add_As_Subwords_Llama2/EBM_Lookup_300',\n",
    "                                                 added_vocab = './Llama-2-7B-EBM-MedicalLookup-Fragment/EBM_Lookup_Vocabs_SplitMoreThan1_Also_Add_As_Subwords_Llama2/EBM_Lookup_300/added_vocab.txt',\n",
    "                                                 use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788695",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), len(tokenizer_MEDVOC),len(tokenizer_MEDVOC_ABPE),len(tokenizer_filter), len(tokenizer_filter_ABPE), len(tokenizer_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TGT  = pd.read_csv(\"../../../../../../Medical/PushToNeumann/CSV-Datasets/EBM-Test.csv\")\n",
    "list_rs = df_TGT['target_text'].to_list()\n",
    "list_rs = [x+'||'+str(idx) for idx,x in enumerate(list_rs)]\n",
    "\n",
    "gs_base = open('./EBM-Summaries/EBM_100_2_Llama2_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_llama3 = open('./EBM-Summaries/EBM_75_1_Llama3.1_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_without_vocab = open('./EBM-Summaries/EBM_75_2_Llama2_CPT_WithoutVocab.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_medvoc = open('./EBM-Summaries/EBM_80_2__Llama2_MEDVOC_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_medvoc_adaptbpe = open('./EBM-Summaries/EBM_80_2__Llama2_MEDVOC_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_filtered_without_adaptbpe = open('./EBM-Summaries/EBM_80_2__Llama2_Filter_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_filtered_adaptbpe = open('./EBM-Summaries/EBM_80_2__Llama2_Filter_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_lookup = open('./EBM-Summaries/EBM_60_2_Llama2_Lookup300.txt','r').read().split('\\n++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_cmp = defaultdict(list)\n",
    "\n",
    "for gs,rs in zip(gs_base, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_llama3, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_without_vocab, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_without_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_lookup, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def RL(ref, hyp):\n",
    "    scores = scorer.score(ref,hyp)\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs,gss in dict_cmp.items():\n",
    "  try:\n",
    "    rl_base = RL(rs.split('||')[0], gss[0])\n",
    "    rl_llama3 = RL(rs.split('||')[0], gss[1])\n",
    "    rl_cpt_without_vocab = RL(rs.split('||')[0], gss[2])\n",
    "    \n",
    "    rl_cpt_medvoc = RL(rs.split('||')[0], gss[3])\n",
    "    rl_cpt_medvoc_adaptbpe = RL(rs.split('||')[0], gss[4])\n",
    "    \n",
    "    rl_cpt_filtered_without_adaptbpe = RL(rs.split('||')[0], gss[5])\n",
    "    rl_cpt_filtered_adaptbpe = RL(rs.split('||')[0], gss[6])\n",
    "    \n",
    "    rl_cpt_lookup = RL(rs.split('||')[0], gss[7])\n",
    "    \n",
    "    dict_cmp[rs].extend([rl_base,rl_llama3,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup])\n",
    "    \n",
    "  except: print(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_src = df_TGT['input_text'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src[0], gs_base[0], list_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                \n",
    "    return oov_frac/consider\n",
    "\n",
    "def Novel_frac(src,rs):\n",
    "    novel_frac = 0\n",
    "    novel_list = list()\n",
    "    for tok in rs.split():\n",
    "        if tok not in src: \n",
    "            novel_frac += 1\n",
    "            novel_list.append(tok)\n",
    "    \n",
    "    return novel_frac/len(rs.split()), novel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c918a-fd33-456a-9291-edc3a0cf137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_frac(tokenizer,list_rs[10]), list_rs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_src = defaultdict(list)\n",
    "for rs,src in zip(list_rs,list_src): dict_src[rs].append(src)\n",
    "\n",
    "list_del_key = list()\n",
    "for key,val in dict_src.items():\n",
    "    # print(key,val)\n",
    "    if len(val[0]) == 0: \n",
    "        list_del_key.append(key)\n",
    "        continue\n",
    "        \n",
    "    dict_src[key].extend(Novel_frac(val[0].lower(),key))\n",
    "    dict_src[key].append(OOV_frac(tokenizer,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_lookup,val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cc4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rl_base = list()\n",
    "list_rl_llama3 = list()\n",
    "list_rl_cpt_without_vocab = list()\n",
    "list_rl_cpt_medvoc = list()\n",
    "list_rl_cpt_medvoc_adaptbpe = list()\n",
    "list_rl_cpt_filtered_without_adaptbpe = list()\n",
    "list_rl_cpt_filtered_adaptbpe = list()\n",
    "list_rl_cpt_lookup = list()\n",
    "\n",
    "list_oov_src_base = list()\n",
    "list_oov_src_medvoc = list()\n",
    "list_oov_src_medvoc_adaptbpe = list()\n",
    "list_oov_src_filtered_without_adaptbpe = list()\n",
    "list_oov_src_filtered_adaptbpe = list()\n",
    "list_oov_src_lookup = list()\n",
    "\n",
    "list_oov_rs_base = list()\n",
    "list_oov_rs_medvoc = list()\n",
    "list_oov_rs_medvoc_adaptbpe = list()\n",
    "list_oov_rs_filtered_without_adaptbpe = list()\n",
    "list_oov_rs_filtered_adaptbpe = list()\n",
    "list_oov_rs_lookup = list()\n",
    "\n",
    "list_len_rs_base = list()\n",
    "list_len_rs_medvoc = list()\n",
    "list_len_rs_medvoc_adaptbpe = list()\n",
    "list_len_rs_filtered_without_adaptbpe = list()\n",
    "list_len_rs_filtered_adaptbpe = list()\n",
    "list_len_rs_lookup = list()\n",
    "\n",
    "list_novel_rs = list()\n",
    "\n",
    "list_gs_base = list()\n",
    "list_gs_llama3 = list()\n",
    "list_gs_cpt_without_vocab = list()\n",
    "list_gs_cpt_medvoc = list()\n",
    "list_gs_cpt_medvoc_adaptbpe = list()\n",
    "list_gs_cpt_filtered_without_adaptbpe = list()\n",
    "list_gs_cpt_filtered_adaptbpe = list()\n",
    "list_gs_cpt_lookup = list()\n",
    "\n",
    "list_rs = list()\n",
    "list_sd = list()\n",
    "\n",
    "for rs in dict_cmp:\n",
    "  try:\n",
    "    gen_base,gen_llama3,gen_cpt_without_vocab,gen_medvoc,gen_medvoc_adaptbpe,gen_cpt_filtered_without_adaptbpe,gen_cpt_filtered_adaptbpe,gen_cpt_lookup,rl_base,rl_llama3,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup = dict_cmp[rs]\n",
    "    \n",
    "    sd = dict_src[rs][0]\n",
    "    \n",
    "    len_rs_base = len(tokenizer.tokenize(rs))\n",
    "    len_rs_medvoc = len(tokenizer_MEDVOC.tokenize(rs))\n",
    "    len_rs_medvoc_adaptbpe = len(tokenizer_MEDVOC_ABPE.tokenize(rs))\n",
    "    len_rs_filtered_without_adaptbpe = len(tokenizer_filter.tokenize(rs))\n",
    "    len_rs_filtered_adaptbpe = len(tokenizer_filter_ABPE.tokenize(rs))\n",
    "    len_rs_lookup = len(tokenizer_lookup.tokenize(rs))\n",
    "\n",
    "    oov_sd_base = dict_src[rs][-6]\n",
    "    oov_sd_medvoc = dict_src[rs][-5]\n",
    "    oov_sd_medvoc_adaptbpe = dict_src[rs][-4]\n",
    "    oov_sd_filtered_without_adaptbpe = dict_src[rs][-3]\n",
    "    oov_sd_filtered_adaptbpe = dict_src[rs][-2]\n",
    "    oov_sd_lookup= dict_src[rs][-1]\n",
    "\n",
    "    oov_rs_base = OOV_frac(tokenizer,rs)\n",
    "    oov_rs_medvoc = OOV_frac(tokenizer_MEDVOC,rs)\n",
    "    oov_rs_medvoc_adaptbpe = OOV_frac(tokenizer_MEDVOC_ABPE,rs)\n",
    "    oov_rs_filtered_without_adaptbpe = OOV_frac(tokenizer_filter,rs)\n",
    "    oov_rs_filtered_adaptbpe = OOV_frac(tokenizer_filter_ABPE,rs)\n",
    "    oov_rs_lookup = OOV_frac(tokenizer_lookup,rs)\n",
    "    \n",
    "    novel_rs = dict_src[rs][1]\n",
    "    \n",
    "    list_rl_base.append(rl_base)\n",
    "    list_rl_llama3.append(rl_llama3)\n",
    "    list_rl_cpt_without_vocab.append(rl_cpt_without_vocab)\n",
    "    list_rl_cpt_medvoc.append(rl_cpt_medvoc)\n",
    "    list_rl_cpt_medvoc_adaptbpe.append(rl_cpt_medvoc_adaptbpe)\n",
    "    list_rl_cpt_filtered_without_adaptbpe.append(rl_cpt_filtered_without_adaptbpe)\n",
    "    list_rl_cpt_filtered_adaptbpe.append(rl_cpt_filtered_adaptbpe)\n",
    "    list_rl_cpt_lookup.append(rl_cpt_lookup)\n",
    "    \n",
    "    list_len_rs_base.append(len_rs_base)\n",
    "    list_len_rs_medvoc.append(len_rs_medvoc)\n",
    "    list_len_rs_medvoc_adaptbpe.append(len_rs_medvoc_adaptbpe)\n",
    "    list_len_rs_filtered_without_adaptbpe.append(len_rs_filtered_without_adaptbpe)\n",
    "    list_len_rs_filtered_adaptbpe.append(len_rs_filtered_adaptbpe)\n",
    "    list_len_rs_lookup.append(len_rs_lookup)\n",
    "    \n",
    "    list_gs_base.append(gen_base)\n",
    "    list_gs_llama3.append(gen_llama3)\n",
    "    list_gs_cpt_medvoc.append(gen_medvoc)\n",
    "    list_gs_cpt_medvoc_adaptbpe.append(gen_medvoc_adaptbpe)\n",
    "    list_gs_cpt_without_vocab.append(gen_cpt_without_vocab)\n",
    "    list_gs_cpt_filtered_without_adaptbpe.append(gen_cpt_filtered_without_adaptbpe)\n",
    "    list_gs_cpt_filtered_adaptbpe.append(gen_cpt_filtered_adaptbpe)\n",
    "    list_gs_cpt_lookup.append(gen_cpt_lookup)\n",
    "    \n",
    "    list_rs.append(rs)\n",
    "    list_sd.append(sd)\n",
    "    \n",
    "    list_novel_rs.append(novel_rs)\n",
    "\n",
    "    list_oov_rs_base.append(oov_rs_base)\n",
    "    list_oov_rs_medvoc.append(list_oov_rs_medvoc)\n",
    "    list_oov_rs_medvoc_adaptbpe.append(list_oov_rs_medvoc_adaptbpe)\n",
    "    list_oov_rs_filtered_without_adaptbpe.append(oov_rs_filtered_without_adaptbpe)\n",
    "    list_oov_rs_filtered_adaptbpe.append(oov_rs_filtered_adaptbpe)\n",
    "    list_oov_rs_lookup.append(oov_rs_lookup)\n",
    "\n",
    "    list_oov_src_base.append(oov_sd_base)\n",
    "    list_oov_src_medvoc.append(oov_sd_medvoc)\n",
    "    list_oov_src_medvoc_adaptbpe.append(oov_sd_medvoc_adaptbpe)\n",
    "    list_oov_src_filtered_without_adaptbpe.append(oov_sd_filtered_without_adaptbpe)\n",
    "    list_oov_src_filtered_adaptbpe.append(oov_sd_filtered_adaptbpe)\n",
    "    list_oov_src_lookup.append(oov_sd_lookup)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "#     print(rs)\n",
    "    print(len(dict_cmp[rs]), len(dict_src[rs]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'SD': list_sd,'RS':list_rs, \\\n",
    "                   'GS_Base': list_gs_base , 'GS_Llama3.1': list_gs_llama3,'GS_CPT_Without_Vocab': list_gs_cpt_without_vocab, \\\n",
    "                   'GS_CPT_MedVoc': list_gs_cpt_medvoc, 'GS_CPT_MedVoc_AdaptBPE': list_gs_cpt_medvoc_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_WithoutAdaptBPE': list_gs_cpt_filtered_without_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_AdaptBPE': list_gs_cpt_filtered_adaptbpe, \\\n",
    "                   'GS_CPT_Lookup':list_gs_cpt_lookup, \\\n",
    "                   'RS_Len_Base': list_len_rs_base, 'RS_Len_Filtering_WithoutAdaptBPE': list_len_rs_filtered_without_adaptbpe, \\\n",
    "                   'RS_Len_MedVoc': list_len_rs_medvoc, 'RS_Len_MedVoc_AdaptBPE': list_len_rs_medvoc_adaptbpe, \\\n",
    "                   'RS_Len_Filtering_AdaptBPE': list_len_rs_filtered_adaptbpe, \\\n",
    "                   'RS_Len_Lookup' : list_len_rs_lookup, \\\n",
    "                   'OOV_RS_Base': list_oov_rs_base, 'OOV_RS_Filtering_WithoutAdaptBPE': list_oov_rs_filtered_without_adaptbpe, \\\n",
    "                   'OOV_RS_MedVoc': list_oov_rs_medvoc, 'OOV_RS_MedVoc_AdaptBPE': list_oov_rs_medvoc_adaptbpe, \\\n",
    "                   'OOV_RS_Filtering_AdaptBPE': list_oov_rs_filtered_adaptbpe, \\\n",
    "                   'OOV_RS_Lookup' : list_oov_rs_lookup, \\\n",
    "                   'Novel_RS': list_novel_rs, \\\n",
    "                   'OOV_SD_Base': list_oov_src_base, 'OOV_SD_Filtering_WithoutAdaptBPE': list_oov_src_filtered_without_adaptbpe, \\\n",
    "                   'OOV_SD_MedVoc': list_oov_src_medvoc, 'OOV_SD_MedVoc_AdaptBPE': list_oov_src_medvoc_adaptbpe, \\\n",
    "                   'OOV_SD_Filtering_AdaptBPE': list_oov_src_filtered_adaptbpe, \\\n",
    "                   'OOV_SD_Lookup' : list_oov_src_lookup, \\\n",
    "                   'R-L_Base': list_rl_base , 'R-L_Llama3.1': list_rl_llama3, 'R-L_CPT_Without_Vocab': list_rl_cpt_without_vocab, \\\n",
    "                   'R-L_CPT_MedVoc': list_rl_cpt_medvoc, 'R-L_CPT_MedVoc_AdaptBPE': list_rl_cpt_medvoc_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_WithoutAdaptBPE': list_rl_cpt_filtered_without_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_AdaptBPE': list_rl_cpt_filtered_adaptbpe, \\\n",
    "                   'R-L_CPT_Lookup':list_rl_cpt_lookup\n",
    "                   })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dc0d-760a-46ca-a626-e7525d551d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08f7b-fb68-41ef-a236-92370936dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_WithLlama3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/Users/gunjanbalde/Documents/QuickUMLS_Files/'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_TGT_RS = defaultdict(list)\n",
    "\n",
    "lines_TGT = df['RS'].to_list()\n",
    "\n",
    "for idx,abs in enumerate(lines_TGT):\n",
    "    if idx%10 == 0: print(f'Processed till {idx+1}... ')\n",
    "\n",
    "    flag = 0\n",
    "    d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "    if len(d) == 0:\n",
    "        counter_TGT_RS[idx].append('happy')\n",
    "    \n",
    "    else:\n",
    "        for l in d:\n",
    "            counter_TGT_RS[idx].append(l[0]['ngram'])\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "counter_TGT_RS_Medical_Words = defaultdict(set)\n",
    "\n",
    "for key,val in counter_TGT_RS.items():\n",
    "    try:\n",
    "      print('---------')\n",
    "      for words in val:\n",
    "        for word in words.split():\n",
    "          print(word)\n",
    "          if re.match(pattern,word.strip()): counter_TGT_RS_Medical_Words[key].add(word.strip())\n",
    "    except: print(\"Error in\", key,val)\n",
    "\n",
    "for key,val in counter_TGT_RS_Medical_Words.items():\n",
    "    print(key,val)\n",
    "    print('-----------------')\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords.pkl','wb') as f:\n",
    "    pkl.dump(counter_TGT_RS_Medical_Words,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter_TGT_RS_Medical_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords.pkl','rb') as f:\n",
    "    counter_TGT_RS_Medical_Words = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_medical_words_splitmorethan1 = list()\n",
    "list_medical_words_splitmorethan2 = list()\n",
    "list_medical_words_splitmorethan3 = list()\n",
    "\n",
    "for idx, words in counter_TGT_RS_Medical_Words.items():\n",
    "    total_words = len(words)\n",
    "    splitmorethan1 = 0\n",
    "    splitmorethan2 = 0\n",
    "    splitmorethan3 = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(tokenizer.tokenize(word)) > 1: splitmorethan1 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 2: splitmorethan2 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 3: splitmorethan3 += 1\n",
    "    \n",
    "    list_medical_words_splitmorethan1.append(splitmorethan1/total_words)\n",
    "    list_medical_words_splitmorethan2.append(splitmorethan2/total_words)\n",
    "    list_medical_words_splitmorethan3.append(splitmorethan3/total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medical_Words_SplitMoreThan1'] = list_medical_words_splitmorethan1\n",
    "df['Medical_Words_SplitMoreThan2'] = list_medical_words_splitmorethan2\n",
    "df['Medical_Words_SplitMoreThan3'] = list_medical_words_splitmorethan3\n",
    "\n",
    "df.to_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df4fe24-0242-4a59-b7fa-50be7b223156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    424.000000\n",
       "mean       0.408903\n",
       "std        0.157894\n",
       "min        0.000000\n",
       "10%        0.223737\n",
       "50%        0.400000\n",
       "90%        0.600000\n",
       "max        1.000000\n",
       "Name: Medical_Words_SplitMoreThan1, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv')\n",
    "df['Medical_Words_SplitMoreThan1'].describe(percentiles=[0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8dc85a-cd81-47d0-9c40-8a8f69d95d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-L_Base</th>\n",
       "      <th>R-L_Llama3.1</th>\n",
       "      <th>R-L_CPT_Without_Vocab</th>\n",
       "      <th>R-L_CPT_MedVoc</th>\n",
       "      <th>R-L_CPT_MedVoc_AdaptBPE</th>\n",
       "      <th>R-L_CPT_Filtering_WithoutAdaptBPE</th>\n",
       "      <th>R-L_CPT_Filtering_AdaptBPE</th>\n",
       "      <th>R-L_CPT_Lookup</th>\n",
       "      <th>CSr_Base</th>\n",
       "      <th>CSr_Llama3.1</th>\n",
       "      <th>CSr_CPT_Without_Vocab</th>\n",
       "      <th>CSr_CPT_MedVoc</th>\n",
       "      <th>CSr_CPT_MedVoc_AdaptBPE</th>\n",
       "      <th>CSr_CPT_Filtering_WithoutAdaptBPE</th>\n",
       "      <th>CSr_CPT_Filtering_AdaptBPE</th>\n",
       "      <th>CSr_CPT_Lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.162176</td>\n",
       "      <td>0.146550</td>\n",
       "      <td>0.166146</td>\n",
       "      <td>0.149923</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>0.169029</td>\n",
       "      <td>0.158431</td>\n",
       "      <td>0.171178</td>\n",
       "      <td>0.171049</td>\n",
       "      <td>0.163421</td>\n",
       "      <td>0.205393</td>\n",
       "      <td>0.183331</td>\n",
       "      <td>0.183979</td>\n",
       "      <td>0.174193</td>\n",
       "      <td>0.179689</td>\n",
       "      <td>0.159918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.094305</td>\n",
       "      <td>0.102310</td>\n",
       "      <td>0.094818</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.114788</td>\n",
       "      <td>0.075742</td>\n",
       "      <td>0.094957</td>\n",
       "      <td>0.161863</td>\n",
       "      <td>0.163681</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.158942</td>\n",
       "      <td>0.154811</td>\n",
       "      <td>0.158202</td>\n",
       "      <td>0.170215</td>\n",
       "      <td>0.181406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.143885</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        R-L_Base  R-L_Llama3.1  R-L_CPT_Without_Vocab  R-L_CPT_MedVoc  \\\n",
       "count  47.000000     47.000000              47.000000       47.000000   \n",
       "mean    0.162176      0.146550               0.166146        0.149923   \n",
       "std     0.094305      0.102310               0.094818        0.075169   \n",
       "min     0.030303      0.029851               0.031746        0.025641   \n",
       "50%     0.156863      0.125000               0.140351        0.147368   \n",
       "max     0.558824      0.439024               0.412698        0.411765   \n",
       "\n",
       "       R-L_CPT_MedVoc_AdaptBPE  R-L_CPT_Filtering_WithoutAdaptBPE  \\\n",
       "count                47.000000                          47.000000   \n",
       "mean                  0.149416                           0.169029   \n",
       "std                   0.071911                           0.114788   \n",
       "min                   0.045977                           0.031250   \n",
       "50%                   0.143885                           0.152672   \n",
       "max                   0.405797                           0.765432   \n",
       "\n",
       "       R-L_CPT_Filtering_AdaptBPE  R-L_CPT_Lookup   CSr_Base  CSr_Llama3.1  \\\n",
       "count                   47.000000       47.000000  47.000000     47.000000   \n",
       "mean                     0.158431        0.171178   0.171049      0.163421   \n",
       "std                      0.075742        0.094957   0.161863      0.163681   \n",
       "min                      0.047619        0.029412   0.000000      0.000000   \n",
       "50%                      0.149533        0.156250   0.129032      0.104651   \n",
       "max                      0.400000        0.400000   0.700000      0.705882   \n",
       "\n",
       "       CSr_CPT_Without_Vocab  CSr_CPT_MedVoc  CSr_CPT_MedVoc_AdaptBPE  \\\n",
       "count              47.000000       47.000000                47.000000   \n",
       "mean                0.205393        0.183331                 0.183979   \n",
       "std                 0.176900        0.158942                 0.154811   \n",
       "min                 0.000000        0.000000                 0.000000   \n",
       "50%                 0.187500        0.153846                 0.181818   \n",
       "max                 0.666667        0.521739                 0.562500   \n",
       "\n",
       "       CSr_CPT_Filtering_WithoutAdaptBPE  CSr_CPT_Filtering_AdaptBPE  \\\n",
       "count                          47.000000                   47.000000   \n",
       "mean                            0.174193                    0.179689   \n",
       "std                             0.158202                    0.170215   \n",
       "min                             0.000000                    0.000000   \n",
       "50%                             0.166667                    0.148148   \n",
       "max                             0.600000                    0.562500   \n",
       "\n",
       "       CSr_CPT_Lookup  \n",
       "count       47.000000  \n",
       "mean         0.159918  \n",
       "std          0.181406  \n",
       "min          0.000000  \n",
       "50%          0.086957  \n",
       "max          0.533333  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1'] >= 0.600000]\n",
    "df_desc = pd.DataFrame()\n",
    "for col in df_high_oov.columns:\n",
    "      if col.startswith('CSr') or col.startswith('R-L') : df_desc[col] = df_high_oov[col]\n",
    "df_desc.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019682e",
   "metadata": {},
   "source": [
    "## R-LCS\n",
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.156863\t0.125000    0.140351    0.147368\t0.143885\t0.152672\t0.149533\t0.156250\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.160000\t0.147059    0.151899\t0.146341\t0.121951\t0.145455\t0.138889\t0.153846\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.149526\t0.140423    0.146341\t0.153052\t0.155644\t0.144454\t0.149811\t0.152681\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.173333\t0.147059    0.181818\t0.181818\t0.183673\t0.172043\t0.183908\t0.166667\n",
    "\n",
    "High-Novelty\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.112000\t0.095238    0.117647\t0.119048\t0.105263\t0.125000\t0.123711\t0.137931\n",
    "\n",
    "Low-Novelty\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.223471    0.203448    0.243944\t0.219244\t0.217223\t0.229206\t0.235423\t0.245091\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "## CSr\n",
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.129032\t0.104651\t0.187500\t0.153846\t0.181818\t0.166667\t0.148148\t0.086957\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.117647\t0.090909\t0.129032\t0.111111\t0.133333\t0.125000\t0.095238\t0.076923\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.114379\t0.109610\t0.100251\t0.125000\t0.129167\t0.129167\t0.087121\t0.111455\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.111111\t0.133333\t0.142857\t0.129032\t0.142857\t0.117647\t0.125000\t0.166667\n",
    "\n",
    "High-Novelty\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.095238\t0.090909\t0.058824\t0.090909\t0.090909\t0.100000\t0.090909\t0.086957\n",
    "\n",
    "Low-Novelty\n",
    "Base        Llama-3.1   W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.182488\t0.219219\t0.281746\t0.326667\t0.228758\t0.272204\t0.270270\t0.246212\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "Long RS (All)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.182796\t0.163934\t0.180000\t0.180905\t0.175000\t0.171779\t0.157576\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.158273\t0.176471\t0.189189\t0.173913\t0.195804\t0.176991\t0.162602\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.175824\t0.172840\t0.165517\t0.166667\t0.171429\t0.164948\t0.178862\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.163636\t0.170213\t0.173516\t0.167939\t0.161905\t0.183908\t0.160920\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.173333\t0.176000\t0.180258\t0.175676\t0.185430\t0.180328\t0.177778\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Base'] >= 212.700000]\n",
    "df_long_rs.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RS_hHighOOV = df_high_oov[df_high_oov['RS_Len_Base']>=192]\n",
    "df_RS_hHighOOV.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98948082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def getMatchInfo(list_matches):\n",
    "    dict_concepts = defaultdict(list)\n",
    "    for match_l in list_matches:\n",
    "        for match_l_d in match_l:\n",
    "            if match_l_d[\"preferred\"] == 1: \n",
    "                cui = match_l_d['cui'] #the concept-id\n",
    "                start = match_l_d['start'] \n",
    "                end = match_l_d['end']\n",
    "                n_gram = match_l_d['ngram'].strip() #the surface form\n",
    "\n",
    "                if '\\n' in n_gram : continue\n",
    "                key = str(start)+'_'+str(end)+'_'+n_gram\n",
    "\n",
    "                if not cui in dict_concepts[key]: dict_concepts[key].append(cui)\n",
    "                    \n",
    "    str_ret = str()\n",
    "    \n",
    "    for key,val in dict_concepts.items():\n",
    "        str_ret += str(key) + ':'\n",
    "        for c_name in val:\n",
    "            if not '\\n' in c_name:\n",
    "                str_ret += c_name + '|'\n",
    "        str_ret += '\\n'\n",
    "    return (str_ret)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "def CIEval(f_score):\n",
    "    dataset = np.array([x[1] for x in f_score])\n",
    "    max_l = len(dataset)\n",
    "    scores = list()\n",
    "    for _ in range(1000):\n",
    "        # bootstrap sample\n",
    "        indices = randint(0, max_l, max_l)\n",
    "        sample = dataset[indices]\n",
    "        # calculate and store statistic\n",
    "        statistic = mean(sample)\n",
    "        scores.append(statistic)\n",
    "\n",
    "    print('50th percentile (median) = %.4f' % median(scores))\n",
    "    # calculate 95% confidence intervals (100 - alpha)\n",
    "    alpha = 5.0\n",
    "    # calculate lower percentile (e.g. 2.5)\n",
    "    lower_p = alpha / 2.0\n",
    "    # retrieve observation at lower percentile\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "#     print('%.1fth percentile = %.4f' % (lower_p, lower))\n",
    "    # calculate upper percentile (e.g. 97.5)\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    # retrieve observation at upper percentile\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "#     print('%.1fth percentile = %.4f' % (upper_p, upper))\n",
    "    print('C.I. Window = %.4f '%max([upper-median(scores),median(scores)-lower]))\n",
    "for column in ['GS_Base', 'GS_Llama3.1', 'GS_CPT_Without_Vocab', 'GS_CPT_MedVoc',\n",
    "       'GS_CPT_MedVoc_AdaptBPE', 'GS_CPT_Filtering_WithoutAdaptBPE',\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'GS_CPT_Lookup']:\n",
    "\n",
    "    print(f'--Processing {column}.....')\n",
    "\n",
    "    f_score = list()\n",
    "    for row_id in range(df.shape[0]):\n",
    "        try:                \n",
    "            dec_sum = df.iloc[row_id][column]\n",
    "            ref_sum = df.iloc[row_id]['RS']\n",
    "            ref_sum = ref_sum.split('||')[0]\n",
    "            \n",
    "            ref_con = getMatchInfo(matcher.match(ref_sum, best_match=True, ignore_syntax=False))\n",
    "            dec_con = getMatchInfo(matcher.match(dec_sum, best_match=True, ignore_syntax=False))\n",
    "            \n",
    "            # print(f'REF_Sum: {ref_sum}')\n",
    "            # print(f'DEC_Sum: {dec_sum}')\n",
    "\n",
    "            # print(check_overlap(ref_con,dec_con))\n",
    "            f_score.append((row_id,check_overlap(ref_con,dec_con)))\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "#         break\n",
    "#     break\n",
    "    df['CSr'+column[2:]] = [x[1] for x in f_score]\n",
    "    CIEval(f_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87262a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv')\n",
    "df['Medical_Words_SplitMoreThan1'].describe(percentiles=[0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1'] >=0.60]\n",
    "df_desc = df_high_oov.drop(columns=['GS_Llama3.1' ,'GS_CPT_MedVoc',\n",
    "       'GS_CPT_MedVoc_AdaptBPE', 'GS_CPT_Filtering_WithoutAdaptBPE',\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'RS_Len_Base',\n",
    "       'RS_Len_Filtering_WithoutAdaptBPE', 'RS_Len_MedVoc',\n",
    "       'RS_Len_MedVoc_AdaptBPE', 'RS_Len_Filtering_AdaptBPE', 'RS_Len_Lookup',\n",
    "       'OOV_RS_Base', 'OOV_RS_Filtering_WithoutAdaptBPE', 'OOV_RS_MedVoc',\n",
    "       'OOV_RS_MedVoc_AdaptBPE', 'OOV_RS_Filtering_AdaptBPE', 'OOV_RS_Lookup',\n",
    "       'Novel_RS', 'OOV_SD_Base', 'OOV_SD_Filtering_WithoutAdaptBPE',\n",
    "       'OOV_SD_MedVoc', 'OOV_SD_MedVoc_AdaptBPE', 'OOV_SD_Filtering_AdaptBPE',\n",
    "       'OOV_SD_Lookup',\n",
    "       'Medical_Words_SplitMoreThan1', 'Medical_Words_SplitMoreThan2',\n",
    "       'Medical_Words_SplitMoreThan3', 'Medical_Words_SplitMoreThan1_SD',\n",
    "       'Medical_Words_SplitMoreThan2_SD', 'Medical_Words_SplitMoreThan3_SD'])\n",
    "\n",
    "df_desc = df_desc[df_desc['R-L_Base']<df_desc['R-L_CPT_Lookup']]\n",
    "df_desc = df_desc[df_desc['R-L_CPT_Without_Vocab']<df_desc['R-L_CPT_Lookup']]\n",
    "df_desc_greatest = df_desc.nlargest(int(20), 'R-L_CPT_Lookup')\n",
    "for idx,row in df_desc_greatest.iterrows():\n",
    "    print('------------------------'*8)\n",
    "    print('+++SD',row['SD'])\n",
    "    print('+++RS        :',row['RS'])\n",
    "    print('######################################')\n",
    "    print('**GS_Base    :',row['GS_Base'])\n",
    "    print('**GS_W/oVocab:',row['GS_CPT_Without_Vocab'])\n",
    "    print('**GS_Lookup  :',row['GS_CPT_Lookup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff81be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bb5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63c1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a0a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c28ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4ba0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5497e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "print(wilcoxon(df_high_oov['R-L_CPT_Without_Vocab'].to_list(),df_high_oov['R-L_CPT_Lookup'].to_list(),alternative='less'), \\\n",
    "wilcoxon(df_low_oov['R-L_CPT_Without_Vocab'].to_list(),df_low_oov['R-L_CPT_Lookup'].to_list(),alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fff050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                oov_list.append([token,tokenizer.tokenize(token)])\n",
    "    return oov_frac/consider, oov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157230",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "list_common_words_worst = []\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_worst = df_high_oov_worst.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE')\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               : ',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Filtering w/o AdaptBPE           :',OOV_frac(tokenizer_filtering,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words_worst.append(common_words)\n",
    "    print('-- Common words Frac                :', common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34241a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Lookup']>=0.27]\n",
    "df_high_oov.describe()\n",
    "\n",
    "df_long_rs = df[df['RS_Len_Lookup']>=93]\n",
    "df_long_rs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long-RS:\n",
    "Base > CPT_Filter_W/OABPE > CPT_W/O_Vocab > CPT_Lookup > CPT_Filter_AdaptBPE\n",
    "0.219003 > 0.213338 > 0.208825 > 0.205657 > 0.185030\n",
    "\n",
    "High-OOV:\n",
    "CPT_Lookup > Base > CPT_Filter_AdaptBPE > CPT_Filter_W/OABPE > CPT_W/O_Vocab\n",
    "0.263158 > 0.258094 > 0.255865 > 0.251984 > 0.245526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee666116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Filtering_WithoutAdaptBPE']>=0.25]\n",
    "df_high_oov.describe()\n",
    "\n",
    "High-OOV:\n",
    "Lookup25.59 > W/O_Vocab25.00 > Base24.83 > Filter_W/OABPE22.04 > Filter_ABPE20.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Filtering_WithoutAdaptBPE']>=89]\n",
    "df_long_rs.describe()\n",
    "\n",
    "Long-RS:\n",
    "Base20.64 > Lookup20.15 > W/O_Vocab19.33 > Filter_W/OABPE18.90 > Filter_ABPE18.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2068244",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Lookup']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Lookup']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_worst.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d35c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMLS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
