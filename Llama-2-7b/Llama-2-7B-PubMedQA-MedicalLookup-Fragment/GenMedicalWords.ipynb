{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script takes the csv files  and filters out the words which are not present in UMLS.\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/path/to/umls'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counter_PAC = defaultdict(int)\n",
    "for csv_path in ['../../../../TxtInputFiles/PAC_input.txt']:\n",
    "\n",
    "    print(f'Starting for {csv_path}.....')\n",
    "    lines_PAC = open(csv_path).readlines()\n",
    "\n",
    "    for idx,abs in enumerate(lines_PAC):\n",
    "        if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "        flag = 0\n",
    "        d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            for l in d:\n",
    "                counter_PAC[l[0]['ngram']] += 1\n",
    "                \n",
    "    if idx%10000 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "    # df.to_csv('./%sWithConsiderFlag_AllSemTypes.csv'%csv_path.split('/')[-1][:-4],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_tokens = list(counter_PAC.keys())\n",
    "val_tokens = list(counter_PAC.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Tokens':list_tokens,'Count':val_tokens})\n",
    "df.to_csv('PAC_Tokens.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.percentile(val_tokens,[0,10,25,50,75,90,95,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_PAC = defaultdict(int)\n",
    "for csv_path in ['./pubmedqa/data/pqal_fold0/train_set.json']:\n",
    "\n",
    "    print(f'Starting for {csv_path}.....')\n",
    "    lines_PAC = json.loads(open(csv_path).read())\n",
    "    \n",
    "    lines_PAC = [' '.join(lines_PAC[x]['CONTEXTS']) for x in lines_PAC]\n",
    "\n",
    "    for idx,abs in enumerate(lines_PAC):\n",
    "        if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "        flag = 0\n",
    "        d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            for l in d:\n",
    "                counter_PAC[l[0]['ngram']] += 1\n",
    "                \n",
    "    if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "    # df.to_csv('./%sWithConsiderFlag_AllSemTypes.csv'%csv_path.split('/')[-1][:-4],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_tokens = list(counter_PAC.keys())\n",
    "val_tokens = list(counter_PAC.values())\n",
    "\n",
    "df = pd.DataFrame({'Tokens':list_tokens,'Count':val_tokens})\n",
    "df.to_csv('PubMedQA_Tokens.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_PAC = pd.read_csv('PAC_Tokens.csv')\n",
    "df_BioASQ = pd.read_csv('PubMedQA_Tokens.csv')\n",
    "\n",
    "df_BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BioASQ['Count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tokens_PAC = df_PAC.set_index('Tokens')['Count'].to_dict()\n",
    "tokens_PAC_words = defaultdict(int)\n",
    "\n",
    "tokens_BioASQ = df_BioASQ.set_index('Tokens')['Count'].to_dict()\n",
    "tokens_BioASQ_words = defaultdict(int)\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "for key,val in tokens_PAC.items():\n",
    "    try:\n",
    "      for word in key.split():  \n",
    "        if re.match(pattern,word.strip()):\n",
    "            tokens_PAC_words[word] += val\n",
    "    except: pass\n",
    "  \n",
    "for key,val in tokens_BioASQ.items():\n",
    "    try:\n",
    "      for word in key.split():  \n",
    "        if re.match(pattern,word.strip()):\n",
    "            tokens_BioASQ_words[word] += val\n",
    "    except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_PAC_words = pd.DataFrame(list(tokens_PAC_words.items()), columns=['Word', 'Count'])\n",
    "print(df_tokens_PAC_words)\n",
    "\n",
    "df_BioASQ_words = pd.DataFrame(list(tokens_BioASQ_words.items()), columns=['Word', 'Count'])\n",
    "print(df_BioASQ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_PAC_words.to_csv('PAC_MedicalWords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenier = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "df_tokens_PAC_words['Splits'] = df_tokens_PAC_words['Word'].apply(lambda x: len(tokenier.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignCategory(x):\n",
    "    if x >=4: return 'Split More than 3'\n",
    "    if x == 2: return 'Split More than Once'\n",
    "    else: return x\n",
    "df_tokens_PAC_words['Splits'] = df_tokens_PAC_words['Splits'].apply(AssignCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_PAC_words['Splits'].value_counts()/df_tokens_PAC_words.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split More than 3       0.420139\n",
    "3                       0.271512\n",
    "Split More than Once    0.239361\n",
    "1                       0.068988\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_percent_PAC = df_tokens_PAC_words.nlargest(int(len(df_tokens_PAC_words) * 0.5), 'Count')\n",
    "top_50_percent_BioASQ = df_BioASQ_words.nlargest(int(len(df_BioASQ_words) * 0.5), 'Count')\n",
    "\n",
    "print(top_50_percent_PAC)\n",
    "print(top_50_percent_BioASQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenier = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_percent_BioASQ['Splits'] = top_50_percent_BioASQ['Word'].apply(lambda x: len(tokenier.tokenize(x)))\n",
    "top_50_percent_PAC['Splits'] = top_50_percent_PAC['Word'].apply(lambda x: len(tokenier.tokenize(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_percent_PAC_3 = top_50_percent_PAC[top_50_percent_PAC['Splits'] > 3]\n",
    "top_50_percent_BioASQ_3 = top_50_percent_BioASQ[top_50_percent_BioASQ['Splits'] > 3]\n",
    "\n",
    "lookup_table_3 = top_50_percent_BioASQ_3[top_50_percent_BioASQ_3['Word'].isin(top_50_percent_PAC_3['Word'])]\n",
    "lookup_table_3.to_csv('lookup_table_EBM_SplitMoreThan3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_percent_PAC_2 = top_50_percent_PAC[top_50_percent_PAC['Splits'] > 2]\n",
    "top_50_percent_BioASQ_2 = top_50_percent_BioASQ[top_50_percent_BioASQ['Splits'] > 2]\n",
    "\n",
    "lookup_table_2 = top_50_percent_BioASQ_2[top_50_percent_BioASQ_2['Word'].isin(top_50_percent_PAC_2['Word'])]\n",
    "lookup_table_2.to_csv('lookup_table_EBM_SplitMoreThan2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_percent_PAC_1 = top_50_percent_PAC[top_50_percent_PAC['Splits'] > 1]\n",
    "top_50_percent_BioASQ_1 = top_50_percent_BioASQ[top_50_percent_BioASQ['Splits'] > 1]\n",
    "\n",
    "lookup_table_1 = top_50_percent_BioASQ_1[top_50_percent_BioASQ_1['Word'].isin(top_50_percent_PAC_1['Word'])]\n",
    "lookup_table_1.to_csv('PubMedQA_SplitMoreThan1_OOV.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('./EBM_Lookup_SplitMoreThan3')\n",
    "os.mkdir('./EBM_Lookup_SplitMoreThan2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "lookup_table_1 = pd.read_csv('./PubMedQA_SplitMoreThan1_OOV.csv')\n",
    "lookup_table_words_1 = lookup_table_1['Word'].to_list()\n",
    "\n",
    "# os.mkdir('./BioASQ_Lookup_SplitMoreThan1_Add')\n",
    "\n",
    "# for i in range(50,501,50):\n",
    "#     print(i)\n",
    "#     dump_words = lookup_table_words_1[:i]#[:i+500]\n",
    "#     with open(f'EBM_Lookup_SplitMoreThan1/EBM_Lookup_{i}.txt','w') as f: #{i+500}.txt','w') as f:\n",
    "#         f.write('\\n'.join([f'''▁{x}\\t{-32000-idx}''' for idx,x in enumerate(dump_words)]))\n",
    "#         f.write('\\n')\n",
    "#     f.close()\n",
    "    \n",
    "# lookup_table_words_2 = lookup_table_2['Word'].to_list()\n",
    "# for i in [50,100,200,500,1000,1500,2000]:\n",
    "#     print(i)\n",
    "#     dump_words = lookup_table_words_2[:i]\n",
    "#     with open(f'EBM_Lookup_SplitMoreThan2/EBM_Lookup_{i}.txt','w') as f: #{i+500}.txt','w') as f:\n",
    "#         f.write('\\n'.join([f'''▁{x}\\t{-32000-idx}''' for idx,x in enumerate(dump_words)]))\n",
    "#         f.write('\\n')\n",
    "#     f.close()\n",
    "\n",
    "# lookup_table_words_3 = lookup_table_3['Word'].to_list()\n",
    "# for i in [50,100,200,500,1000,1500,2000]: \n",
    "#     print(i)\n",
    "#     dump_words = lookup_table_words_3[:i]\n",
    "#     with open(f'EBM_Lookup_SplitMoreThan3/EBM_Lookup_{i}.txt','w') as f: #{i+500}.txt','w') as f:\n",
    "#         f.write('\\n'.join([f'''▁{x}\\t{-32000-idx}''' for idx,x in enumerate(dump_words)]))\n",
    "#         f.write('\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "os.mkdir('./PubMedQA_Lookup_SplitMoreThan1_Also_Add_As_Subwords')\n",
    "\n",
    "\n",
    "for i in range(50,501,50):\n",
    "    dump_words = lookup_table_words_1[:i]\n",
    "    list_words_to_add = defaultdict(list)\n",
    "    for word1 in dump_words:\n",
    "        list_words_to_add[f'▁{word1}'].append(word1)\n",
    "        for word2 in lookup_table_words_1:\n",
    "            if word2.startswith(word1) and word1 != word2:\n",
    "                list_words_to_add[f'▁{word1}'].append(word2)\n",
    "                continue\n",
    "            if word1 != word2 and word1 in word2 and not word2.startswith(word1):\n",
    "                list_words_to_add[word1].append(word2)\n",
    "            \n",
    "    print(i,len(list_words_to_add))\n",
    "    dump_words = list(list_words_to_add.keys()) \n",
    "    with open(f'PubMedQA_Lookup_SplitMoreThan1_Also_Add_As_Subwords/PubMedQA_Lookup_{i}.txt','w') as f: #{i+500}.txt','w') as f:\n",
    "        f.write('\\n'.join([f'''{x}\\t{-32000-idx}''' for idx,x in enumerate(dump_words)]))\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [50,100,150,200,250,300,350,400]\n",
    "y= [2.29,2.14,2.02,1.93,1.85,1.76,1.7,1.63]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "y_r = [round(i,1) for i in y]\n",
    "plt.plot(x,y_r)\n",
    "plt.ylim(1.5,2.3)\n",
    "plt.xlim(50,400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer.tokenize('antioxidant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
