{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-2-7b-hf/tokenizer_config.json',\n",
       " 'Llama-2-7b-hf/special_tokens_map.json',\n",
       " 'Llama-2-7b-hf/tokenizer.model',\n",
       " 'Llama-2-7b-hf/added_tokens.json',\n",
       " 'Llama-2-7b-hf/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer.save_pretrained('Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "dict_OOV_freq = defaultdict(int)\n",
    "dict_splits = defaultdict(int)\n",
    "\n",
    "with open('../../../../TxtInputFiles/BioASQ_input.txt','r') as f:\n",
    "    for line in f:\n",
    "        target_text = line.strip()\n",
    "        target_text = re.sub(r'[^\\w\\s]', '', target_text)\n",
    "        sws = tokenizer.tokenize(target_text)\n",
    "        print(sws)\n",
    "        i = 0\n",
    "        while(i < len(sws)-1):\n",
    "            if sws[i].startswith('▁'):\n",
    "                if sws[i+1].startswith('▁'):\n",
    "                    i+=1\n",
    "                    continue\n",
    "                else:\n",
    "                    sw = []\n",
    "                    while(True):\n",
    "                        sw.append(sws[i])\n",
    "                        i+=1\n",
    "                        if i == len(sws) or sws[i].startswith('▁'):\n",
    "                            break\n",
    "                    print(sw)\n",
    "                    dict_OOV_freq[''.join(sw).replace('▁','')] += 1\n",
    "                    dict_splits[''.join(sw).replace('▁','')] = len(sw)\n",
    "\n",
    "list_token, list_freq, list_split = list(), list(), list()        \n",
    "for token in dict_OOV_freq:\n",
    "    list_token.append(token)\n",
    "    list_freq.append(dict_OOV_freq[token])\n",
    "    list_split.append(dict_splits[token])\n",
    "\n",
    "df = pd.DataFrame({'token': list_token, 'freq': list_freq, 'split': list_split})\n",
    "df.to_csv(f'BioASQ_OOV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./CHQ_Vocabs/25K_0.5_/')\n",
    "# tokenizer.save_pretrained('./EBM-20K-1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_OOV = 0\n",
    "vocab = tokenizer.get_vocab()\n",
    "for v in vocab:\n",
    "    if vocab[v] < 258: continue\n",
    "    sws = tokenizer.tokenize(v.replace('▁',' '))\n",
    "    if len(sws) > 1 and not sws[1]==v:\n",
    "        # if vocab[v] < 32000: print(v,'PRE',vocab[v],tokenizer.tokenize(v.replace('▁',' ')))\n",
    "        if vocab[v] >= 32000: \n",
    "            print(v,'ADDED',vocab[v],tokenizer.tokenize(v.replace('▁',' ')))\n",
    "            COUNT_OOV += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3156/12212 + 984/3610 + 2579/9330 + 715/2358)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import glob\n",
    "\n",
    "oov_frac = 0.\n",
    "for fname in glob.glob('/Users/gunjanbalde/Documents/SR-NG-MedVoc/SR-NG-MedVoc/Expert-Domain/Unfiltered_Test/BART-Vocabs/*'):\n",
    "    tokenizer = BartTokenizer.from_pretrained(fname)\n",
    "    pre_vocab = tokenizer.get_vocab()\n",
    "    a_vocab = 0\n",
    "    count_oov = 0\n",
    "    \n",
    "    for v in pre_vocab:\n",
    "        if pre_vocab[v] < 50265: continue\n",
    "        a_vocab += 1\n",
    "        sws = tokenizer.tokenize(v.replace('Ġ',' '))\n",
    "        if len(sws) > 1 and not sws[1]==v:\n",
    "            count_oov += 1\n",
    "            print(v,pre_vocab[v],tokenizer.tokenize(v.replace('▁',' ')))\n",
    "    oov_frac += count_oov/a_vocab\n",
    "oov_frac/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "list_abstracts = []\n",
    "for fname in glob.glob('./BioASQ-training9b/*.json'):\n",
    "  data = json.loads(open(fname,'r').read())\n",
    "  for item in data['questions']:\n",
    "      if item['type'] == 'summary':\n",
    "          for doc in item['snippets']:\n",
    "            print(fname,doc['document'])\n",
    "            list_abstracts.append(doc['document'].split('/')[-1])\n",
    "list_abstracts = list(set(list_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'7270517' in list_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import sys\n",
    "import csv\n",
    "from time import sleep\n",
    "import random\n",
    "Entrez.email = 'balde.gunjan0812@kgpian.iitkgp.ac.in'\n",
    " \n",
    "def fetch_abstracts(pub_ids, retmax=1000, output_file='abstracts.csv'):    \n",
    "    # Make sure requests to NCBI are not too big\n",
    "    for i in range(0, len(pub_ids), retmax):\n",
    "        j = i + retmax\n",
    "        if j >= len(pub_ids):\n",
    "            j = len(pub_ids)\n",
    "\n",
    "        print(f\"Fetching abstracts from {i} to {j}.\")\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=','.join(pub_ids[i:j]),\n",
    "                        rettype=\"xml\", retmode=\"text\", retmax=retmax)\n",
    "        \n",
    "        records = Entrez.read(handle)\n",
    "        abstracts = []\n",
    "        for idx,pubmed_article in enumerate(records['PubmedArticle']):\n",
    "          try:\n",
    "            abstracts.append(pubmed_article['MedlineCitation']['Article']['ArticleTitle']+ '\\n' +\n",
    "                     ' '.join(pubmed_article['MedlineCitation']['Article']['Abstract']['AbstractText']))\n",
    "          except:\n",
    "            print(f\"Error in fetching abstract for {pub_ids[i+idx]}\")\n",
    "\n",
    "        abstract_dict = dict(zip(pub_ids[i:j], abstracts))\n",
    "\n",
    "        with open(output_file, 'a', newline='') as csvfile:\n",
    "            fieldnames = ['pub_id', 'abstract']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "            if i == 0:\n",
    "              writer.writeheader()\n",
    "            for pub_id, abstract in abstract_dict.items():\n",
    "              writer.writerow({'pub_id': pub_id, 'abstract': abstract})\n",
    "        \n",
    "        sleep(random.randint(10, 30))        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  fetch_abstracts(list_abstracts, output_file='bioasq_train_abstracts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_train = json.loads(open('./pubmedqa/data/pqal_fold0/train_set.json','r').read())\n",
    "json_test = json.loads(open('./pubmedqa/data/test_set.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in json_train:\n",
    "    print(json_train[entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_sd, list_train_rs = [], []\n",
    "for entry in json_train:\n",
    "    q,sd = json_train[entry]['QUESTION'], ' '.join(json_train[entry]['CONTEXTS'])\n",
    "    rs = json_train[entry]['LONG_ANSWER']\n",
    "    \n",
    "    list_train_sd.append(q+'\\n'+sd)\n",
    "    list_train_rs.append(rs)\n",
    "\n",
    "list_test_sd, list_test_rs = [], []\n",
    "for entry in json_test:\n",
    "    q,sd = json_test[entry]['QUESTION'], ' '.join(json_test[entry]['CONTEXTS'])\n",
    "    rs = json_test[entry]['LONG_ANSWER']\n",
    "    \n",
    "    list_test_sd.append(q+'\\n'+sd)\n",
    "    list_test_rs.append(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame({'inputs': list_train_sd, 'target': list_train_rs})\n",
    "df_test = pd.DataFrame({'inputs': list_test_sd, 'target': list_test_rs})\n",
    "\n",
    "df_train.to_csv('PubMedQA_train.csv', index=False)\n",
    "df_test.to_csv('PubMedQA_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_train = pd.read_json('',orient='records',lines=True)\n",
    "# df_test = pd.read_json('',orient='records',lines=True)\n",
    "\n",
    "import pandas as pd\n",
    "model_path = \"pritamdeka/PubMedBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "train_encodings = model.encode(df_train['inputs'].tolist(), convert_to_tensor=True)\n",
    "test_encodings = model.encode(df_test['inputs'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "train_encodings = train_encodings.detach().cpu().numpy()\n",
    "test_encodings = test_encodings.detach().cpu().numpy()\n",
    "\n",
    "import numpy as np\n",
    "dist_rep = np.zeros((test_encodings.shape[0],train_encodings.shape[0]))\n",
    "\n",
    "for idx1,entry in enumerate(test_encodings):\n",
    "    for idx2,train_vec in enumerate(train_encodings):\n",
    "        dist_rep[idx1,idx2] = np.linalg.norm(entry-train_vec)\n",
    "\n",
    "closest_neighbors = np.zeros((len(test_encodings),16))\n",
    "for idx in range(len(test_encodings)):\n",
    "    closest_neighbors[idx] = np.argsort(dist_rep[idx])[:16]\n",
    "    \n",
    "np.save('closest_neighbors_PubMedQA.npy', closest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_path = \"pritamdeka/PubMedBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "df_BioASQ = pd.read_json('./bioasq_train.json',orient='records',lines=True)\n",
    "text_PAC = open('../../../../TxtInputFiles/PAC_input.txt','r').readlines()\n",
    "text_BioASQ = df_BioASQ['inputs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pac_encodings = np.zeros((len(text_PAC),768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for idx in tqdm.tqdm(range(0,len(text_PAC),1024)):\n",
    "    if idx+1024 > len(text_PAC):\n",
    "        pac_encodings[idx:] = model.encode(text_PAC[idx:], convert_to_tensor=True).detach().cpu().numpy()\n",
    "    else:\n",
    "        pac_encodings[idx:idx+1024] = model.encode(text_PAC[idx:idx+1024], convert_to_tensor=True).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BioASQ_Encodings = model.encode(text_BioASQ, convert_to_tensor=True)\n",
    "BioASQ_Encodings = BioASQ_Encodings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pac_encodings = np.load('./PAC_Encodings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "dist_rep = np.zeros((len(text_PAC),len(text_BioASQ)))\n",
    "\n",
    "for idx1,entry in enumerate(pac_encodings):\n",
    "    if (idx1+1)%1000 == 0:\n",
    "        print(f'Completed {idx1+1} entries')\n",
    "    for idx2,train_vec in enumerate(BioASQ_Encodings):\n",
    "        dist_rep[idx1,idx2] = np.linalg.norm(entry-train_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_rep_mean = dist_rep.mean(axis=1)\n",
    "dist_rep_mean.shape\n",
    "\n",
    "sorted_indices = dist_rep_mean.argsort()[:50000]\n",
    "\n",
    "PAC_BioASQ = [text_PAC[idx] for idx in sorted_indices]\n",
    "with open('./BioASQ_PAC.txt','w') as f:\n",
    "    f.write(''.join(PAC_BioASQ))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(text_PAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_PAC = text_PAC[:50000]\n",
    "with open('./Random_PAC.txt','w') as f:\n",
    "    f.write(''.join(random_PAC))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string = \"_hello_hello_hello\"\n",
    "substring = \"_hello\"\n",
    "\n",
    "count = len(re.findall(f\"(?={substring})\", string))\n",
    "print(\"Number of overlapping occurrences:\", count)\n",
    "string.count(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
