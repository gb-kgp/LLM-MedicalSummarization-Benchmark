{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de46360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenization_llama_Lookup import LlamaTokenizer\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "tokenizer_MEDVOC = AutoTokenizer.from_pretrained('./Llama-2-7B-PubMedQA-MEDVOC/PubMedQA_Vocabs_MEDVOC_Llama2/25K_0.75_')\n",
    "tokenizer_MEDVOC_ABPE = LlamaTokenizer.from_pretrained('./Llama-2-7B-PubMedQA-MEDVOC/PubMedQA_Vocabs_MEDVOC_Llama2/25K_0.75_',\n",
    "                                                             added_vocab = './Llama-2-7B-PubMedQA-MEDVOC/PubMedQA_Vocabs_MEDVOC_Llama2/25K_0.75_/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "\n",
    "tokenizer_filter = AutoTokenizer.from_pretrained('./Llama-2-7B-PubMedQA-Filter/PubMedQA_Filter_Vocabs_Llama2/20K')\n",
    "\n",
    "tokenizer_filter_ABPE = LlamaTokenizer.from_pretrained('./Llama-2-7B-PubMedQA-Filter/PubMedQA_Filter_Vocabs_Llama2/20K',\n",
    "                                                             added_vocab = './Llama-2-7B-PubMedQA-Filter/PubMedQA_Filter_Vocabs_Llama2/20K/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "tokenizer_lookup = LlamaTokenizer.from_pretrained('./Llama-2-7B-PubMedQA-MedicalLookup-Fragment/PubMedQA_Lookup_Vocabs_SplitMoreThan1_Also_Add_As_Subwords_Llama2/PubMedQA_Lookup_100',\n",
    "                                                 added_vocab = './Llama-2-7B-PubMedQA-MedicalLookup-Fragment/PubMedQA_Lookup_Vocabs_SplitMoreThan1_Also_Add_As_Subwords_Llama2/PubMedQA_Lookup_100/added_vocab.txt',\n",
    "                                                 use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788695",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), len(tokenizer_MEDVOC),len(tokenizer_MEDVOC_ABPE),len(tokenizer_filter), len(tokenizer_filter_ABPE), len(tokenizer_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TGT  = pd.read_csv(\"../../LlaMa-2-7B/Llama-2-7B-PubMedQA-MedicalLookup-Fragment/PubMedQA_test.csv\")\n",
    "list_rs = df_TGT['target'].to_list()\n",
    "list_rs = [x+'||'+str(idx) for idx,x in enumerate(list_rs)]\n",
    "\n",
    "gs_base = open('./PubMedQA-Summaries/PubMedQA_50_4_Llama2_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_llama3 = open('./PubMedQA-Summaries/PubMedQA_50_1_Llama3.1_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_without_vocab = open('./PubMedQA-Summaries/PubMedQA_50_4_Llama2_CPT_WithoutVocab.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_medvoc = open('./PubMedQA-Summaries/PubMedQA_40_1__Llama2_MEDVOC_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_medvoc_adaptbpe = open('./PubMedQA-Summaries/PubMedQA_40_1__Llama2_MEDVOC_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_filtered_without_adaptbpe = open('./PubMedQA-Summaries/PubMedQA_40_1__Llama2_Filter_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_filtered_adaptbpe = open('./PubMedQA-Summaries/PubMedQA_40_1__Llama2_Filter_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_lookup = open('./PubMedQA-Summaries/PubMedQA_40_1_Llama2_Lookup100.txt','r').read().split('\\n++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_cmp = defaultdict(list)\n",
    "\n",
    "for gs,rs in zip(gs_base, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_llama3, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_without_vocab, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_without_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_lookup, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def RL(ref, hyp):\n",
    "    scores = scorer.score(ref,hyp)\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs,gss in dict_cmp.items():\n",
    "  try:\n",
    "    rl_base = RL(rs.split('||')[0], gss[0])\n",
    "    rl_llama3 = RL(rs.split('||')[0], gss[1])\n",
    "    rl_cpt_without_vocab = RL(rs.split('||')[0], gss[2])\n",
    "    \n",
    "    rl_cpt_medvoc = RL(rs.split('||')[0], gss[3])\n",
    "    rl_cpt_medvoc_adaptbpe = RL(rs.split('||')[0], gss[4])\n",
    "    \n",
    "    rl_cpt_filtered_without_adaptbpe = RL(rs.split('||')[0], gss[5])\n",
    "    rl_cpt_filtered_adaptbpe = RL(rs.split('||')[0], gss[6])\n",
    "    \n",
    "    rl_cpt_lookup = RL(rs.split('||')[0], gss[7])\n",
    "    \n",
    "    dict_cmp[rs].extend([rl_base,rl_llama3,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup])\n",
    "    \n",
    "  except: print(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_src = df_TGT['inputs'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src[0], gs_base[0], list_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                \n",
    "    return oov_frac/consider\n",
    "\n",
    "def Novel_frac(src,rs):\n",
    "    novel_frac = 0\n",
    "    novel_list = list()\n",
    "    for tok in rs.split():\n",
    "        if tok not in src: \n",
    "            novel_frac += 1\n",
    "            novel_list.append(tok)\n",
    "    \n",
    "    return novel_frac/len(rs.split()), novel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c918a-fd33-456a-9291-edc3a0cf137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_frac(tokenizer,list_rs[10]), list_rs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_src = defaultdict(list)\n",
    "for rs,src in zip(list_rs,list_src): dict_src[rs].append(src)\n",
    "\n",
    "list_del_key = list()\n",
    "for key,val in dict_src.items():\n",
    "    # print(key,val)\n",
    "    if len(val[0]) == 0: \n",
    "        list_del_key.append(key)\n",
    "        continue\n",
    "        \n",
    "    dict_src[key].extend(Novel_frac(val[0].lower(),key))\n",
    "    dict_src[key].append(OOV_frac(tokenizer,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_lookup,val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cc4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rl_base = list()\n",
    "list_rl_llama3 = list()\n",
    "list_rl_cpt_without_vocab = list()\n",
    "list_rl_cpt_medvoc = list()\n",
    "list_rl_cpt_medvoc_adaptbpe = list()\n",
    "list_rl_cpt_filtered_without_adaptbpe = list()\n",
    "list_rl_cpt_filtered_adaptbpe = list()\n",
    "list_rl_cpt_lookup = list()\n",
    "\n",
    "list_oov_src_base = list()\n",
    "list_oov_src_medvoc = list()\n",
    "list_oov_src_medvoc_adaptbpe = list()\n",
    "list_oov_src_filtered_without_adaptbpe = list()\n",
    "list_oov_src_filtered_adaptbpe = list()\n",
    "list_oov_src_lookup = list()\n",
    "\n",
    "list_oov_rs_base = list()\n",
    "list_oov_rs_medvoc = list()\n",
    "list_oov_rs_medvoc_adaptbpe = list()\n",
    "list_oov_rs_filtered_without_adaptbpe = list()\n",
    "list_oov_rs_filtered_adaptbpe = list()\n",
    "list_oov_rs_lookup = list()\n",
    "\n",
    "list_len_rs_base = list()\n",
    "list_len_rs_medvoc = list()\n",
    "list_len_rs_medvoc_adaptbpe = list()\n",
    "list_len_rs_filtered_without_adaptbpe = list()\n",
    "list_len_rs_filtered_adaptbpe = list()\n",
    "list_len_rs_lookup = list()\n",
    "\n",
    "list_novel_rs = list()\n",
    "\n",
    "list_gs_base = list()\n",
    "list_gs_llama3 = list()\n",
    "list_gs_cpt_without_vocab = list()\n",
    "list_gs_cpt_medvoc = list()\n",
    "list_gs_cpt_medvoc_adaptbpe = list()\n",
    "list_gs_cpt_filtered_without_adaptbpe = list()\n",
    "list_gs_cpt_filtered_adaptbpe = list()\n",
    "list_gs_cpt_lookup = list()\n",
    "\n",
    "list_rs = list()\n",
    "list_sd = list()\n",
    "\n",
    "for rs in dict_cmp:\n",
    "  try:\n",
    "    gen_base,gen_llama3,gen_cpt_without_vocab,gen_medvoc,gen_medvoc_adaptbpe,gen_cpt_filtered_without_adaptbpe,gen_cpt_filtered_adaptbpe,gen_cpt_lookup,rl_base,rl_llama3,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup = dict_cmp[rs]\n",
    "    \n",
    "    sd = dict_src[rs][0]\n",
    "    \n",
    "    len_rs_base = len(tokenizer.tokenize(rs))\n",
    "    len_rs_medvoc = len(tokenizer_MEDVOC.tokenize(rs))\n",
    "    len_rs_medvoc_adaptbpe = len(tokenizer_MEDVOC_ABPE.tokenize(rs))\n",
    "    len_rs_filtered_without_adaptbpe = len(tokenizer_filter.tokenize(rs))\n",
    "    len_rs_filtered_adaptbpe = len(tokenizer_filter_ABPE.tokenize(rs))\n",
    "    len_rs_lookup = len(tokenizer_lookup.tokenize(rs))\n",
    "\n",
    "    oov_sd_base = dict_src[rs][-6]\n",
    "    oov_sd_medvoc = dict_src[rs][-5]\n",
    "    oov_sd_medvoc_adaptbpe = dict_src[rs][-4]\n",
    "    oov_sd_filtered_without_adaptbpe = dict_src[rs][-3]\n",
    "    oov_sd_filtered_adaptbpe = dict_src[rs][-2]\n",
    "    oov_sd_lookup= dict_src[rs][-1]\n",
    "\n",
    "    oov_rs_base = OOV_frac(tokenizer,rs)\n",
    "    oov_rs_medvoc = OOV_frac(tokenizer_MEDVOC,rs)\n",
    "    oov_rs_medvoc_adaptbpe = OOV_frac(tokenizer_MEDVOC_ABPE,rs)\n",
    "    oov_rs_filtered_without_adaptbpe = OOV_frac(tokenizer_filter,rs)\n",
    "    oov_rs_filtered_adaptbpe = OOV_frac(tokenizer_filter_ABPE,rs)\n",
    "    oov_rs_lookup = OOV_frac(tokenizer_lookup,rs)\n",
    "    \n",
    "    novel_rs = dict_src[rs][1]\n",
    "    \n",
    "    list_rl_base.append(rl_base)\n",
    "    list_rl_llama3.append(rl_llama3)\n",
    "    list_rl_cpt_without_vocab.append(rl_cpt_without_vocab)\n",
    "    list_rl_cpt_medvoc.append(rl_cpt_medvoc)\n",
    "    list_rl_cpt_medvoc_adaptbpe.append(rl_cpt_medvoc_adaptbpe)\n",
    "    list_rl_cpt_filtered_without_adaptbpe.append(rl_cpt_filtered_without_adaptbpe)\n",
    "    list_rl_cpt_filtered_adaptbpe.append(rl_cpt_filtered_adaptbpe)\n",
    "    list_rl_cpt_lookup.append(rl_cpt_lookup)\n",
    "    \n",
    "    list_len_rs_base.append(len_rs_base)\n",
    "    list_len_rs_medvoc.append(len_rs_medvoc)\n",
    "    list_len_rs_medvoc_adaptbpe.append(len_rs_medvoc_adaptbpe)\n",
    "    list_len_rs_filtered_without_adaptbpe.append(len_rs_filtered_without_adaptbpe)\n",
    "    list_len_rs_filtered_adaptbpe.append(len_rs_filtered_adaptbpe)\n",
    "    list_len_rs_lookup.append(len_rs_lookup)\n",
    "    \n",
    "    list_gs_base.append(gen_base)\n",
    "    list_gs_llama3.append(gen_llama3)\n",
    "    list_gs_cpt_medvoc.append(gen_medvoc)\n",
    "    list_gs_cpt_medvoc_adaptbpe.append(gen_medvoc_adaptbpe)\n",
    "    list_gs_cpt_without_vocab.append(gen_cpt_without_vocab)\n",
    "    list_gs_cpt_filtered_without_adaptbpe.append(gen_cpt_filtered_without_adaptbpe)\n",
    "    list_gs_cpt_filtered_adaptbpe.append(gen_cpt_filtered_adaptbpe)\n",
    "    list_gs_cpt_lookup.append(gen_cpt_lookup)\n",
    "    \n",
    "    list_rs.append(rs)\n",
    "    list_sd.append(sd)\n",
    "    \n",
    "    list_novel_rs.append(novel_rs)\n",
    "\n",
    "    list_oov_rs_base.append(oov_rs_base)\n",
    "    list_oov_rs_medvoc.append(list_oov_rs_medvoc)\n",
    "    list_oov_rs_medvoc_adaptbpe.append(list_oov_rs_medvoc_adaptbpe)\n",
    "    list_oov_rs_filtered_without_adaptbpe.append(oov_rs_filtered_without_adaptbpe)\n",
    "    list_oov_rs_filtered_adaptbpe.append(oov_rs_filtered_adaptbpe)\n",
    "    list_oov_rs_lookup.append(oov_rs_lookup)\n",
    "\n",
    "    list_oov_src_base.append(oov_sd_base)\n",
    "    list_oov_src_medvoc.append(oov_sd_medvoc)\n",
    "    list_oov_src_medvoc_adaptbpe.append(oov_sd_medvoc_adaptbpe)\n",
    "    list_oov_src_filtered_without_adaptbpe.append(oov_sd_filtered_without_adaptbpe)\n",
    "    list_oov_src_filtered_adaptbpe.append(oov_sd_filtered_adaptbpe)\n",
    "    list_oov_src_lookup.append(oov_sd_lookup)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "#     print(rs)\n",
    "    print(len(dict_cmp[rs]), len(dict_src[rs]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'SD': list_sd,'RS':list_rs, \\\n",
    "                   'GS_Base': list_gs_base , 'GS_Llama3.1': list_gs_llama3, 'GS_CPT_Without_Vocab': list_gs_cpt_without_vocab, \\\n",
    "                   'GS_CPT_MedVoc': list_gs_cpt_medvoc, 'GS_CPT_MedVoc_AdaptBPE': list_gs_cpt_medvoc_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_WithoutAdaptBPE': list_gs_cpt_filtered_without_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_AdaptBPE': list_gs_cpt_filtered_adaptbpe, \\\n",
    "                   'GS_CPT_Lookup':list_gs_cpt_lookup, \\\n",
    "                   'RS_Len_Base': list_len_rs_base, 'RS_Len_Filtering_WithoutAdaptBPE': list_len_rs_filtered_without_adaptbpe, \\\n",
    "                   'RS_Len_MedVoc': list_len_rs_medvoc, 'RS_Len_MedVoc_AdaptBPE': list_len_rs_medvoc_adaptbpe, \\\n",
    "                   'RS_Len_Filtering_AdaptBPE': list_len_rs_filtered_adaptbpe, \\\n",
    "                   'RS_Len_Lookup' : list_len_rs_lookup, \\\n",
    "                   'OOV_RS_Base': list_oov_rs_base, 'OOV_RS_Filtering_WithoutAdaptBPE': list_oov_rs_filtered_without_adaptbpe, \\\n",
    "                   'OOV_RS_MedVoc': list_oov_rs_medvoc, 'OOV_RS_MedVoc_AdaptBPE': list_oov_rs_medvoc_adaptbpe, \\\n",
    "                   'OOV_RS_Filtering_AdaptBPE': list_oov_rs_filtered_adaptbpe, \\\n",
    "                   'OOV_RS_Lookup' : list_oov_rs_lookup, \\\n",
    "                   'Novel_RS': list_novel_rs, \\\n",
    "                   'OOV_SD_Base': list_oov_src_base, 'OOV_SD_Filtering_WithoutAdaptBPE': list_oov_src_filtered_without_adaptbpe, \\\n",
    "                   'OOV_SD_MedVoc': list_oov_src_medvoc, 'OOV_SD_MedVoc_AdaptBPE': list_oov_src_medvoc_adaptbpe, \\\n",
    "                   'OOV_SD_Filtering_AdaptBPE': list_oov_src_filtered_adaptbpe, \\\n",
    "                   'OOV_SD_Lookup' : list_oov_src_lookup, \\\n",
    "                   'R-L_Base': list_rl_base , 'R-L_Llama3.1': list_rl_llama3,'R-L_CPT_Without_Vocab': list_rl_cpt_without_vocab, \\\n",
    "                   'R-L_CPT_MedVoc': list_rl_cpt_medvoc, 'R-L_CPT_MedVoc_AdaptBPE': list_rl_cpt_medvoc_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_WithoutAdaptBPE': list_rl_cpt_filtered_without_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_AdaptBPE': list_rl_cpt_filtered_adaptbpe, \\\n",
    "                   'R-L_CPT_Lookup':list_rl_cpt_lookup\n",
    "                   })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dc0d-760a-46ca-a626-e7525d551d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08f7b-fb68-41ef-a236-92370936dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/path/to/umls'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_TGT_RS = defaultdict(list)\n",
    "\n",
    "lines_TGT = df['RS'].to_list()\n",
    "\n",
    "for idx,abs in enumerate(lines_TGT):\n",
    "    if idx%10 == 0: print(f'Processed till {idx+1}... ')\n",
    "\n",
    "    flag = 0\n",
    "    d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "    if len(d) == 0:\n",
    "        print('No match found for',idx,abs)\n",
    "        counter_TGT_RS[idx].append('happy')\n",
    "    \n",
    "    else:\n",
    "        for l in d:\n",
    "            counter_TGT_RS[idx].append(l[0]['ngram'])\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "counter_TGT_RS_Medical_Words = defaultdict(set)\n",
    "\n",
    "for key,val in counter_TGT_RS.items():\n",
    "    try:\n",
    "      print('---------')\n",
    "      for words in val:\n",
    "        for word in words.split():\n",
    "          print(word)\n",
    "          if re.match(pattern,word.strip()): counter_TGT_RS_Medical_Words[key].add(word.strip())\n",
    "    except: print(\"Error in\", key,val)\n",
    "\n",
    "for key,val in counter_TGT_RS_Medical_Words.items():\n",
    "    print(key,val)\n",
    "    print('-----------------')\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE.pkl','wb') as f:\n",
    "    pkl.dump(counter_TGT_RS_Medical_Words,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter_TGT_RS_Medical_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open('../Mistral-7b/PubMedQA-Summaries/PubMedQA_Mistral_MedicalWords_SD.pkl','rb') as f:\n",
    "    counter_TGT_RS_Medical_Words = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_medical_words_splitmorethan1 = list()\n",
    "list_medical_words_splitmorethan2 = list()\n",
    "list_medical_words_splitmorethan3 = list()\n",
    "\n",
    "\n",
    "for idx, words in counter_TGT_RS_Medical_Words.items():\n",
    "    total_words = len(words)\n",
    "    splitmorethan1 = 0\n",
    "    splitmorethan2 = 0\n",
    "    splitmorethan3 = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(tokenizer.tokenize(word)) > 1: splitmorethan1 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 2: splitmorethan2 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 3: splitmorethan3 += 1\n",
    "    \n",
    "    list_medical_words_splitmorethan1.append(splitmorethan1/total_words)\n",
    "    list_medical_words_splitmorethan2.append(splitmorethan2/total_words)\n",
    "    list_medical_words_splitmorethan3.append(splitmorethan3/total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medical_Words_SplitMoreThan1_SD'] = list_medical_words_splitmorethan1\n",
    "df['Medical_Words_SplitMoreThan2_SD'] = list_medical_words_splitmorethan2\n",
    "df['Medical_Words_SplitMoreThan3_SD'] = list_medical_words_splitmorethan3\n",
    "\n",
    "df.to_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4fe24-0242-4a59-b7fa-50be7b223156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv')\n",
    "df['Medical_Words_SplitMoreThan1_SD'].describe(percentiles=[0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dc85a-cd81-47d0-9c40-8a8f69d95d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1_SD'] >= 0.494156]\n",
    "df_desc = pd.DataFrame()\n",
    "for col in df_high_oov.columns:\n",
    "      if col.startswith('CSr') or col.startswith('R-L') : df_desc[col] = df_high_oov[col]\n",
    "df_desc.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019682e",
   "metadata": {},
   "source": [
    "## Roue-L\n",
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.274510\t0.253968    0.275862\t0.276923\t0.266667\t0.271186\t0.279070\t0.285714\t\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.243956\t0.266667    0.280000\t0.228758\t0.266691\t0.252660\t0.275862\t0.285714\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SDs\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.254545\t0.246622    0.278556\t0.245906\t0.246212\t0.252660\t0.233032\t0.258333\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.235294\t0.237288    0.259740\t0.243478\t0.245614\t0.244898\t0.240000\t0.263158\n",
    "\n",
    "High Novelty -RS\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.197802\t0.183960    0.222085\t0.201449\t0.195865\t0.200041\t0.216542\t0.222222\n",
    "\n",
    "Low Novelty --RS\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.365854\t0.303571    0.317460\t0.338983\t0.338028\t0.333333\t0.333333\t0.318841\n",
    "\n",
    "## Csr\n",
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.260870\t0.250000\t0.266667\t0.250000\t0.230769\t0.296296\t0.250000\t0.285714\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.226496\t0.228758\t0.250000\t0.235294\t0.235294\t0.237647\t0.205263\t0.258333\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.222222\t0.222222\t0.242647\t0.279221\t0.215385\t0.210526\t0.263768\t0.248082\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.222222\t0.210526\t0.235294\t0.250000\t0.250000\t0.210526\t0.250000\t0.235294\n",
    "\n",
    "High Novelty -RS\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.105263\t0.155405\t0.172161\t0.142857\t0.143590\t0.117647\t0.148352\t0.143590\n",
    "\n",
    "Low Novelty --CSr\n",
    "Base        Llama3.1    W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.421053\t0.341463\t0.400000\t0.380952\t0.333333\t0.428571\t0.416667\t0.428571\n",
    "\n",
    "Long RS (All)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.226415\t0.234043\t0.196721\t0.206897\t0.209524\t0.214286\t0.218182\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.255180\t0.267381\t0.245526\t0.265409\t0.254257\t0.242807\t0.250000\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.250000\t0.260870\t0.253968\t0.258065\t0.254902\t0.245614\t0.255814\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once) --SDs\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.232784\t0.261582\t0.250000\t0.251437\t0.249433\t0.231884\t0.241951\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice) --SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.240000\t0.253521\t0.246575\t0.253521\t0.240000\t0.242424\t0.260163\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RS_hHighOOV = df_high_oov[df_high_oov['RS_Len_Base']>=89]\n",
    "df_RS_hHighOOV.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69dacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Base'] >= 95]\n",
    "df_long_rs.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98948082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def getMatchInfo(list_matches):\n",
    "    dict_concepts = defaultdict(list)\n",
    "    for match_l in list_matches:\n",
    "        for match_l_d in match_l:\n",
    "            if match_l_d[\"preferred\"] == 1: \n",
    "                cui = match_l_d['cui'] #the concept-id\n",
    "                start = match_l_d['start'] \n",
    "                end = match_l_d['end']\n",
    "                n_gram = match_l_d['ngram'].strip() #the surface form\n",
    "\n",
    "                if '\\n' in n_gram : continue\n",
    "                key = str(start)+'_'+str(end)+'_'+n_gram\n",
    "\n",
    "                if not cui in dict_concepts[key]: dict_concepts[key].append(cui)\n",
    "                    \n",
    "    str_ret = str()\n",
    "    \n",
    "    for key,val in dict_concepts.items():\n",
    "        str_ret += str(key) + ':'\n",
    "        for c_name in val:\n",
    "            if not '\\n' in c_name:\n",
    "                str_ret += c_name + '|'\n",
    "        str_ret += '\\n'\n",
    "    return (str_ret)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "def CIEval(f_score):\n",
    "    dataset = np.array([x[1] for x in f_score])\n",
    "    max_l = len(dataset)\n",
    "    scores = list()\n",
    "    for _ in range(1000):\n",
    "        # bootstrap sample\n",
    "        indices = randint(0, max_l, max_l)\n",
    "        sample = dataset[indices]\n",
    "        # calculate and store statistic\n",
    "        statistic = mean(sample)\n",
    "        scores.append(statistic)\n",
    "\n",
    "    print('50th percentile (median) = %.4f' % median(scores))\n",
    "    # calculate 95% confidence intervals (100 - alpha)\n",
    "    alpha = 5.0\n",
    "    # calculate lower percentile (e.g. 2.5)\n",
    "    lower_p = alpha / 2.0\n",
    "    # retrieve observation at lower percentile\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "#     print('%.1fth percentile = %.4f' % (lower_p, lower))\n",
    "    # calculate upper percentile (e.g. 97.5)\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    # retrieve observation at upper percentile\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "#     print('%.1fth percentile = %.4f' % (upper_p, upper))\n",
    "    print('C.I. Window = %.4f '%max([upper-median(scores),median(scores)-lower]))\n",
    "    \n",
    "for column in ['GS_Base', 'GS_Llama3.1','GS_CPT_Without_Vocab', 'GS_CPT_MedVoc',\n",
    "       'GS_CPT_MedVoc_AdaptBPE', 'GS_CPT_Filtering_WithoutAdaptBPE',\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'GS_CPT_Lookup']:\n",
    "\n",
    "    print(f'--Processing {column}.....')\n",
    "\n",
    "    f_score = list()\n",
    "    for row_id in range(df.shape[0]):\n",
    "        try:                \n",
    "            dec_sum = df.iloc[row_id][column]\n",
    "            ref_sum = df.iloc[row_id]['RS']\n",
    "            ref_sum = ref_sum.split('||')[0]\n",
    "            \n",
    "            ref_con = getMatchInfo(matcher.match(ref_sum, best_match=True, ignore_syntax=False))\n",
    "            dec_con = getMatchInfo(matcher.match(dec_sum, best_match=True, ignore_syntax=False))\n",
    "            \n",
    "            # print(f'REF_Sum: {ref_sum}')\n",
    "            # print(f'DEC_Sum: {dec_sum}')\n",
    "\n",
    "            # print(check_overlap(ref_con,dec_con))\n",
    "            f_score.append((row_id,check_overlap(ref_con,dec_con)))\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "#         break\n",
    "#     break\n",
    "    df['CSr'+column[2:]] = [x[1] for x in f_score]\n",
    "    CIEval(f_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[ 'CI_Base', 'CI_CPT_Without_Vocab', 'CI_CPT_MedVoc',\n",
    "       'CI_CPT_MedVoc_AdaptBPE', 'CI_Llama3.1',\n",
    "       'CI_CPT_Filtering_WithoutAdaptBPE', 'CI_CPT_Filtering_AdaptBPE',\n",
    "       'CI_CPT_Lookup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87262a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./PubMedQA-Summaries/PubMedQA_Llama2_Compare_SelfAdaptBPE_MedicalWords_WithLlama3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9940e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1'] >=0.600000]\n",
    "df_desc = df_high_oov.drop(columns=['GS_Llama3.1' ,'GS_CPT_MedVoc',\n",
    "       'GS_CPT_MedVoc_AdaptBPE', 'GS_CPT_Filtering_WithoutAdaptBPE',\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'RS_Len_Base',\n",
    "       'RS_Len_Filtering_WithoutAdaptBPE', 'RS_Len_MedVoc',\n",
    "       'RS_Len_MedVoc_AdaptBPE', 'RS_Len_Filtering_AdaptBPE', 'RS_Len_Lookup',\n",
    "       'OOV_RS_Base', 'OOV_RS_Filtering_WithoutAdaptBPE', 'OOV_RS_MedVoc',\n",
    "       'OOV_RS_MedVoc_AdaptBPE', 'OOV_RS_Filtering_AdaptBPE', 'OOV_RS_Lookup',\n",
    "       'Novel_RS', 'OOV_SD_Base', 'OOV_SD_Filtering_WithoutAdaptBPE',\n",
    "       'OOV_SD_MedVoc', 'OOV_SD_MedVoc_AdaptBPE', 'OOV_SD_Filtering_AdaptBPE',\n",
    "       'OOV_SD_Lookup',\n",
    "       'Medical_Words_SplitMoreThan1', 'Medical_Words_SplitMoreThan2',\n",
    "       'Medical_Words_SplitMoreThan3', 'Medical_Words_SplitMoreThan1_SD',\n",
    "       'Medical_Words_SplitMoreThan2_SD', 'Medical_Words_SplitMoreThan3_SD'])\n",
    "df_desc.describe(percentiles=[0.5])\n",
    "df_desc = df_desc[df_desc['R-L_Base']<df_desc['R-L_CPT_Lookup']]\n",
    "df_desc = df_desc[df_desc['R-L_CPT_Without_Vocab']<df_desc['R-L_CPT_Lookup']]\n",
    "df_desc_greatest = df_desc.nlargest(int(20), 'R-L_CPT_Lookup')\n",
    "for idx,row in df_desc_greatest.iterrows():\n",
    "    print('------------------------'*8)\n",
    "    print('+++SD',row['SD'])\n",
    "    print('+++RS        :',row['RS'])\n",
    "    print('######################################')\n",
    "    print('**GS_Base    :',row['GS_Base'])\n",
    "    print('**GS_W/oVocab:',row['GS_CPT_Without_Vocab'])\n",
    "    print('**GS_Lookup  :',row['GS_CPT_Lookup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb325566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff81be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bb5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "print(wilcoxon(df_high_oov['R-L_CPT_Without_Vocab'].to_list(),df_high_oov['R-L_CPT_Lookup'].to_list(),alternative='less'), \\\n",
    "wilcoxon(df_low_oov['R-L_CPT_Without_Vocab'].to_list(),df_low_oov['R-L_CPT_Lookup'].to_list(),alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fff050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    x\n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                oov_list.append([token,tokenizer.tokenize(token)])\n",
    "    return oov_frac/consider, oov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157230",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "list_common_words_worst = []\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_worst = df_high_oov_worst.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE')\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               : ',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Filtering w/o AdaptBPE           :',OOV_frac(tokenizer_filtering,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words_worst.append(common_words)\n",
    "    print('-- Common words Frac                :', common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34241a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Lookup']>=0.27]\n",
    "df_high_oov.describe()\n",
    "\n",
    "df_long_rs = df[df['RS_Len_Lookup']>=93]\n",
    "df_long_rs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long-RS:\n",
    "Base > CPT_Filter_W/OABPE > CPT_W/O_Vocab > CPT_Lookup > CPT_Filter_AdaptBPE\n",
    "0.219003 > 0.213338 > 0.208825 > 0.205657 > 0.185030\n",
    "\n",
    "High-OOV:\n",
    "CPT_Lookup > Base > CPT_Filter_AdaptBPE > CPT_Filter_W/OABPE > CPT_W/O_Vocab\n",
    "0.263158 > 0.258094 > 0.255865 > 0.251984 > 0.245526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee666116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Filtering_WithoutAdaptBPE']>=0.25]\n",
    "df_high_oov.describe()\n",
    "\n",
    "High-OOV:\n",
    "Lookup25.59 > W/O_Vocab25.00 > Base24.83 > Filter_W/OABPE22.04 > Filter_ABPE20.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Filtering_WithoutAdaptBPE']>=89]\n",
    "df_long_rs.describe()\n",
    "\n",
    "Long-RS:\n",
    "Base20.64 > Lookup20.15 > W/O_Vocab19.33 > Filter_W/OABPE18.90 > Filter_ABPE18.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2068244",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Lookup']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Lookup']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_worst.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d35c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMLS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
