{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "tokenizer_base.save_pretrained(\"Llama-3.1-8B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "vocab_to_merges = defaultdict(list)\n",
    "for k,v in vocab_base.items():\n",
    "    if v < 256: continue\n",
    "    print('------------------------------')\n",
    "    for item in merges_base:\n",
    "        left,right = item.split()\n",
    "        if left+right == k:\n",
    "            print('++',k,item)\n",
    "            vocab_to_merges[k].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(vocab_to_merges, open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pickle as pkl\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "os.mkdir(\"EBM-Filter-FromScratch-Llama3_Vocab\")\n",
    "\n",
    "for fname in glob.glob(\"./EBM/*.txt\"):\n",
    "    print('***********Processing:',fname)\n",
    "    vocab_to_merges = defaultdict(list) #pkl.load(open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"rb\"))\n",
    "    \n",
    "    tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "    tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\",'r',encoding='utf-8'))\n",
    "    \n",
    "    words_to_add = open(fname,'r',encoding='utf-8').read().splitlines()\n",
    "    words_to_add = sorted(words_to_add, key=lambda x: len(x))\n",
    "\n",
    "    for word in words_to_add:\n",
    "        word = word.strip()\n",
    "        split = tokenizer_base.tokenize(word if not word.startswith('Ä ') else ' '+word[1:])\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        \n",
    "        if len(split) == 2: #pass\n",
    "            vocab_to_merges[word] = [split[0],split[1]]\n",
    "        \n",
    "        if len(split) >= 3:\n",
    "            # print('--word:',word,split)\n",
    "            new_word = split[0]\n",
    "            for i in range(1,len(split)):\n",
    "                left = new_word\n",
    "                right = split[i]\n",
    "                new_word += split[i]\n",
    "                # print('new_word:',new_word, 'Merge:',left,right)\n",
    "                if new_word not in vocab_to_merges: vocab_to_merges[new_word] = [left,right]\n",
    "    \n",
    "    idx = 0\n",
    "    for key,val in vocab_to_merges.items():\n",
    "        if key not in tokenizer_json[\"model\"][\"vocab\"]:\n",
    "        # print(key,val,idx)\n",
    "            print('Adding:',key,val)\n",
    "            tokenizer_json[\"model\"][\"vocab\"][key] = 128000+idx\n",
    "            tokenizer_json[\"model\"][\"merges\"].append(val)\n",
    "            idx += 1\n",
    "        \n",
    "    tokenizer_json['post_processor']['processors'][-1]['special_tokens']['<|begin_of_text|>']['ids'] = [128000+idx]\n",
    "    \n",
    "    dump_dir = f'EBM-Filter-FromScratch-Llama3_Vocab/EBM_{fname.split(\"/\")[-1][:-4]}'\n",
    "    tokenizer_base.save_pretrained(dump_dir)\n",
    "    \n",
    "    with open(dump_dir+'/tokenizer.json', 'w',encoding='utf-8') as f:\n",
    "        json.dump(tokenizer_json, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Llama-3-EBM-MedicalLookup-Fragment/EBM_SplitMoreThan1_OOV.csv')\n",
    "freq_ebm = df['Count'].to_list()\n",
    "terms_EBM = df['Word'].to_list()\n",
    "split_bart = df['Splits'].to_list()\n",
    "\n",
    "sum_num = 0.\n",
    "sum_den = 0.\n",
    "for idx,term in enumerate(terms_EBM):\n",
    "    sum_num += split_bart[idx]*freq_ebm[idx]\n",
    "    sum_den += freq_ebm[idx]\n",
    "\n",
    "old_score = sum_num/sum_den\n",
    "\n",
    "\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_scores = defaultdict(lambda : defaultdict(dict))\n",
    "for fname in sorted(glob.glob('./EBM-Filter-FromScratch-Llama3_Vocab/*') ,key = lambda x: int(x.split('/')[-1].split('_')[-1][:-1])):\n",
    "    print(fname)\n",
    "    try:\n",
    "        domain_tok = AutoTokenizer.from_pretrained(fname)\n",
    "    except Exception as e:\n",
    "        print('Error in loading:',fname, e)\n",
    "        continue\n",
    "    sum_num = 0.\n",
    "    sum_den = 0.\n",
    "    \n",
    "    for idx,term in enumerate(terms_EBM):\n",
    "        sum_num += min(len(domain_tok.tokenize(term)),len(domain_tok.tokenize(' '+term)))*freq_ebm[idx]\n",
    "        sum_den += freq_ebm[idx]\n",
    "\n",
    "    key = fname.split('/')[-1].split('_')[-1]\n",
    "    dict_scores[key] = [round(sum_num/sum_den,2),len(domain_tok)-128256]\n",
    "\n",
    "with open(f'EBM-FERTILITY','a') as f:\n",
    "    f.write(f'-------------\\nEBM-Filter-SelfFromScratch\\n--------------------\\n')\n",
    "    f.write('LLM_Tok: '+str(round(old_score,2))+'\\n')\n",
    "\n",
    "    for k1 in dict_scores:\n",
    "        f.write(k1+'\\t')\n",
    "        f.write(f'{dict_scores[k1][0]}/{dict_scores[k1][1]}\\t')\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "gain_in_fragments = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./EBM-Filter-FromScratch-Llama3_Vocab/EBM_20K/\")\n",
    "import numpy as np\n",
    "with open('../../../../../TxtInputFiles/PAC_input.txt') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            org_enc = tokenizer_base.encode(line)\n",
    "            vocab_enc = tokenizer.encode(line)\n",
    "            \n",
    "            org_dec = tokenizer_base.decode(org_enc)\n",
    "            vocab_dec = tokenizer.decode(vocab_enc)\n",
    "            \n",
    "            assert org_dec == vocab_dec, f'Failed at {idx}'\n",
    "            \n",
    "            gain_in_fragments.append((len(org_enc)-len(vocab_enc))/len(org_enc))\n",
    "            \n",
    "            idx += 1\n",
    "            if idx%1000 == 0:\n",
    "                print(f'Processed {idx}.... {np.percentile(gain_in_fragments, [0,10,50,90,100])}')\n",
    "        except:\n",
    "            print(f'--------------------Failed at {idx}--------------------')\n",
    "            print(f'Orig : {org_dec}')\n",
    "            print(f'Vocab: {vocab_dec}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(gain_in_fragments, [0,10,50,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"./EBM-Filter-FromScratch-Llama3_Vocab/EBM_20K/\")\n",
    "source_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_multiple(vocabulary_size, multiple):\n",
    "    rounded_size = math.ceil(vocabulary_size / multiple) * multiple\n",
    "    return rounded_size\n",
    "\n",
    "def instantiate_model_by_mean(\n",
    "    source_model: AutoModelForCausalLM,\n",
    "    source_tokenizer: AutoTokenizer,\n",
    "    target_tokenizer: AutoTokenizer,\n",
    "    tie_word_embeddings: bool = False\n",
    "):\n",
    "    # Determine the device (GPU or CPU)\n",
    "    print(\"Inside instantiate model by mean\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize source embeddings\n",
    "    source_embeddings = source_model.get_input_embeddings().weight#.detach().to(device)\n",
    "    target_embeddings = torch.zeros(\n",
    "        (round_to_nearest_multiple(len(target_tokenizer), 8), \n",
    "         source_embeddings.shape[1]), device=device\n",
    "    )\n",
    "    \n",
    "    #PLM-Part\n",
    "    target_embeddings[:128000] = source_embeddings[:128000]\n",
    "    #Reserved Tokens\n",
    "    for idx in range(target_tokenizer.bos_token_id,len(target_tokenizer)):\n",
    "        target_embeddings[idx] = source_embeddings[source_tokenizer.convert_tokens_to_ids(target_tokenizer.convert_ids_to_tokens(idx))]\n",
    "\n",
    "    if not tie_word_embeddings:\n",
    "        print(\"You are using the output projection init.\")\n",
    "        source_head_embeddings = source_model.get_output_embeddings()#.weight.detach().to(device)\n",
    "        target_head_embeddings = torch.zeros(\n",
    "            (round_to_nearest_multiple(len(target_tokenizer), 8), \n",
    "             source_head_embeddings.shape[1]), device=device\n",
    "        )\n",
    "        \n",
    "        #PLM-Part\n",
    "        target_head_embeddings[:128000] = source_head_embeddings[:128000]\n",
    "        #Reserved Tokens        \n",
    "        for idx in range(target_tokenizer.bos_token_id,len(target_tokenizer)):\n",
    "            target_head_embeddings[idx] = source_head_embeddings[source_tokenizer.convert_tokens_to_ids(target_tokenizer.convert_ids_to_tokens(idx))]\n",
    "\n",
    "    # Initialize the rest of the embeddings\n",
    "    for i in range(len(source_tokenizer)-256, len(target_tokenizer)-256):\n",
    "        token = target_tokenizer.convert_ids_to_tokens(i)\n",
    "        if token.startswith('Ä '): token = ' '+token[1:]\n",
    "        source_ids = source_tokenizer.convert_tokens_to_ids(source_tokenizer.tokenize(token))\n",
    "        source_ids = torch.tensor(source_ids)#, device=device)\n",
    "        target_embeddings[i] = source_embeddings[source_ids].mean(dim=0)\n",
    "        print(i,token, target_embeddings[i], source_ids)\n",
    "        if not tie_word_embeddings:\n",
    "            target_head_embeddings[i] = source_head_embeddings[source_ids].mean(dim=0)\n",
    "\n",
    "    # Expand the embeddings\n",
    "    target_model = source_model #AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    target_model.resize_token_embeddings(\n",
    "        len(target_tokenizer), \n",
    "        pad_to_multiple_of=8  # See https://github.com/huggingface/transformers/issues/26303\n",
    "    )\n",
    "    target_model.get_input_embeddings().weight.data = target_embeddings\n",
    "    target_model.config.vocab_size = round_to_nearest_multiple(len(target_tokenizer), 8)\n",
    "    \n",
    "    if not tie_word_embeddings:\n",
    "        target_model.get_output_embeddings().weight.data = target_head_embeddings\n",
    "\n",
    "    return target_model, target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "old_embeddings = source_model.get_input_embeddings().weight[source_tokenizer.bos_token_id:].detach().cpu().numpy().copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model, target_tokenizer = instantiate_model_by_mean(source_model,source_tokenizer,target_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_tokenizer), len(source_tokenizer), target_tokenizer.bos_token_id, source_tokenizer.bos_token_id, len(source_model.get_input_embeddings().weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = target_model.get_input_embeddings().weight[target_tokenizer.bos_token_id:len(target_tokenizer)].detach().cpu().numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "for v1,v2 in zip(old_embeddings,new_embeddings):\n",
    "    # s_id = source_tokenizer.convert_tokens_to_ids(target_tokenizer.convert_ids_to_tokens(idx))\n",
    "    assert numpy.all(v1 == v2), f'Failed at {v1},{v2}'\n",
    "    # print(f'Passed at {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "\n",
    "vocab_tokens = [x.strip() for x in open('./EBM/20K.txt').read().splitlines()]\n",
    "\n",
    "max_token, max_token_to_add = '',0\n",
    "for word in vocab_tokens:\n",
    "    tokens_to_add = len(tokenizer.tokenize(word if not word.startswith('Ä ') else ' '+word[1:]))-2\n",
    "    if tokens_to_add > max_token_to_add:\n",
    "        max_token = word\n",
    "        max_token_to_add = tokens_to_add\n",
    "        print(word,tokens_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(' hypercholesterolemia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers_LLama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
