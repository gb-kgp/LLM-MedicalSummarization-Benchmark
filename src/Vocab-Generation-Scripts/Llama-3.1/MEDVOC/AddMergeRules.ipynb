{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "tokenizer_base.save_pretrained(\"Llama-3.1-8B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "vocab_to_merges = defaultdict(list)\n",
    "for k,v in vocab_base.items():\n",
    "    if v < 256: continue\n",
    "    print('------------------------------')\n",
    "    for item in merges_base:\n",
    "        left,right = item.split()\n",
    "        if left+right == k:\n",
    "            print('++',k,item)\n",
    "            vocab_to_merges[k].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(vocab_to_merges, open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pickle as pkl\n",
    "import json\n",
    "from collections import defaultdict\n",
    "os.mkdir(\"EBM-MEDVOC-FromScratch-Llama3_Vocab\")\n",
    "\n",
    "for fname in glob.glob(\"./EBM/*.txt\"):\n",
    "    print('***********Processing:',fname)\n",
    "    vocab_to_merges = defaultdict(list) #pkl.load(open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"rb\"))\n",
    "    \n",
    "    tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "    \n",
    "    tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\", 'r', encoding='utf-8'))\n",
    "    vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "    merges_base = tokenizer_json[\"model\"][\"merges\"]\n",
    "    \n",
    "    words_to_add = open(fname,'r',encoding='utf-8').read().splitlines()\n",
    "    words_to_add = sorted(words_to_add, key=lambda x: len(x))\n",
    "\n",
    "    for word in words_to_add:\n",
    "        split = tokenizer_base.tokenize(word if not word.startswith('Ġ') else ' '+word[1:])\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        \n",
    "        if len(split) == 2: #pass\n",
    "            vocab_to_merges[word] = [split[0],split[1]]\n",
    "        \n",
    "        if len(split) >= 3:\n",
    "            # print('--word:',word,split)\n",
    "            new_word = split[0]\n",
    "            for i in range(1,len(split)):\n",
    "                left = new_word\n",
    "                right = split[i]\n",
    "                new_word += split[i]\n",
    "                # print('new_word:',new_word, 'Merge:',left,right)\n",
    "                if new_word not in vocab_to_merges:\n",
    "                    vocab_to_merges[new_word] = [left,right]\n",
    "    \n",
    "    idx = 0\n",
    "    for key,val in vocab_to_merges.items():\n",
    "        if key not in tokenizer_json[\"model\"][\"vocab\"]:\n",
    "        # print(key,val,idx)\n",
    "            tokenizer_json[\"model\"][\"vocab\"][key] = 128000+idx\n",
    "            tokenizer_json[\"model\"][\"merges\"].append(val)\n",
    "            idx += 1\n",
    "        \n",
    "    tokenizer_json['post_processor']['processors'][-1]['special_tokens']['<|begin_of_text|>']['ids'] = [128000+idx]\n",
    "    \n",
    "    dump_dir = f'EBM-FromScratch-Llama3_Vocab/EBM_{fname.split(\"/\")[-1][:-4]}'\n",
    "    tokenizer_base.save_pretrained(dump_dir)\n",
    "    \n",
    "    with open(dump_dir+'/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer_json, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Llama-3-EBM-MedicalLookup-Fragment/EBM_SplitMoreThan1_OOV.csv')\n",
    "freq_ebm = df['Count'].to_list()\n",
    "terms_EBM = df['Word'].to_list()\n",
    "split_bart = df['Splits'].to_list()\n",
    "\n",
    "sum_num = 0.\n",
    "sum_den = 0.\n",
    "for idx,term in enumerate(terms_EBM):\n",
    "    sum_num += split_bart[idx]*freq_ebm[idx]\n",
    "    sum_den += freq_ebm[idx]\n",
    "\n",
    "old_score = sum_num/sum_den\n",
    "\n",
    "\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_scores = defaultdict(lambda : defaultdict(dict))\n",
    "for fname in sorted(glob.glob('./EBM-MEDVOC-FromScratch-Llama3_Vocab/*') ,key = lambda x: [int(x.split('/')[-1].split('_')[-3][:-1]),float(x.split('/')[-1].split('_')[-2])]):\n",
    "    print('Processing:',fname)\n",
    "    # if 'BioASQ_0K_0.0_' in fname: continue\n",
    "    domain_tok = AutoTokenizer.from_pretrained(fname)\n",
    "    sum_num = 0.\n",
    "    sum_den = 0.\n",
    "    \n",
    "    for idx,term in enumerate(terms_EBM):\n",
    "        sum_num += min(len(domain_tok.tokenize(term)),len(domain_tok.tokenize(' '+term)))*freq_ebm[idx]\n",
    "        sum_den += freq_ebm[idx]\n",
    "\n",
    "    key = fname.split('/')[-1].split('_')\n",
    "    dict_scores[key[-3]][key[-2]] = [round(sum_num/sum_den,2),len(domain_tok)-128256]\n",
    "\n",
    "with open(f'EBM-FERTILITY','a') as f:\n",
    "    f.write(f'-------------\\nEBM-MEDVOC-SelfFromScratch\\n--------------------\\n')\n",
    "    f.write('BART_Tok: '+str(round(old_score,2))+'\\n')\n",
    "    for k1 in dict_scores:\n",
    "        if k1 == '0K': continue\n",
    "        f.write('data\\t')\n",
    "        for k2 in dict_scores[k1]:\n",
    "            f.write(k2+'\\t')\n",
    "        f.write('\\n')\n",
    "        break\n",
    "\n",
    "    for k1 in dict_scores:\n",
    "        f.write(k1+'\\t')\n",
    "        for k2 in dict_scores[k1]:\n",
    "            f.write(f'{dict_scores[k1][k2][0]}/{dict_scores[k1][k2][1]}\\t')\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "gain_in_fragments = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./BioASQ-FromScratch-Llama3_Vocab/BioASQ_10K_1.0_\")\n",
    "import numpy as np\n",
    "with open('../../../../../TxtInputFiles/PAC_input.txt') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            org_enc = tokenizer_base.encode(line)\n",
    "            vocab_enc = tokenizer.encode(line)\n",
    "            \n",
    "            org_dec = tokenizer_base.decode(org_enc)\n",
    "            vocab_dec = tokenizer.decode(vocab_enc)\n",
    "            \n",
    "            assert org_dec == vocab_dec, f'Failed at {idx}'\n",
    "            \n",
    "            gain_in_fragments.append((len(org_enc)-len(vocab_enc))/len(org_enc))\n",
    "            \n",
    "            idx += 1\n",
    "            if idx%1000 == 0:\n",
    "                print(f'Processed {idx}.... {np.percentile(gain_in_fragments, [0,10,50,90,100])}')\n",
    "        except:\n",
    "            print(f'--------------------Failed at {idx}--------------------')\n",
    "            print(f'Orig : {org_dec}')\n",
    "            print(f'Vocab: {vocab_dec}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(gain_in_fragments, [0,10,50,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\\u00c4\\u00aa\\u00c4\\u00a8\"\n",
    "print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "\n",
    "vocab_tokens = [x.strip() for x in open('./EBM/25K_0.5_.txt').read().splitlines()]\n",
    "\n",
    "max_token, max_token_to_add = '',0\n",
    "for word in vocab_tokens:\n",
    "    tokens_to_add = len(tokenizer.tokenize(word if not word.startswith('Ġ') else ' '+word[1:]))-2\n",
    "    if tokens_to_add > max_token_to_add:\n",
    "        max_token = word\n",
    "        max_token_to_add = tokens_to_add\n",
    "        print(word,tokens_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers_LLama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
