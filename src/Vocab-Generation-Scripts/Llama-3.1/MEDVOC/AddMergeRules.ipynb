{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunjanbalde/miniconda3/envs/env_transformers_LLama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B-Base/tokenizer_config.json',\n",
       " 'Llama-3.1-8B-Base/special_tokens_map.json',\n",
       " 'Llama-3.1-8B-Base/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "tokenizer_base.save_pretrained(\"Llama-3.1-8B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "vocab_to_merges = defaultdict(list)\n",
    "for k,v in vocab_base.items():\n",
    "    if v < 256: continue\n",
    "    print('------------------------------')\n",
    "    for item in merges_base:\n",
    "        left,right = item.split()\n",
    "        if left+right == k:\n",
    "            print('++',k,item)\n",
    "            vocab_to_merges[k].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(vocab_to_merges, open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\"))\n",
    "vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges_base = tokenizer_json[\"model\"][\"merges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Processing: ./EBM/20K_1.0_.txt\n",
      "***********Processing: ./EBM/45K_0.25_.txt\n",
      "***********Processing: ./EBM/55K_0.0_.txt\n",
      "***********Processing: ./EBM/10K_0.25_.txt\n",
      "***********Processing: ./EBM/5K_0.0_.txt\n",
      "***********Processing: ./EBM/45K_0.5_.txt\n",
      "***********Processing: ./EBM/125K_1.0_.txt\n",
      "***********Processing: ./EBM/60K_0.0_.txt\n",
      "***********Processing: ./EBM/105K_0.25_.txt\n",
      "***********Processing: ./EBM/15K_1.0_.txt\n",
      "***********Processing: ./EBM/110K_1.0_.txt\n",
      "***********Processing: ./EBM/70K_0.5_.txt\n",
      "***********Processing: ./EBM/75K_0.75_.txt\n",
      "***********Processing: ./EBM/80K_0.25_.txt\n",
      "***********Processing: ./EBM/20K_0.75_.txt\n",
      "***********Processing: ./EBM/95K_0.5_.txt\n",
      "***********Processing: ./EBM/85K_0.0_.txt\n",
      "***********Processing: ./EBM/85K_0.25_.txt\n",
      "***********Processing: ./EBM/70K_0.75_.txt\n",
      "***********Processing: ./EBM/25K_0.75_.txt\n",
      "***********Processing: ./EBM/95K_1.0_.txt\n",
      "***********Processing: ./EBM/130K_0.75_.txt\n",
      "***********Processing: ./EBM/15K_0.5_.txt\n",
      "***********Processing: ./EBM/100K_0.0_.txt\n",
      "***********Processing: ./EBM/70K_1.0_.txt\n",
      "***********Processing: ./EBM/40K_0.25_.txt\n",
      "***********Processing: ./EBM/110K_0.5_.txt\n",
      "***********Processing: ./EBM/15K_0.25_.txt\n",
      "***********Processing: ./EBM/20K_0.5_.txt\n",
      "***********Processing: ./EBM/125K_0.5_.txt\n",
      "***********Processing: ./EBM/45K_1.0_.txt\n",
      "***********Processing: ./EBM/100K_0.25_.txt\n",
      "***********Processing: ./EBM/30K_0.0_.txt\n",
      "***********Processing: ./EBM/40K_0.0_.txt\n",
      "***********Processing: ./EBM/130K_0.25_.txt\n",
      "***********Processing: ./EBM/35K_1.0_.txt\n",
      "***********Processing: ./EBM/130K_1.0_.txt\n",
      "***********Processing: ./EBM/50K_0.5_.txt\n",
      "***********Processing: ./EBM/70K_0.25_.txt\n",
      "***********Processing: ./EBM/85K_0.75_.txt\n",
      "***********Processing: ./EBM/75K_0.0_.txt\n",
      "***********Processing: ./EBM/25K_0.25_.txt\n",
      "***********Processing: ./EBM/65K_0.5_.txt\n",
      "***********Processing: ./EBM/105K_1.0_.txt\n",
      "***********Processing: ./EBM/80K_0.5_.txt\n",
      "***********Processing: ./EBM/100K_0.75_.txt\n",
      "***********Processing: ./EBM/90K_0.0_.txt\n",
      "***********Processing: ./EBM/40K_0.75_.txt\n",
      "***********Processing: ./EBM/15K_0.75_.txt\n",
      "***********Processing: ./EBM/105K_0.75_.txt\n",
      "***********Processing: ./EBM/80K_1.0_.txt\n",
      "***********Processing: ./EBM/45K_0.75_.txt\n",
      "***********Processing: ./EBM/10K_0.75_.txt\n",
      "***********Processing: ./EBM/115K_0.0_.txt\n",
      "***********Processing: ./EBM/105K_0.5_.txt\n",
      "***********Processing: ./EBM/65K_1.0_.txt\n",
      "***********Processing: ./EBM/10K_0.0_.txt\n",
      "***********Processing: ./EBM/35K_0.5_.txt\n",
      "***********Processing: ./EBM/120K_0.0_.txt\n",
      "***********Processing: ./EBM/80K_0.75_.txt\n",
      "***********Processing: ./EBM/25K_0.0_.txt\n",
      "***********Processing: ./EBM/75K_0.25_.txt\n",
      "***********Processing: ./EBM/50K_1.0_.txt\n",
      "***********Processing: ./EBM/130K_0.5_.txt\n",
      "***********Processing: ./EBM/20K_0.25_.txt\n",
      "***********Processing: ./EBM/110K_0.25_.txt\n",
      "***********Processing: ./EBM/85K_1.0_.txt\n",
      "***********Processing: ./EBM/50K_0.25_.txt\n",
      "***********Processing: ./EBM/15K_0.0_.txt\n",
      "***********Processing: ./EBM/60K_1.0_.txt\n",
      "***********Processing: ./EBM/100K_0.5_.txt\n",
      "***********Processing: ./EBM/120K_0.75_.txt\n",
      "***********Processing: ./EBM/110K_0.0_.txt\n",
      "***********Processing: ./EBM/5K_1.0_.txt\n",
      "***********Processing: ./EBM/55K_1.0_.txt\n",
      "***********Processing: ./EBM/20K_0.0_.txt\n",
      "***********Processing: ./EBM/35K_0.75_.txt\n",
      "***********Processing: ./EBM/125K_0.0_.txt\n",
      "***********Processing: ./EBM/60K_0.75_.txt\n",
      "***********Processing: ./EBM/30K_0.5_.txt\n",
      "***********Processing: ./EBM/95K_0.25_.txt\n",
      "***********Processing: ./EBM/125K_0.75_.txt\n",
      "***********Processing: ./EBM/55K_0.5_.txt\n",
      "***********Processing: ./EBM/5K_0.5_.txt\n",
      "***********Processing: ./EBM/30K_1.0_.txt\n",
      "***********Processing: ./EBM/45K_0.0_.txt\n",
      "***********Processing: ./EBM/30K_0.75_.txt\n",
      "***********Processing: ./EBM/100K_1.0_.txt\n",
      "***********Processing: ./EBM/90K_0.25_.txt\n",
      "***********Processing: ./EBM/60K_0.5_.txt\n",
      "***********Processing: ./EBM/65K_0.75_.txt\n",
      "***********Processing: ./EBM/70K_0.0_.txt\n",
      "***********Processing: ./EBM/115K_0.25_.txt\n",
      "***********Processing: ./EBM/95K_0.0_.txt\n",
      "***********Processing: ./EBM/85K_0.5_.txt\n",
      "***********Processing: ./EBM/5K_0.75_.txt\n",
      "***********Processing: ./EBM/55K_0.25_.txt\n",
      "***********Processing: ./EBM/30K_0.25_.txt\n",
      "***********Processing: ./EBM/65K_0.25_.txt\n",
      "***********Processing: ./EBM/90K_0.75_.txt\n",
      "***********Processing: ./EBM/125K_0.25_.txt\n",
      "***********Processing: ./EBM/90K_1.0_.txt\n",
      "***********Processing: ./EBM/115K_0.5_.txt\n",
      "***********Processing: ./EBM/75K_1.0_.txt\n",
      "***********Processing: ./EBM/105K_0.0_.txt\n",
      "***********Processing: ./EBM/5K_0.25_.txt\n",
      "***********Processing: ./EBM/55K_0.75_.txt\n",
      "***********Processing: ./EBM/10K_0.5_.txt\n",
      "***********Processing: ./EBM/35K_0.0_.txt\n",
      "***********Processing: ./EBM/40K_1.0_.txt\n",
      "***********Processing: ./EBM/120K_0.5_.txt\n",
      "***********Processing: ./EBM/25K_0.5_.txt\n",
      "***********Processing: ./EBM/115K_0.75_.txt\n",
      "***********Processing: ./EBM/130K_0.0_.txt\n",
      "***********Processing: ./EBM/120K_1.0_.txt\n",
      "***********Processing: ./EBM/40K_0.5_.txt\n",
      "***********Processing: ./EBM/50K_0.75_.txt\n",
      "***********Processing: ./EBM/0K_0.0_.txt\n",
      "***********Processing: ./EBM/50K_0.0_.txt\n",
      "***********Processing: ./EBM/25K_1.0_.txt\n",
      "***********Processing: ./EBM/110K_0.75_.txt\n",
      "***********Processing: ./EBM/75K_0.5_.txt\n",
      "***********Processing: ./EBM/115K_1.0_.txt\n",
      "***********Processing: ./EBM/10K_1.0_.txt\n",
      "***********Processing: ./EBM/65K_0.0_.txt\n",
      "***********Processing: ./EBM/80K_0.0_.txt\n",
      "***********Processing: ./EBM/35K_0.25_.txt\n",
      "***********Processing: ./EBM/95K_0.75_.txt\n",
      "***********Processing: ./EBM/60K_0.25_.txt\n",
      "***********Processing: ./EBM/90K_0.5_.txt\n",
      "***********Processing: ./EBM/120K_0.25_.txt\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pickle as pkl\n",
    "import json\n",
    "from collections import defaultdict\n",
    "os.mkdir(\"EBM-MEDVOC-FromScratch-Llama3_Vocab\")\n",
    "\n",
    "for fname in glob.glob(\"./EBM/*.txt\"):\n",
    "    print('***********Processing:',fname)\n",
    "    vocab_to_merges = defaultdict(list) #pkl.load(open(\"./Llama-3.1-8B-Base/vocab_to_merges_MAPPING.pkl\", \"rb\"))\n",
    "    \n",
    "    tokenizer_base = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "    \n",
    "    tokenizer_json = json.load(open(\"Llama-3.1-8B-Base/tokenizer.json\", 'r', encoding='utf-8'))\n",
    "    vocab_base = tokenizer_json[\"model\"][\"vocab\"]\n",
    "    merges_base = tokenizer_json[\"model\"][\"merges\"]\n",
    "    \n",
    "    words_to_add = open(fname,'r',encoding='utf-8').read().splitlines()\n",
    "    words_to_add = sorted(words_to_add, key=lambda x: len(x))\n",
    "\n",
    "    for word in words_to_add:\n",
    "        split = tokenizer_base.tokenize(word if not word.startswith('Ġ') else ' '+word[1:])\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        \n",
    "        if len(split) == 2: #pass\n",
    "            vocab_to_merges[word] = [split[0],split[1]]\n",
    "        \n",
    "        if len(split) >= 3:\n",
    "            # print('--word:',word,split)\n",
    "            new_word = split[0]\n",
    "            for i in range(1,len(split)):\n",
    "                left = new_word\n",
    "                right = split[i]\n",
    "                new_word += split[i]\n",
    "                # print('new_word:',new_word, 'Merge:',left,right)\n",
    "                if new_word not in vocab_to_merges:\n",
    "                    vocab_to_merges[new_word] = [left,right]\n",
    "    \n",
    "    idx = 0\n",
    "    for key,val in vocab_to_merges.items():\n",
    "        if key not in tokenizer_json[\"model\"][\"vocab\"]:\n",
    "        # print(key,val,idx)\n",
    "            tokenizer_json[\"model\"][\"vocab\"][key] = 128000+idx\n",
    "            tokenizer_json[\"model\"][\"merges\"].append(val)\n",
    "            idx += 1\n",
    "        \n",
    "    tokenizer_json['post_processor']['processors'][-1]['special_tokens']['<|begin_of_text|>']['ids'] = [128000+idx]\n",
    "    \n",
    "    dump_dir = f'EBM-FromScratch-Llama3_Vocab/EBM_{fname.split(\"/\")[-1][:-4]}'\n",
    "    tokenizer_base.save_pretrained(dump_dir)\n",
    "    \n",
    "    with open(dump_dir+'/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer_json, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_0K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_5K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_5K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_5K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_5K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_5K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_10K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_10K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_10K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_10K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_10K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_15K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_15K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_15K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_15K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_15K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_20K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_20K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_20K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_20K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_20K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_25K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_25K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_25K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_25K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_25K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_30K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_30K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_30K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_30K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_30K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_35K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_35K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_35K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_35K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_35K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_40K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_40K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_40K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_40K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_40K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_45K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_45K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_45K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_45K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_45K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_50K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_50K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_50K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_50K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_50K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_55K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_55K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_55K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_55K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_55K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_60K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_60K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_60K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_60K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_60K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_65K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_65K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_65K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_65K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_65K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_70K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_70K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_70K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_70K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_70K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_75K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_75K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_75K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_75K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_75K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_80K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_80K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_80K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_80K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_80K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_85K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_85K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_85K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_85K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_85K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_90K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_90K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_90K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_90K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_90K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_95K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_95K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_95K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_95K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_95K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_100K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_100K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_100K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_100K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_100K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_105K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_105K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_105K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_105K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_105K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_110K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_110K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_110K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_110K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_110K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_115K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_115K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_115K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_115K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_115K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_120K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_120K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_120K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_120K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_120K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_125K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_125K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_125K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_125K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_125K_1.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_130K_0.0_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_130K_0.25_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_130K_0.5_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_130K_0.75_\n",
      "Processing: ./EBM-MEDVOC-FromScratch-Llama3_Vocab/EBM_130K_1.0_\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Llama-3-EBM-MedicalLookup-Fragment/EBM_SplitMoreThan1_OOV.csv')\n",
    "freq_ebm = df['Count'].to_list()\n",
    "terms_EBM = df['Word'].to_list()\n",
    "split_bart = df['Splits'].to_list()\n",
    "\n",
    "sum_num = 0.\n",
    "sum_den = 0.\n",
    "for idx,term in enumerate(terms_EBM):\n",
    "    sum_num += split_bart[idx]*freq_ebm[idx]\n",
    "    sum_den += freq_ebm[idx]\n",
    "\n",
    "old_score = sum_num/sum_den\n",
    "\n",
    "\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_scores = defaultdict(lambda : defaultdict(dict))\n",
    "for fname in sorted(glob.glob('./EBM-MEDVOC-FromScratch-Llama3_Vocab/*') ,key = lambda x: [int(x.split('/')[-1].split('_')[-3][:-1]),float(x.split('/')[-1].split('_')[-2])]):\n",
    "    print('Processing:',fname)\n",
    "    # if 'BioASQ_0K_0.0_' in fname: continue\n",
    "    domain_tok = AutoTokenizer.from_pretrained(fname)\n",
    "    sum_num = 0.\n",
    "    sum_den = 0.\n",
    "    \n",
    "    for idx,term in enumerate(terms_EBM):\n",
    "        sum_num += min(len(domain_tok.tokenize(term)),len(domain_tok.tokenize(' '+term)))*freq_ebm[idx]\n",
    "        sum_den += freq_ebm[idx]\n",
    "\n",
    "    key = fname.split('/')[-1].split('_')\n",
    "    dict_scores[key[-3]][key[-2]] = [round(sum_num/sum_den,2),len(domain_tok)-128256]\n",
    "\n",
    "with open(f'EBM-FERTILITY','a') as f:\n",
    "    f.write(f'-------------\\nEBM-MEDVOC-SelfFromScratch\\n--------------------\\n')\n",
    "    f.write('BART_Tok: '+str(round(old_score,2))+'\\n')\n",
    "    for k1 in dict_scores:\n",
    "        if k1 == '0K': continue\n",
    "        f.write('data\\t')\n",
    "        for k2 in dict_scores[k1]:\n",
    "            f.write(k2+'\\t')\n",
    "        f.write('\\n')\n",
    "        break\n",
    "\n",
    "    for k1 in dict_scores:\n",
    "        f.write(k1+'\\t')\n",
    "        for k2 in dict_scores[k1]:\n",
    "            f.write(f'{dict_scores[k1][k2][0]}/{dict_scores[k1][k2][1]}\\t')\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "gain_in_fragments = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./BioASQ-FromScratch-Llama3_Vocab/BioASQ_10K_1.0_\")\n",
    "import numpy as np\n",
    "with open('../../../../../TxtInputFiles/PAC_input.txt') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            org_enc = tokenizer_base.encode(line)\n",
    "            vocab_enc = tokenizer.encode(line)\n",
    "            \n",
    "            org_dec = tokenizer_base.decode(org_enc)\n",
    "            vocab_dec = tokenizer.decode(vocab_enc)\n",
    "            \n",
    "            assert org_dec == vocab_dec, f'Failed at {idx}'\n",
    "            \n",
    "            gain_in_fragments.append((len(org_enc)-len(vocab_enc))/len(org_enc))\n",
    "            \n",
    "            idx += 1\n",
    "            if idx%1000 == 0:\n",
    "                print(f'Processed {idx}.... {np.percentile(gain_in_fragments, [0,10,50,90,100])}')\n",
    "        except:\n",
    "            print(f'--------------------Failed at {idx}--------------------')\n",
    "            print(f'Orig : {org_dec}')\n",
    "            print(f'Vocab: {vocab_dec}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(gain_in_fragments, [0,10,50,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\\u00c4\\u00aa\\u00c4\\u00a8\"\n",
    "print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunjanbalde/miniconda3/envs/env_transformers_LLama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "havioral 1\n",
      "Ġacetaminophen 2\n",
      "Ġantihypertensive 3\n",
      "Ġhypercholesterolemia 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "\n",
    "vocab_tokens = [x.strip() for x in open('./EBM/25K_0.5_.txt').read().splitlines()]\n",
    "\n",
    "max_token, max_token_to_add = '',0\n",
    "for word in vocab_tokens:\n",
    "    tokens_to_add = len(tokenizer.tokenize(word if not word.startswith('Ġ') else ' '+word[1:]))-2\n",
    "    if tokens_to_add > max_token_to_add:\n",
    "        max_token = word\n",
    "        max_token_to_add = tokens_to_add\n",
    "        print(word,tokens_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers_LLama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
