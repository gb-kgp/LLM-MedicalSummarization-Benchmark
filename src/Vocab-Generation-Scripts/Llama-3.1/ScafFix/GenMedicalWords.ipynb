{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script takes the csv files  and filters out the words which are not present in UMLS.\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/path/to/umls'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counter_PAC = defaultdict(int)\n",
    "for csv_path in ['../../../../TxtInputFiles/PAC_input.txt']:\n",
    "\n",
    "    print(f'Starting for {csv_path}.....')\n",
    "    lines_PAC = open(csv_path).readlines()\n",
    "\n",
    "    for idx,abs in enumerate(lines_PAC):\n",
    "        if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "        flag = 0\n",
    "        d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            for l in d:\n",
    "                counter_PAC[l[0]['ngram']] += 1\n",
    "                \n",
    "    if idx%10000 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "    # df.to_csv('./%sWithConsiderFlag_AllSemTypes.csv'%csv_path.split('/')[-1][:-4],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_tokens = list(counter_PAC.keys())\n",
    "val_tokens = list(counter_PAC.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Tokens':list_tokens,'Count':val_tokens})\n",
    "df.to_csv('PAC_Tokens.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.percentile(val_tokens,[0,10,25,50,75,90,95,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_PAC = defaultdict(int)\n",
    "for csv_path in ['./EBM_Train.jsonl']:\n",
    "\n",
    "    print(f'Starting for {csv_path}.....')\n",
    "    lines_PAC = open(csv_path).readlines()\n",
    "    lines_PAC = [json.loads(l)['prompt'] for l in lines_PAC]\n",
    "\n",
    "    for idx,abs in enumerate(lines_PAC):\n",
    "        if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "        flag = 0\n",
    "        d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            for l in d:\n",
    "                counter_PAC[l[0]['ngram']] += 1\n",
    "                \n",
    "    if idx%100 == 0: print(f'Processed till {idx+1}... {len(counter_PAC)} are considered till now.')\n",
    "\n",
    "    # df.to_csv('./%sWithConsiderFlag_AllSemTypes.csv'%csv_path.split('/')[-1][:-4],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_tokens = list(counter_PAC.keys())\n",
    "val_tokens = list(counter_PAC.values())\n",
    "\n",
    "df = pd.DataFrame({'Tokens':list_tokens,'Count':val_tokens})\n",
    "df.to_csv('EBM_Tokens.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>previous myocardial infarction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high-density lipoprotein</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low-density lipoprotein</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronary heart disease</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardiovascular events</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>band counts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>cell counts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>lidocaine-prilocaine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>topical anaesthetic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>intercourse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12340 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Tokens  Count\n",
       "0      previous myocardial infarction      2\n",
       "1            high-density lipoprotein     27\n",
       "2             low-density lipoprotein     35\n",
       "3              Coronary heart disease      2\n",
       "4               cardiovascular events     37\n",
       "...                               ...    ...\n",
       "12335                     band counts      1\n",
       "12336                     cell counts      2\n",
       "12337            lidocaine-prilocaine      1\n",
       "12338             topical anaesthetic      1\n",
       "12339                     intercourse      1\n",
       "\n",
       "[12340 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_PAC = pd.read_csv('PAC_Tokens.csv')\n",
    "df_BioASQ = pd.read_csv('EBM_Tokens.csv')\n",
    "\n",
    "df_BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12340.000000\n",
       "mean        13.448703\n",
       "std         87.294873\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          6.000000\n",
       "max       4269.000000\n",
       "Name: Count, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BioASQ['Count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tokens_PAC = df_PAC.set_index('Tokens')['Count'].to_dict()\n",
    "tokens_PAC_words = defaultdict(int)\n",
    "\n",
    "tokens_BioASQ = df_BioASQ.set_index('Tokens')['Count'].to_dict()\n",
    "tokens_BioASQ_words = defaultdict(int)\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "for key,val in tokens_PAC.items():\n",
    "    try:\n",
    "      for word in key.split():  \n",
    "        if re.match(pattern,word.strip()):\n",
    "            tokens_PAC_words[word] += val\n",
    "    except: pass\n",
    "  \n",
    "for key,val in tokens_BioASQ.items():\n",
    "    try:\n",
    "      for word in key.split():  \n",
    "        if re.match(pattern,word.strip()):\n",
    "            tokens_BioASQ_words[word] += val\n",
    "    except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Word  Count\n",
      "0                Yersinia    387\n",
      "1          enterocolitica    155\n",
      "2           agglutination   1261\n",
      "3                reaction  30928\n",
      "4              rheumatoid   2403\n",
      "...                   ...    ...\n",
      "81111  choledochoduodenal      1\n",
      "81112           mabuterol      3\n",
      "81113              Varroa      1\n",
      "81114           jacobsoni      1\n",
      "81115           anopheles      1\n",
      "\n",
      "[81116 rows x 2 columns]\n",
      "             Word  Count\n",
      "0        previous    108\n",
      "1      myocardial    128\n",
      "2      infarction    116\n",
      "3     lipoprotein    105\n",
      "4        Coronary      5\n",
      "...           ...    ...\n",
      "8686   clustering      1\n",
      "8687    offspring      4\n",
      "8688      origins      1\n",
      "8689        Today      1\n",
      "8690          CBC      1\n",
      "\n",
      "[8691 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_tokens_PAC_words = pd.DataFrame(list(tokens_PAC_words.items()), columns=['Word', 'Count'])\n",
    "print(df_tokens_PAC_words)\n",
    "\n",
    "df_BioASQ_words = pd.DataFrame(list(tokens_BioASQ_words.items()), columns=['Word', 'Count'])\n",
    "print(df_BioASQ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Word   Count\n",
      "240           cells  406536\n",
      "29         patients  393424\n",
      "262            cell  221685\n",
      "106            less  204305\n",
      "438        activity  184463\n",
      "...             ...     ...\n",
      "66178  textilotoxin       1\n",
      "66177   ammodytoxin       1\n",
      "47236    Converting       1\n",
      "66174         Brant       1\n",
      "81115     anopheles       1\n",
      "\n",
      "[81116 rows x 2 columns]\n",
      "              Word  Count\n",
      "58           query   4269\n",
      "37        patients   3291\n",
      "23     information   2993\n",
      "72             You   2846\n",
      "146      treatment   2166\n",
      "...            ...    ...\n",
      "4046   orthopaedic      1\n",
      "4045  orthopaedics      1\n",
      "6797          four      1\n",
      "6798     quadrants      1\n",
      "8690           CBC      1\n",
      "\n",
      "[8691 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "top_50_percent_PAC = df_tokens_PAC_words.nlargest(int(len(df_tokens_PAC_words)), 'Count')\n",
    "top_50_percent_BioASQ = df_BioASQ_words.nlargest(int(len(df_BioASQ_words)), 'Count')\n",
    "\n",
    "print(top_50_percent_PAC)\n",
    "print(top_50_percent_BioASQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenier = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "pre_vocab = tokenier.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/b5tg3bsn4l5cnf3dy16wx6fh0000gn/T/ipykernel_13204/1201604217.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_common['Splits'] = df_common['Word'].apply(lambda x: len(tokenier.tokenize(x)))\n"
     ]
    }
   ],
   "source": [
    "df_common = top_50_percent_PAC[top_50_percent_PAC['Word'].isin(top_50_percent_BioASQ['Word'])]\n",
    "df_common['Splits'] = df_common['Word'].apply(lambda x: len(tokenier.tokenize(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table_1 = df_common[df_common['Splits'] > 1]\n",
    "lookup_table_1.to_csv('EBM_SplitMoreThan1_OOV.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "lookup_table_1 = pd.read_csv('./EBM_SplitMoreThan1_OOV.csv')\n",
    "lookup_table_words_1 = lookup_table_1['Word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "os.mkdir('./EBM_Lookup_SplitMoreThan1_Vocab')\n",
    "\n",
    "\n",
    "for i in range(50,501,50):\n",
    "    dump_words = []\n",
    "    for word in lookup_table_words_1:\n",
    "        if len(dump_words) == i:\n",
    "            break\n",
    "        \n",
    "        if word not in pre_vocab:\n",
    "            dump_words.append(word)\n",
    "        \n",
    "    with open(f'EBM_Lookup_SplitMoreThan1_Vocab/EBM_Lookup_{i}.txt','w') as f: #{i+500}.txt','w') as f:\n",
    "        f.write('\\n'.join([f'''{x}''' for idx,x in enumerate(dump_words)]))\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "vocab_tokenizer = AutoTokenizer.from_pretrained('./EBM_Lookup_SplitMoreThan1_Vocab_Llama3/EBM_Lookup_500')\n",
    "idx = 0\n",
    "with open('../../../../../TxtInputFiles/PAC_input.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        org_dec = org_tokenizer.decode(org_tokenizer.encode(line))\n",
    "        vocab_dec = vocab_tokenizer.decode(vocab_tokenizer.encode(line))\n",
    "        \n",
    "        assert org_dec == vocab_dec, f'Failed at {idx}'\n",
    "        \n",
    "        idx += 1\n",
    "        if idx%1000 == 0:\n",
    "            print(f'Processed {idx}....')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
