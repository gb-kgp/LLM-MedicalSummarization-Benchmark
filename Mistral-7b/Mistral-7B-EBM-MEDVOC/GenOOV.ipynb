{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer.save_pretrained('Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "dict_OOV_freq = defaultdict(int)\n",
    "dict_splits = defaultdict(int)\n",
    "\n",
    "with open('../../../../TxtInputFiles/EBM_input.txt','r') as f:\n",
    "    for line in f:\n",
    "        # data = json.loads(line)\n",
    "        target_text = line.strip()\n",
    "        target_text = re.sub(r'[^\\w\\s]', '', target_text)\n",
    "        sws = tokenizer.tokenize(target_text)\n",
    "        print(sws)\n",
    "        i = 0\n",
    "        while(i < len(sws)-1):\n",
    "            if sws[i].startswith('▁'):\n",
    "                if sws[i+1].startswith('▁'):\n",
    "                    i+=1\n",
    "                    continue\n",
    "                else:\n",
    "                    sw = []\n",
    "                    while(True):\n",
    "                        sw.append(sws[i])\n",
    "                        i+=1\n",
    "                        if i == len(sws) or sws[i].startswith('▁'):\n",
    "                            break\n",
    "                    print(sw)\n",
    "                    dict_OOV_freq[''.join(sw).replace('▁','')] += 1\n",
    "                    dict_splits[''.join(sw).replace('▁','')] = len(sw)\n",
    "\n",
    "list_token, list_freq, list_split = list(), list(), list()        \n",
    "for token in dict_OOV_freq:\n",
    "    list_token.append(token)\n",
    "    list_freq.append(dict_OOV_freq[token])\n",
    "    list_split.append(dict_splits[token])\n",
    "\n",
    "df = pd.DataFrame({'token': list_token, 'freq': list_freq, 'split': list_split})\n",
    "df.to_csv(f'EBM_OOV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./CHQ_Vocabs/25K_0.5_/')\n",
    "# tokenizer.save_pretrained('./EBM-20K-1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_OOV = 0\n",
    "vocab = tokenizer.get_vocab()\n",
    "for v in vocab:\n",
    "    if vocab[v] < 258: continue\n",
    "    sws = tokenizer.tokenize(v.replace('▁',' '))\n",
    "    if len(sws) > 1 and not sws[1]==v:\n",
    "        # if vocab[v] < 32000: print(v,'PRE',vocab[v],tokenizer.tokenize(v.replace('▁',' ')))\n",
    "        if vocab[v] >= 32000: \n",
    "            print(v,'ADDED',vocab[v],tokenizer.tokenize(v.replace('▁',' ')))\n",
    "            COUNT_OOV += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3156/12212 + 984/3610 + 2579/9330 + 715/2358)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./IFT+CHQ_Train_20K.jsonl','r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        print(data['prompt'].replace('##Title','##Resp'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_path = \"pritamdeka/PubMedBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "df_BioASQ = pd.read_csv('../../../../../../Medical/PushToNeumann/CSV-Datasets/EBM-Train.csv')\n",
    "text_PAC = open('../../../../TxtInputFiles/PAC_input.txt','r').readlines()\n",
    "text_BioASQ = df_BioASQ['input_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BioASQ_Encodings = model.encode(text_BioASQ, convert_to_tensor=True)\n",
    "BioASQ_Encodings = BioASQ_Encodings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pac_encodings = np.load('../Llama-2-7B-BioASQ/PAC_Encodings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "dist_rep = np.zeros((len(text_PAC),len(text_BioASQ)))\n",
    "\n",
    "for idx1,entry in enumerate(pac_encodings):\n",
    "    if (idx1+1)%1000 == 0:\n",
    "        print(f'Completed {idx1+1} entries')\n",
    "    for idx2,train_vec in enumerate(BioASQ_Encodings):\n",
    "        dist_rep[idx1,idx2] = np.linalg.norm(entry-train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_rep_mean = dist_rep.mean(axis=1)\n",
    "dist_rep_mean.shape\n",
    "\n",
    "sorted_indices = dist_rep_mean.argsort()[:50000]\n",
    "\n",
    "PAC_BioASQ = [text_PAC[idx] for idx in sorted_indices]\n",
    "with open('./EBM_PAC.txt','w') as f:\n",
    "    f.write(''.join(PAC_BioASQ))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
