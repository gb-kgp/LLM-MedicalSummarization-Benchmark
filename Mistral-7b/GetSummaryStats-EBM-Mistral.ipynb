{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de46360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenization_llama_Lookup import LlamaTokenizer\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "tokenizer_MEDVOC = AutoTokenizer.from_pretrained('./Mistral-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Mistral/25K_0.75_')\n",
    "tokenizer_MEDVOC_ABPE = LlamaTokenizer.from_pretrained('./Mistral-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Mistral/25K_0.75_',\n",
    "                                                             added_vocab = './Mistral-7B-EBM-MEDVOC/EBM_Vocabs_MEDVOC_Mistral/25K_0.75_/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "\n",
    "tokenizer_filter = AutoTokenizer.from_pretrained('./Mistral-7B-EBM-Filter/EBM_Filter_Vocabs_Mistral/20K')\n",
    "\n",
    "tokenizer_filter_ABPE = LlamaTokenizer.from_pretrained('./Mistral-7B-EBM-Filter/EBM_Filter_Vocabs_Mistral/20K',\n",
    "                                                             added_vocab = './Mistral-7B-EBM-Filter/EBM_Filter_Vocabs_Mistral/20K/added_vocab.txt',\n",
    "                                                             use_fast=False)\n",
    "\n",
    "tokenizer_lookup = LlamaTokenizer.from_pretrained('./Mistral-7B-EBM-MedicalLookup-Fragment/EBM_Lookup_SplitMoreThan1_Also_Add_As_Subwords_Mistral/EBM_Lookup_50',\n",
    "                                                 added_vocab = './Mistral-7B-EBM-MedicalLookup-Fragment/EBM_Lookup_SplitMoreThan1_Also_Add_As_Subwords_Mistral/EBM_Lookup_50/added_vocab.txt',\n",
    "                                                 use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788695",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), len(tokenizer_MEDVOC),len(tokenizer_MEDVOC_ABPE),len(tokenizer_filter), len(tokenizer_filter_ABPE), len(tokenizer_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TGT  = pd.read_csv(\"../../../../../../Medical/PushToNeumann/CSV-Datasets/EBM-Test.csv\")\n",
    "list_rs = df_TGT['target_text'].to_list()\n",
    "list_rs = [x+'||'+str(idx) for idx,x in enumerate(list_rs)]\n",
    "\n",
    "gs_base = open('./EBM-Summaries/EBM_100_2_Mistral_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_biomistral = open('./EBM-Summaries/EBM_100_2_BioMistral_Base.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_without_vocab = open('./EBM-Summaries/EBM_75_4_WithoutVocab_Mistral.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_medvoc = open('./EBM-Summaries/EBM_80_2__Mistral_MEDVOC_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_medvoc_adaptbpe = open('./EBM-Summaries/EBM_80_4__Mistral_MEDVOC_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_filtered_without_adaptbpe = open('./EBM-Summaries/EBM_80_4__Mistral_Filter_WithoutAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "gs_cpt_filtered_adaptbpe = open('./EBM-Summaries/EBM_80_4__Mistral_Filter_WithAdaptBPE.txt','r').read().split('\\n++++++++++++++++++\\n')\n",
    "\n",
    "gs_cpt_lookup = open('./EBM-Summaries/EBM_80_2_Mistral_Lookup50.txt','r').read().split('\\n++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_cmp = defaultdict(list)\n",
    "\n",
    "for gs,rs in zip(gs_base, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_biomistral, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_without_vocab, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_medvoc_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_without_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_filtered_adaptbpe, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n",
    "\n",
    "for gs,rs in zip(gs_cpt_lookup, list_rs):\n",
    "    dict_cmp[rs].append(gs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def RL(ref, hyp):\n",
    "    scores = scorer.score(ref,hyp)\n",
    "    return scores['rougeL'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs,gss in dict_cmp.items():\n",
    "  try:\n",
    "    rl_base = RL(rs.split('||')[0], gss[0])\n",
    "    rl_biomistral = RL(rs.split('||')[0], gss[1])\n",
    "    rl_cpt_without_vocab = RL(rs.split('||')[0], gss[2])\n",
    "    \n",
    "    rl_cpt_medvoc = RL(rs.split('||')[0], gss[3])\n",
    "    rl_cpt_medvoc_adaptbpe = RL(rs.split('||')[0], gss[4])\n",
    "    \n",
    "    rl_cpt_filtered_without_adaptbpe = RL(rs.split('||')[0], gss[5])\n",
    "    rl_cpt_filtered_adaptbpe = RL(rs.split('||')[0], gss[6])\n",
    "    \n",
    "    rl_cpt_lookup = RL(rs.split('||')[0], gss[7])\n",
    "    \n",
    "    dict_cmp[rs].extend([rl_base,rl_biomistral,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup])\n",
    "    \n",
    "  except: print(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_src = df_TGT['input_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_src[0], gs_base[0], list_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                \n",
    "    return oov_frac/consider\n",
    "\n",
    "def Novel_frac(src,rs):\n",
    "    novel_frac = 0\n",
    "    novel_list = list()\n",
    "    for tok in rs.split():\n",
    "        if tok not in src: \n",
    "            novel_frac += 1\n",
    "            novel_list.append(tok)\n",
    "    \n",
    "    return novel_frac/len(rs.split()), novel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c918a-fd33-456a-9291-edc3a0cf137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_frac(tokenizer,list_rs[10]), list_rs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7f99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_src = defaultdict(list)\n",
    "for rs,src in zip(list_rs,list_src): dict_src[rs].append(src)\n",
    "\n",
    "list_del_key = list()\n",
    "for key,val in dict_src.items():\n",
    "    # print(key,val)\n",
    "    if len(val[0]) == 0: \n",
    "        list_del_key.append(key)\n",
    "        continue\n",
    "        \n",
    "    dict_src[key].extend(Novel_frac(val[0].lower(),key))\n",
    "    dict_src[key].append(OOV_frac(tokenizer,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_MEDVOC_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_filter_ABPE,val[0]))\n",
    "    dict_src[key].append(OOV_frac(tokenizer_lookup,val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cc4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rl_base = list()\n",
    "list_rl_biomistral = list()\n",
    "list_rl_cpt_without_vocab = list()\n",
    "list_rl_cpt_medvoc = list()\n",
    "list_rl_cpt_medvoc_adaptbpe = list()\n",
    "list_rl_cpt_filtered_without_adaptbpe = list()\n",
    "list_rl_cpt_filtered_adaptbpe = list()\n",
    "list_rl_cpt_lookup = list()\n",
    "\n",
    "list_oov_src_base = list()\n",
    "list_oov_src_medvoc = list()\n",
    "list_oov_src_medvoc_adaptbpe = list()\n",
    "list_oov_src_filtered_without_adaptbpe = list()\n",
    "list_oov_src_filtered_adaptbpe = list()\n",
    "list_oov_src_lookup = list()\n",
    "\n",
    "list_oov_rs_base = list()\n",
    "list_oov_rs_medvoc = list()\n",
    "list_oov_rs_medvoc_adaptbpe = list()\n",
    "list_oov_rs_filtered_without_adaptbpe = list()\n",
    "list_oov_rs_filtered_adaptbpe = list()\n",
    "list_oov_rs_lookup = list()\n",
    "\n",
    "list_len_rs_base = list()\n",
    "list_len_rs_medvoc = list()\n",
    "list_len_rs_medvoc_adaptbpe = list()\n",
    "list_len_rs_filtered_without_adaptbpe = list()\n",
    "list_len_rs_filtered_adaptbpe = list()\n",
    "list_len_rs_lookup = list()\n",
    "\n",
    "list_novel_rs = list()\n",
    "\n",
    "list_gs_base = list()\n",
    "list_gs_biomistral = list()\n",
    "list_gs_cpt_without_vocab = list()\n",
    "list_gs_cpt_medvoc = list()\n",
    "list_gs_cpt_medvoc_adaptbpe = list()\n",
    "list_gs_cpt_filtered_without_adaptbpe = list()\n",
    "list_gs_cpt_filtered_adaptbpe = list()\n",
    "list_gs_cpt_lookup = list()\n",
    "\n",
    "list_rs = list()\n",
    "list_sd = list()\n",
    "\n",
    "for rs in dict_cmp:\n",
    "  try:\n",
    "    gen_base,gen_biomistral,gen_cpt_without_vocab,gen_medvoc,gen_medvoc_adaptbpe,gen_cpt_filtered_without_adaptbpe,gen_cpt_filtered_adaptbpe,gen_cpt_lookup,rl_base,rl_biomistral,rl_cpt_without_vocab,rl_cpt_medvoc,rl_cpt_medvoc_adaptbpe,rl_cpt_filtered_without_adaptbpe,rl_cpt_filtered_adaptbpe,rl_cpt_lookup = dict_cmp[rs]\n",
    "    \n",
    "    sd = dict_src[rs][0]\n",
    "    \n",
    "    len_rs_base = len(tokenizer.tokenize(rs))\n",
    "    len_rs_medvoc = len(tokenizer_MEDVOC.tokenize(rs))\n",
    "    len_rs_medvoc_adaptbpe = len(tokenizer_MEDVOC_ABPE.tokenize(rs))\n",
    "    len_rs_filtered_without_adaptbpe = len(tokenizer_filter.tokenize(rs))\n",
    "    len_rs_filtered_adaptbpe = len(tokenizer_filter_ABPE.tokenize(rs))\n",
    "    len_rs_lookup = len(tokenizer_lookup.tokenize(rs))\n",
    "\n",
    "    oov_sd_base = dict_src[rs][-6]\n",
    "    oov_sd_medvoc = dict_src[rs][-5]\n",
    "    oov_sd_medvoc_adaptbpe = dict_src[rs][-4]\n",
    "    oov_sd_filtered_without_adaptbpe = dict_src[rs][-3]\n",
    "    oov_sd_filtered_adaptbpe = dict_src[rs][-2]\n",
    "    oov_sd_lookup= dict_src[rs][-1]\n",
    "\n",
    "    oov_rs_base = OOV_frac(tokenizer,rs)\n",
    "    oov_rs_medvoc = OOV_frac(tokenizer_MEDVOC,rs)\n",
    "    oov_rs_medvoc_adaptbpe = OOV_frac(tokenizer_MEDVOC_ABPE,rs)\n",
    "    oov_rs_filtered_without_adaptbpe = OOV_frac(tokenizer_filter,rs)\n",
    "    oov_rs_filtered_adaptbpe = OOV_frac(tokenizer_filter_ABPE,rs)\n",
    "    oov_rs_lookup = OOV_frac(tokenizer_lookup,rs)\n",
    "    \n",
    "    novel_rs = dict_src[rs][1]\n",
    "    \n",
    "    list_rl_base.append(rl_base)\n",
    "    list_rl_biomistral.append(rl_biomistral)\n",
    "    list_rl_cpt_without_vocab.append(rl_cpt_without_vocab)\n",
    "    list_rl_cpt_medvoc.append(rl_cpt_medvoc)\n",
    "    list_rl_cpt_medvoc_adaptbpe.append(rl_cpt_medvoc_adaptbpe)\n",
    "    list_rl_cpt_filtered_without_adaptbpe.append(rl_cpt_filtered_without_adaptbpe)\n",
    "    list_rl_cpt_filtered_adaptbpe.append(rl_cpt_filtered_adaptbpe)\n",
    "    list_rl_cpt_lookup.append(rl_cpt_lookup)\n",
    "    \n",
    "    list_len_rs_base.append(len_rs_base)\n",
    "    list_len_rs_medvoc.append(len_rs_medvoc)\n",
    "    list_len_rs_medvoc_adaptbpe.append(len_rs_medvoc_adaptbpe)\n",
    "    list_len_rs_filtered_without_adaptbpe.append(len_rs_filtered_without_adaptbpe)\n",
    "    list_len_rs_filtered_adaptbpe.append(len_rs_filtered_adaptbpe)\n",
    "    list_len_rs_lookup.append(len_rs_lookup)\n",
    "    \n",
    "    list_gs_base.append(gen_base)\n",
    "    list_gs_biomistral.append(gen_biomistral)\n",
    "    list_gs_cpt_medvoc.append(gen_medvoc)\n",
    "    list_gs_cpt_medvoc_adaptbpe.append(gen_medvoc_adaptbpe)\n",
    "    list_gs_cpt_without_vocab.append(gen_cpt_without_vocab)\n",
    "    list_gs_cpt_filtered_without_adaptbpe.append(gen_cpt_filtered_without_adaptbpe)\n",
    "    list_gs_cpt_filtered_adaptbpe.append(gen_cpt_filtered_adaptbpe)\n",
    "    list_gs_cpt_lookup.append(gen_cpt_lookup)\n",
    "    \n",
    "    list_rs.append(rs)\n",
    "    list_sd.append(sd)\n",
    "    \n",
    "    list_novel_rs.append(novel_rs)\n",
    "\n",
    "    list_oov_rs_base.append(oov_rs_base)\n",
    "    list_oov_rs_medvoc.append(list_oov_rs_medvoc)\n",
    "    list_oov_rs_medvoc_adaptbpe.append(list_oov_rs_medvoc_adaptbpe)\n",
    "    list_oov_rs_filtered_without_adaptbpe.append(oov_rs_filtered_without_adaptbpe)\n",
    "    list_oov_rs_filtered_adaptbpe.append(oov_rs_filtered_adaptbpe)\n",
    "    list_oov_rs_lookup.append(oov_rs_lookup)\n",
    "\n",
    "    list_oov_src_base.append(oov_sd_base)\n",
    "    list_oov_src_medvoc.append(oov_sd_medvoc)\n",
    "    list_oov_src_medvoc_adaptbpe.append(oov_sd_medvoc_adaptbpe)\n",
    "    list_oov_src_filtered_without_adaptbpe.append(oov_sd_filtered_without_adaptbpe)\n",
    "    list_oov_src_filtered_adaptbpe.append(oov_sd_filtered_adaptbpe)\n",
    "    list_oov_src_lookup.append(oov_sd_lookup)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "#     print(rs)\n",
    "    print(len(dict_cmp[rs]), len(dict_src[rs]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb72290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'SD': list_sd,'RS':list_rs, \\\n",
    "                   'GS_Base': list_gs_base , 'GS_BioMistral': list_gs_biomistral,'GS_CPT_Without_Vocab': list_gs_cpt_without_vocab, \\\n",
    "                   'GS_CPT_MedVoc': list_gs_cpt_medvoc, 'GS_CPT_MedVoc_AdaptBPE': list_gs_cpt_medvoc_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_WithoutAdaptBPE': list_gs_cpt_filtered_without_adaptbpe, \\\n",
    "                   'GS_CPT_Filtering_AdaptBPE': list_gs_cpt_filtered_adaptbpe, \\\n",
    "                   'GS_CPT_Lookup':list_gs_cpt_lookup, \\\n",
    "                   'RS_Len_Base': list_len_rs_base, 'RS_Len_Filtering_WithoutAdaptBPE': list_len_rs_filtered_without_adaptbpe, \\\n",
    "                   'RS_Len_MedVoc': list_len_rs_medvoc, 'RS_Len_MedVoc_AdaptBPE': list_len_rs_medvoc_adaptbpe, \\\n",
    "                   'RS_Len_Filtering_AdaptBPE': list_len_rs_filtered_adaptbpe, \\\n",
    "                   'RS_Len_Lookup' : list_len_rs_lookup, \\\n",
    "                   'OOV_RS_Base': list_oov_rs_base, 'OOV_RS_Filtering_WithoutAdaptBPE': list_oov_rs_filtered_without_adaptbpe, \\\n",
    "                   'OOV_RS_MedVoc': list_oov_rs_medvoc, 'OOV_RS_MedVoc_AdaptBPE': list_oov_rs_medvoc_adaptbpe, \\\n",
    "                   'OOV_RS_Filtering_AdaptBPE': list_oov_rs_filtered_adaptbpe, \\\n",
    "                   'OOV_RS_Lookup' : list_oov_rs_lookup, \\\n",
    "                   'Novel_RS': list_novel_rs, \\\n",
    "                   'OOV_SD_Base': list_oov_src_base, 'OOV_SD_Filtering_WithoutAdaptBPE': list_oov_src_filtered_without_adaptbpe, \\\n",
    "                   'OOV_SD_MedVoc': list_oov_src_medvoc, 'OOV_SD_MedVoc_AdaptBPE': list_oov_src_medvoc_adaptbpe, \\\n",
    "                   'OOV_SD_Filtering_AdaptBPE': list_oov_src_filtered_adaptbpe, \\\n",
    "                   'OOV_SD_Lookup' : list_oov_src_lookup, \\\n",
    "                   'R-L_Base': list_rl_base , 'R-L_BioMistral': list_rl_biomistral, 'R-L_CPT_Without_Vocab': list_rl_cpt_without_vocab, \\\n",
    "                   'R-L_CPT_MedVoc': list_rl_cpt_medvoc, 'R-L_CPT_MedVoc_AdaptBPE': list_rl_cpt_medvoc_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_WithoutAdaptBPE': list_rl_cpt_filtered_without_adaptbpe, \\\n",
    "                   'R-L_CPT_Filtering_AdaptBPE': list_rl_cpt_filtered_adaptbpe, \\\n",
    "                   'R-L_CPT_Lookup':list_rl_cpt_lookup\n",
    "                   })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dc0d-760a-46ca-a626-e7525d551d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./EBM-Summaries/EBM_Mistral_Compare_WithBioMistral.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08f7b-fb68-41ef-a236-92370936dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Mistral_Compare_MedicalWords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "umls_path = '/Users/gunjanbalde/Documents/QuickUMLS_Files/'\n",
    "matcher = QuickUMLS(umls_path,similarity_name='cosine',threshold=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "counter_TGT_RS = defaultdict(list)\n",
    "\n",
    "lines_TGT = df['SD'].to_list()\n",
    "\n",
    "for idx,abs in enumerate(lines_TGT):\n",
    "    if idx%10 == 0: print(f'Processed till {idx+1}... ')\n",
    "\n",
    "    flag = 0\n",
    "    d = matcher.match(abs, best_match=True, ignore_syntax=False)\n",
    "    if len(d) == 0:\n",
    "        print('No match found for',idx)\n",
    "        # counter_TGT_RS[idx].append('happy')\n",
    "    \n",
    "    else:\n",
    "        for l in d:\n",
    "            counter_TGT_RS[idx].append(l[0]['ngram'])\n",
    "\n",
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "counter_TGT_RS_Medical_Words = defaultdict(set)\n",
    "\n",
    "for key,val in counter_TGT_RS.items():\n",
    "    try:\n",
    "      print('---------')\n",
    "      for words in val:\n",
    "        for word in words.split():\n",
    "          print(word)\n",
    "          if re.match(pattern,word.strip()): counter_TGT_RS_Medical_Words[key].add(word.strip())\n",
    "    except: print(\"Error in\", key,val)\n",
    "\n",
    "for key,val in counter_TGT_RS_Medical_Words.items():\n",
    "    print(key,val)\n",
    "    print('-----------------')\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./EBM-Summaries/EBM_Mistral_MedicalWords_SD.pkl','wb') as f:\n",
    "    pkl.dump(counter_TGT_RS_Medical_Words,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter_TGT_RS_Medical_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open('../Llama-2-7b/EBM-Summaries/EBM_Llama2_Compare_SelfAdaptBPE_MedicalWords.pkl','rb') as f:\n",
    "    counter_TGT_RS_Medical_Words = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_medical_words_splitmorethan1 = list()\n",
    "list_medical_words_splitmorethan2 = list()\n",
    "list_medical_words_splitmorethan3 = list()\n",
    "\n",
    "for idx, words in counter_TGT_RS_Medical_Words.items():\n",
    "    total_words = len(words)\n",
    "    splitmorethan1 = 0\n",
    "    splitmorethan2 = 0\n",
    "    splitmorethan3 = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(tokenizer.tokenize(word)) > 1: splitmorethan1 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 2: splitmorethan2 += 1\n",
    "        if len(tokenizer.tokenize(word)) > 3: splitmorethan3 += 1\n",
    "    \n",
    "    list_medical_words_splitmorethan1.append(splitmorethan1/total_words)\n",
    "    list_medical_words_splitmorethan2.append(splitmorethan2/total_words)\n",
    "    list_medical_words_splitmorethan3.append(splitmorethan3/total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medical_Words_SplitMoreThan1'] = list_medical_words_splitmorethan1\n",
    "df['Medical_Words_SplitMoreThan2'] = list_medical_words_splitmorethan2\n",
    "df['Medical_Words_SplitMoreThan3'] = list_medical_words_splitmorethan3\n",
    "\n",
    "df.to_csv('./EBM-Summaries/EBM_Mistral_Compare_MedicalWords_WithBioMistral.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df4fe24-0242-4a59-b7fa-50be7b223156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    424.000000\n",
       "mean       0.294247\n",
       "std        0.085050\n",
       "min        0.111111\n",
       "10%        0.180606\n",
       "50%        0.288675\n",
       "90%        0.410227\n",
       "max        0.545455\n",
       "Name: Medical_Words_SplitMoreThan1_SD, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./EBM-Summaries/EBM_Mistral_Compare_MedicalWords_WithBioMistral.csv')\n",
    "df['Medical_Words_SplitMoreThan1_SD'].describe(percentiles=[0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8dc85a-cd81-47d0-9c40-8a8f69d95d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-L_Base</th>\n",
       "      <th>R-L_BioMistral</th>\n",
       "      <th>R-L_CPT_Without_Vocab</th>\n",
       "      <th>R-L_CPT_MedVoc</th>\n",
       "      <th>R-L_CPT_MedVoc_AdaptBPE</th>\n",
       "      <th>R-L_CPT_Filtering_WithoutAdaptBPE</th>\n",
       "      <th>R-L_CPT_Filtering_AdaptBPE</th>\n",
       "      <th>R-L_CPT_Lookup</th>\n",
       "      <th>CSr_Base</th>\n",
       "      <th>CSr_BioMistral</th>\n",
       "      <th>CSr_CPT_Without_Vocab</th>\n",
       "      <th>CSr_CPT_MedVoc</th>\n",
       "      <th>CSr_CPT_MedVoc_AdaptBPE</th>\n",
       "      <th>CSr_CPT_Filtering_WithoutAdaptBPE</th>\n",
       "      <th>CSr_CPT_Filtering_AdaptBPE</th>\n",
       "      <th>CSr_CPT_Lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.176632</td>\n",
       "      <td>0.166996</td>\n",
       "      <td>0.160582</td>\n",
       "      <td>0.163848</td>\n",
       "      <td>0.160171</td>\n",
       "      <td>0.145187</td>\n",
       "      <td>0.157652</td>\n",
       "      <td>0.168174</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.139780</td>\n",
       "      <td>0.112431</td>\n",
       "      <td>0.155537</td>\n",
       "      <td>0.140241</td>\n",
       "      <td>0.113455</td>\n",
       "      <td>0.108331</td>\n",
       "      <td>0.133819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.087715</td>\n",
       "      <td>0.090725</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>0.080869</td>\n",
       "      <td>0.074721</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.087298</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.154279</td>\n",
       "      <td>0.140067</td>\n",
       "      <td>0.153284</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.110901</td>\n",
       "      <td>0.122340</td>\n",
       "      <td>0.134449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.416107</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        R-L_Base  R-L_BioMistral  R-L_CPT_Without_Vocab  R-L_CPT_MedVoc  \\\n",
       "count  43.000000       43.000000              43.000000       43.000000   \n",
       "mean    0.176632        0.166996               0.160582        0.163848   \n",
       "std     0.087715        0.090725               0.077781        0.080869   \n",
       "min     0.050000        0.043956               0.028571        0.000000   \n",
       "50%     0.168421        0.148515               0.146341        0.153846   \n",
       "max     0.460606        0.407407               0.416107        0.391892   \n",
       "\n",
       "       R-L_CPT_MedVoc_AdaptBPE  R-L_CPT_Filtering_WithoutAdaptBPE  \\\n",
       "count                43.000000                          43.000000   \n",
       "mean                  0.160171                           0.145187   \n",
       "std                   0.074721                           0.069568   \n",
       "min                   0.000000                           0.046512   \n",
       "50%                   0.155039                           0.126761   \n",
       "max                   0.318182                           0.333333   \n",
       "\n",
       "       R-L_CPT_Filtering_AdaptBPE  R-L_CPT_Lookup   CSr_Base  CSr_BioMistral  \\\n",
       "count                   43.000000       43.000000  43.000000       43.000000   \n",
       "mean                     0.157652        0.168174   0.133300        0.139780   \n",
       "std                      0.075701        0.087298   0.147645        0.154279   \n",
       "min                      0.031250        0.020833   0.000000        0.000000   \n",
       "50%                      0.144330        0.153846   0.085106        0.095238   \n",
       "max                      0.400000        0.361111   0.555556        0.742857   \n",
       "\n",
       "       CSr_CPT_Without_Vocab  CSr_CPT_MedVoc  CSr_CPT_MedVoc_AdaptBPE  \\\n",
       "count              43.000000       43.000000                43.000000   \n",
       "mean                0.112431        0.155537                 0.140241   \n",
       "std                 0.140067        0.153284                 0.139303   \n",
       "min                 0.000000        0.000000                 0.000000   \n",
       "50%                 0.074074        0.088889                 0.102564   \n",
       "max                 0.562500        0.571429                 0.550000   \n",
       "\n",
       "       CSr_CPT_Filtering_WithoutAdaptBPE  CSr_CPT_Filtering_AdaptBPE  \\\n",
       "count                          43.000000                   43.000000   \n",
       "mean                            0.113455                    0.108331   \n",
       "std                             0.110901                    0.122340   \n",
       "min                             0.000000                    0.000000   \n",
       "50%                             0.102564                    0.071429   \n",
       "max                             0.451613                    0.482759   \n",
       "\n",
       "       CSr_CPT_Lookup  \n",
       "count       43.000000  \n",
       "mean         0.133819  \n",
       "std          0.134449  \n",
       "min          0.000000  \n",
       "50%          0.105263  \n",
       "max          0.606061  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1_SD'] >= 0.410227]\n",
    "df_desc = pd.DataFrame()\n",
    "for col in df_high_oov.columns:\n",
    "      if col.startswith('CSr') or col.startswith('R-L') : df_desc[col] = df_high_oov[col]\n",
    "df_desc.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019682e",
   "metadata": {},
   "source": [
    "High Expert OOV Concentration (Terms Split more than once)\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.136364\t0.140845\t0.137931\t0.136364\t0.137255\t0.150000\t0.135135\t0.125000\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.159509\t0.161290\t0.150000\t0.140000\t0.155039\t0.150000\t0.150000\t0.140351\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than once) -SD\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.168421\t0.148515\t0.146341\t0.153846\t0.155039\t0.126761\t0.144330\t0.153846\n",
    "\n",
    "High Expert OOV Concentration (Terms Split more than thrice) -SD\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.180645\t0.180000\t0.166667\t0.167539\t0.170732\t0.161616\t0.148571\t0.177778\n",
    "\n",
    "\n",
    "High Novelty\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.112360\t0.125000\t0.123288\t0.121212\t0.121212\t0.117647\t0.124031\t0.116667\n",
    "\n",
    "Low Novelty\n",
    "Base        BioMistral  W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.229708\t0.234754\t0.226600\t0.217939\t0.220063\t0.223471\t0.221203\t0.200592\n",
    "\n",
    "Long RS (All)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.168776\t0.160428\t0.153846\t0.163043\t0.164557\t0.151815\t0.153110\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.160000\t0.150376\t0.144330\t0.144578\t0.161616\t0.146789\t0.155340\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice)\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.165181\t0.171184\t0.162877\t0.161528\t0.171652\t0.162331\t0.162791\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than once) -SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.191781\t0.204082\t0.191781\t0.198675\t0.203704\t0.192000\t0.183333\n",
    "\n",
    "Low Expert OOV Concentration (Terms Split more than thrice) -SD\n",
    "Base        W/O Vocab   MEDVOC      MEDVOC-ABPE Filter      Filter-ABPE Lookup\n",
    "0.178571\t0.166667\t0.166667\t0.158416\t0.171875\t0.156863\t0.158140\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Base'] >= 203]\n",
    "df_long_rs.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98948082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def getMatchInfo(list_matches):\n",
    "    dict_concepts = defaultdict(list)\n",
    "    for match_l in list_matches:\n",
    "        for match_l_d in match_l:\n",
    "            if match_l_d[\"preferred\"] == 1: \n",
    "                cui = match_l_d['cui'] #the concept-id\n",
    "                start = match_l_d['start'] \n",
    "                end = match_l_d['end']\n",
    "                n_gram = match_l_d['ngram'].strip() #the surface form\n",
    "\n",
    "                if '\\n' in n_gram : continue\n",
    "                key = str(start)+'_'+str(end)+'_'+n_gram\n",
    "\n",
    "                if not cui in dict_concepts[key]: dict_concepts[key].append(cui)\n",
    "                    \n",
    "    str_ret = str()\n",
    "    \n",
    "    for key,val in dict_concepts.items():\n",
    "        str_ret += str(key) + ':'\n",
    "        for c_name in val:\n",
    "            if not '\\n' in c_name:\n",
    "                str_ret += c_name + '|'\n",
    "        str_ret += '\\n'\n",
    "    return (str_ret)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_overlap(concepts_ref, concepts_dec,verbose=False):\n",
    "    list_concepts_ref = concepts_ref.splitlines()\n",
    "    list_concepts_dec = concepts_dec.splitlines()\n",
    "    \n",
    "    cuis_dec, cuis_ref = list(), list()\n",
    "    \n",
    "    for line in list_concepts_ref:\n",
    "        cui_ref = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_ref.extend(cui_ref)\n",
    "    \n",
    "    for line in list_concepts_dec:\n",
    "        cui_dec = line.split(':')[1].split('|')[:-1]\n",
    "        cuis_dec.extend(cui_dec)\n",
    "    \n",
    "    cuis_ref = set(cuis_ref)\n",
    "    cuis_dec = set(cuis_dec)\n",
    "    \n",
    "    \n",
    "    common_cuis = cuis_dec.intersection(cuis_ref)\n",
    "    \n",
    "    if verbose: print('Common_cuis:', common_cuis)\n",
    "    \n",
    "    if len(common_cuis) == 0: return 0.\n",
    "    \n",
    "    prec = len(common_cuis)/len(cuis_dec)\n",
    "    rec = len(common_cuis)/len(cuis_ref)\n",
    "    \n",
    "    return ((2*prec*rec)/(prec+rec))\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import percentile\n",
    "import numpy as np\n",
    "def CIEval(f_score):\n",
    "    dataset = np.array([x[1] for x in f_score])\n",
    "    max_l = len(dataset)\n",
    "    scores = list()\n",
    "    for _ in range(1000):\n",
    "        # bootstrap sample\n",
    "        indices = randint(0, max_l, max_l)\n",
    "        sample = dataset[indices]\n",
    "        # calculate and store statistic\n",
    "        statistic = mean(sample)\n",
    "        scores.append(statistic)\n",
    "\n",
    "    print('50th percentile (median) = %.4f' % median(scores))\n",
    "    # calculate 95% confidence intervals (100 - alpha)\n",
    "    alpha = 5.0\n",
    "    # calculate lower percentile (e.g. 2.5)\n",
    "    lower_p = alpha / 2.0\n",
    "    # retrieve observation at lower percentile\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "#     print('%.1fth percentile = %.4f' % (lower_p, lower))\n",
    "    # calculate upper percentile (e.g. 97.5)\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    # retrieve observation at upper percentile\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "#     print('%.1fth percentile = %.4f' % (upper_p, upper))\n",
    "    print('C.I. Window = %.4f '%max([upper-median(scores),median(scores)-lower]))\n",
    "for column in ['GS_Base', 'GS_BioMistral','GS_CPT_Without_Vocab', 'GS_CPT_MedVoc',\n",
    "       'GS_CPT_MedVoc_AdaptBPE', 'GS_CPT_Filtering_WithoutAdaptBPE',\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'GS_CPT_Lookup']:\n",
    "\n",
    "    print(f'--Processing {column}.....')\n",
    "\n",
    "    f_score = list()\n",
    "    for row_id in range(df.shape[0]):\n",
    "        try:                \n",
    "            dec_sum = df.iloc[row_id][column]\n",
    "            ref_sum = df.iloc[row_id]['RS']\n",
    "            ref_sum = ref_sum.split('||')[0]\n",
    "            \n",
    "            ref_con = getMatchInfo(matcher.match(ref_sum, best_match=True, ignore_syntax=False))\n",
    "            dec_con = getMatchInfo(matcher.match(dec_sum, best_match=True, ignore_syntax=False))\n",
    "            \n",
    "            # print(f'REF_Sum: {ref_sum}')\n",
    "            # print(f'DEC_Sum: {dec_sum}')\n",
    "\n",
    "            # print(check_overlap(ref_con,dec_con))\n",
    "            f_score.append((row_id,check_overlap(ref_con,dec_con)))\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "#         break\n",
    "#     break\n",
    "    df['CSr'+column[2:]] = [x[1] for x in f_score]\n",
    "    CIEval(f_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87262a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./EBM-Summaries/EBM_Mistral_Compare_MedicalWords_WithBioMistral.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['Medical_Words_SplitMoreThan1'] >= 0.500000]\n",
    "df_desc = df_high_oov.drop(columns=[\n",
    "       'GS_CPT_Filtering_AdaptBPE', 'RS_Len_Base',\n",
    "       'RS_Len_Filtering_WithoutAdaptBPE', 'RS_Len_MedVoc',\n",
    "       'RS_Len_MedVoc_AdaptBPE', 'RS_Len_Filtering_AdaptBPE', 'RS_Len_Lookup',\n",
    "       'OOV_RS_Base', 'OOV_RS_Filtering_WithoutAdaptBPE', 'OOV_RS_MedVoc',\n",
    "       'OOV_RS_MedVoc_AdaptBPE', 'OOV_RS_Filtering_AdaptBPE', 'OOV_RS_Lookup',\n",
    "       'Novel_RS', 'OOV_SD_Base', 'OOV_SD_Filtering_WithoutAdaptBPE',\n",
    "       'OOV_SD_MedVoc', 'OOV_SD_MedVoc_AdaptBPE', 'OOV_SD_Filtering_AdaptBPE',\n",
    "       'OOV_SD_Lookup'])\n",
    "df_desc.describe(percentiles=[0.5])\n",
    "df_desc = df_desc[df_desc['R-L_Base']<df_desc['R-L_CPT_Filtering_WithoutAdaptBPE']]\n",
    "df_desc = df_desc[df_desc['R-L_CPT_Without_Vocab']<df_desc['R-L_CPT_Filtering_WithoutAdaptBPE']]\n",
    "df_desc_greatest = df_desc.nlargest(int(20), 'R-L_CPT_Lookup')\n",
    "for idx,row in df_desc_greatest.iterrows():\n",
    "    print('------------------------'*8)\n",
    "    print('+++SD',row['SD'])\n",
    "    print('+++RS        :',row['RS'])\n",
    "    print('######################################')\n",
    "    print('**GS_Base    :',row['GS_Base'])\n",
    "    print('**GS_W/oVocab:',row['GS_CPT_Without_Vocab'])\n",
    "    print('**GS_Lookup  :',row['GS_CPT_Filtering_WithoutAdaptBPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff81be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bb5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "print(wilcoxon(df_high_oov['R-L_CPT_Without_Vocab'].to_list(),df_high_oov['R-L_CPT_Lookup'].to_list(),alternative='less'), \\\n",
    "wilcoxon(df_low_oov['R-L_CPT_Without_Vocab'].to_list(),df_low_oov['R-L_CPT_Lookup'].to_list(),alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fff050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"^[A-Za-z▁]+$\"\n",
    "\n",
    "def OOV_frac(tokenizer,rs):\n",
    "    tokens = rs.split()\n",
    "    \n",
    "    oov_frac = 0\n",
    "    consider = 0\n",
    "    oov_list = list()\n",
    "    for token in tokens:\n",
    "        if re.match(pattern,token): \n",
    "            consider += 1\n",
    "            if len(tokenizer.tokenize(token)) > 1: \n",
    "                oov_frac += 1\n",
    "                oov_list.append([token,tokenizer.tokenize(token)])\n",
    "    return oov_frac/consider, oov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157230",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Filtering_WithoutAdaptBPE']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "list_common_words_worst = []\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_worst = df_high_oov_worst.sort_values(by='R-L_CPT_Filtering_WithoutAdaptBPE')\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Filtering_WithoutAdaptBPE'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Filtering_WithoutAdaptBPE'],row['RS_Len_Base'],row['RS_Len_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               : ',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Filtering_WithoutAdaptBPE'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Filtering w/o AdaptBPE           :',OOV_frac(tokenizer_filtering,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Filtering_WithoutAdaptBPE'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Filtering_WithoutAdaptBPE'].split()) \n",
    "    \n",
    "    list_common_words_worst.append(common_words)\n",
    "    print('-- Common words Frac                :', common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34241a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Lookup']>=0.27]\n",
    "df_high_oov.describe()\n",
    "\n",
    "df_long_rs = df[df['RS_Len_Lookup']>=93]\n",
    "df_long_rs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long-RS:\n",
    "Base > CPT_Filter_W/OABPE > CPT_W/O_Vocab > CPT_Lookup > CPT_Filter_AdaptBPE\n",
    "0.219003 > 0.213338 > 0.208825 > 0.205657 > 0.185030\n",
    "\n",
    "High-OOV:\n",
    "CPT_Lookup > Base > CPT_Filter_AdaptBPE > CPT_Filter_W/OABPE > CPT_W/O_Vocab\n",
    "0.263158 > 0.258094 > 0.255865 > 0.251984 > 0.245526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee666116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_oov = df[df['OOV_RS_Filtering_WithoutAdaptBPE']>=0.25]\n",
    "df_high_oov.describe()\n",
    "\n",
    "High-OOV:\n",
    "Lookup25.59 > W/O_Vocab25.00 > Base24.83 > Filter_W/OABPE22.04 > Filter_ABPE20.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_rs = df[df['RS_Len_Filtering_WithoutAdaptBPE']>=89]\n",
    "df_long_rs.describe()\n",
    "\n",
    "Long-RS:\n",
    "Base20.64 > Lookup20.15 > W/O_Vocab19.33 > Filter_W/OABPE18.90 > Filter_ABPE18.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2068244",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_better = df_high_oov[df_high_oov['R-L_CPT_Lookup']>df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_better.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_better.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_better.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_common_words = []\n",
    "df_high_oov_worst = df_high_oov[df_high_oov['R-L_CPT_Lookup']<=df_high_oov['R-L_CPT_Without_Vocab']]\n",
    "print(df_high_oov_worst.shape, df_high_oov.shape)\n",
    "df_high_oov_better = df_high_oov_worst.sort_values(by='R-L_CPT_Lookup',ascending=False)\n",
    "for idx,row in df_high_oov_worst.iterrows():\n",
    "    rs = row['RS']\n",
    "    print('++++++++++++', row['R-L_CPT_Lookup'], row['R-L_CPT_Without_Vocab'],row['Novel_RS'],row['OOV_RS_Base'],row['OOV_RS_Lookup'],row['RS_Len_Base'],row['RS_Len_Lookup'])\n",
    "    print('-- SD: ',row['SD'])\n",
    "    print('-- RS                               :',row['RS'])\n",
    "    print('***')\n",
    "    print('-- GS_CPT_Without_Vocab             :',row['GS_CPT_Without_Vocab'])\n",
    "    print('-- GS_CPT_Without_Filtering_AdaptBPE:',row['GS_CPT_Lookup'])\n",
    "    print('-- OOV Base Tokenizer               :', OOV_frac(tokenizer,rs)) \n",
    "    print('-- Lookup-1000                      :',OOV_frac(tokenizer_lookup,rs))\n",
    "    common_words = 0\n",
    "    for token1 in  row['GS_CPT_Lookup'].split():\n",
    "        if token1 in row['GS_CPT_Without_Vocab']: common_words+=1\n",
    "        \n",
    "    common_words /= len(row['GS_CPT_Lookup'].split()) \n",
    "    \n",
    "    list_common_words.append(common_words)\n",
    "    print('-- Common Words Frac               :', common_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d35c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMLS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
